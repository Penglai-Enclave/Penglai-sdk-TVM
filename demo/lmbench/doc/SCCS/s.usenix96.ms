h30085
s 00014/00007/01784
d D 1.53 97/12/01 15:50:06 lm 54 53
c some updates
cC
cK43025
e
s 00004/00004/01787
d D 1.52 96/01/22 23:26:28 lm 53 52
c *** empty log message ***
cK20394
e
s 00001/00001/01790
d D 1.51 95/11/14 11:12:31 lm 52 51
c typo.
cK19974
e
s 00030/00048/01761
d D 1.50 95/11/14 00:55:09 lm 51 50
c *** empty log message ***
cK19621
e
s 00101/00091/01708
d D 1.49 95/11/14 00:06:03 lm 50 49
c spelled it.
cK21464
e
s 00117/00116/01682
d D 1.48 95/11/13 22:40:32 lm 49 48
c checkpoint of Carl'\''s diffs...
cK55104
e
s 00247/00133/01551
d D 1.47 95/11/13 20:41:29 lm 48 47
c before carl edits.
cK53421
e
s 00133/00121/01551
d D 1.46 95/11/11 16:40:07 lm 47 46
c Saturday hacks; handoff to Carl.
cK47425
e
s 00001/00000/01671
d D 1.45 95/11/10 20:14:46 lm 46 45
c bug.
cK06034
e
s 00005/00005/01666
d D 1.44 95/11/10 20:12:33 lm 45 44
c *** empty log message ***
cK05282
e
s 00282/00266/01389
d D 1.43 95/11/10 20:08:54 lm 44 43
c merged.
cK04452
e
s 00033/00026/01622
d D 1.42 95/11/10 19:04:38 staelin 43 42
c more fixes from Susan McBain
cK41738
e
s 00001/00001/01647
d D 1.41 95/11/10 18:36:13 staelin 42 41
c oops --- missed two mods.
cK19262
e
s 00130/00121/01518
d D 1.40 95/11/10 18:34:06 staelin 41 40
c Susan McBain'\''s changes for the second half of the paper.
cK18584
e
s 00004/00004/01635
d D 1.39 95/11/10 17:40:31 staelin 40 39
c more comments
cK16897
e
s 00010/00010/01629
d D 1.38 95/11/10 17:32:54 staelin 39 38
c another two pages...
cK18959
e
s 00039/00038/01600
d D 1.37 95/11/10 16:47:58 staelin 38 37
c fixes to the first two pages.
cK20161
e
s 00091/00072/01547
d D 1.36 95/11/10 16:15:55 lm 37 36
c sending to carl
cK29862
e
s 00039/00013/01580
d D 1.35 95/11/08 20:38:14 lm 36 35
c stuff.
cK41355
e
s 00007/00005/01586
d D 1.34 95/11/07 16:33:01 lm 35 34
c a little fixing.
cK62026
e
s 00008/00003/01583
d D 1.33 95/11/07 15:55:47 staelin 34 33
c fixup reference format; one minor nit in the text.
cK49073
e
s 00074/00049/01512
d D 1.32 95/11/07 14:23:02 staelin 33 32
c Fixup file to recover lost updates...
cK39128
e
s 00148/00136/01413
d D 1.31 95/11/07 13:09:21 staelin 32 31
c fixed up a lot of text things; fixed up the references
cK17584
e
s 00070/00046/01479
d D 1.30 95/11/06 19:14:05 lm 31 30
c all sorts of stuff.
cK36651
e
s 00005/00001/01520
d D 1.29 95/11/03 21:20:57 lm 30 29
c systems.tbl.
cK09261
e
s 00013/00005/01508
d D 1.28 95/11/03 18:20:29 lm 29 28
c Ken'\''s comments
cK08971
e
s 00220/00159/01293
d D 1.27 95/11/03 13:34:04 lm 28 27
c Carl & Marty'\''s comments; my hacks.
cK01236
e
s 00004/00001/01448
d D 1.26 95/11/02 18:56:03 lm 27 26
c disks
cK63093
e
s 00006/00003/01443
d D 1.25 95/11/01 22:24:36 lm 26 25
c more eqn bugs.
cK56815
e
s 00003/00003/01443
d D 1.24 95/11/01 22:20:38 lm 25 24
c eqn bug.
cK55526
e
s 00074/00139/01372
d D 1.23 95/11/01 22:18:55 lm 24 23
c going home.
cK55526
e
s 00059/00023/01452
d D 1.22 95/11/01 16:58:32 lm 23 22
c merge with Carl.
cK08241
e
s 00148/00056/01327
d D 1.21 95/11/01 16:33:44 lm 22 21
c sending to carl.
cK36340
e
s 00047/00025/01336
d D 1.20 95/10/31 20:07:25 lm 21 20
c about to go home.
cK17505
e
s 00025/00034/01336
d D 1.19 95/10/31 16:55:14 lm 20 19
c various and sundry minor changes to fix blindingly obvious
c problems that became evident during a quick scan of the paper.
cK29614
e
s 00082/00071/01288
d D 1.18 95/10/30 21:27:42 staelin 19 18
c more changes near the end... Too tired to see straight, going to bed...
cK04432
e
s 00080/00058/01279
d D 1.17 95/10/30 17:40:05 staelin 18 17
c changes to memory latency and some other latency stuff.
cK36656
cZ-08:00
e
s 00175/00159/01162
d D 1.16 95/10/24 10:52:51 staelin 17 16
c some more changes by Carl; some rewrites to tighten the text; moved
c memory latency section forward to beginning of latency section.
cK59519
e
s 00098/00106/01223
d D 1.15 95/10/23 10:07:30 staelin 16 15
c editorial modifications through memory bandwidth section.
cK17291
e
s 00225/00300/01104
d D 1.14 95/10/19 13:12:27 lm 15 14
c *** empty log message ***
cK25410
e
s 00033/00019/01371
d D 1.13 95/10/10 11:21:59 staelin 14 13
c Some changes in response to comments by John Wilkes.
cK38644
e
s 00024/00018/01366
d D 1.12 95/10/10 09:32:56 staelin 13 12
c Some minor diffs by Carl (add a paragraph about non-local TCP)
cK16262
e
s 00240/00310/01144
d D 1.11 95/10/09 09:58:30 lm 12 11
c reworked the begining up to bandwidth section.
cK33063
e
s 00053/00036/01401
d D 1.10 95/10/08 15:20:01 lm 11 10
c converting tables.
cK02613
e
s 00088/00093/01349
d D 1.9 95/10/05 13:16:25 lm 10 9
c Scrunching.
cK30505
e
s 00032/00068/01410
d D 1.8 95/10/02 10:56:50 staelin 9 8
c fixed up refer stuff so it looks right
cK28275
e
s 00124/00072/01354
d D 1.7 95/09/29 15:34:14 staelin 8 7
c Modified "prior work"; spell checked; added some inter-benchmark
c comparison graphs; added some references; ...
cK14748
e
s 00002/00001/01424
d D 1.6 95/09/28 15:28:07 lm 7 6
c *** empty log message ***
cK44535
e
s 00138/00056/01287
d D 1.5 95/09/28 15:22:01 lm 6 5
c carl handoff.
cK42264
e
s 00025/00022/01318
d D 1.4 95/09/28 11:14:48 lm 5 4
c tables.
cK00682
e
s 00275/00080/01065
d D 1.3 95/09/27 18:39:03 staelin 4 3
c carl & larry dat @ hp :-)
cK58752
e
s 00995/00477/00150
d D 1.2 95/09/27 10:15:14 lm 3 2
c sending to carl.
cK42379
e
s 00627/00000/00000
d D 1.1 95/09/18 13:37:57 lm 2 1
c Initial revision
cK38418
e
s 00000/00000/00000
d D 1.0 95/09/18 13:37:56 lm 1 0
cBlm@lm.bitmover.com|ChangeSet|20000131225335|47351|--LMBENCH--
cHlm.bitmover.com
cK03998
cPdoc/usenix96.ms
cR941e479c9cec20f0
cV4
cX0x23
cZ-07:00
e
u
U
f e 0
f x 0x23
t
T
I 2
D 10
.\" $X$ xroff -mgs $file
.\" $tty$ groff -mgs $file | colcrt - | more
.\" $lpr$ groff -mgs $file > ${file}.lpr
E 10
I 10
.\" This document is GNU groff -mgs -t -p -R -s
.\" It will not print with normal troffs, it uses groff features, in particular,
.\" long names for registers & strings.
.\" Deal with it and use groff - it makes things portable.
.\"
.\" $X$ xroff -mgs -t -p -R -s $file
.\" $tty$ groff -mgs -t -p -R -s $file | colcrt - | more
.\" $lpr$ groff -mgs -t -p -R -s $file > ${file}.lpr
I 15
.VARPS
E 15
E 10
.\" Define a page top that looks cool
I 3
.\" HELLO CARL!  To turn this off, s/PT/oldPT/
I 15
D 53
.de PT
E 53
I 53
.de draftPT
E 53
I 22
D 51
.tl '\fBDRAFT\fP'Printed \\*(DY'\fBDRAFT\fP'
E 51
I 51
.\" .tl '\fBDRAFT\fP'Printed \\*(DY'\fBDRAFT\fP'
E 51
E 22
..
E 15
E 3
D 12
.de PT
E 12
I 12
D 53
.de oldPT
E 53
I 53
.de PT
E 53
E 12
D 3
.\" .if \\n%>1 \{\
.\" .	sp -.1i
.\" .	ps 14
.\" .	ft 3
.\" .	nr big 24
.\" .	nr space \\w'XXX'
.\" .	nr titlewid \\w'\\*[title]'
.\" .	nr barwid (\\n[LL]-(\\n[titlewid]+(2*\\n[space])))/2
.\" .	ds ln \\l'\\n[barwid]u'\\h'-\\n[barwid]u'\v'-.25'
.\" .	ds bar \\s(\\n[big]\\*(ln\\*(ln\\*(ln\\*(ln\\*(ln\v'1.25'\\h'\\n[barwid]
u'\\s0
.\" .	ce 1
.\" \\*[bar]\h'\\n[space]u'\v'-.15'\\*[title]\v'.15'\h'\\n[space]u'\\*[bar]
.\" .	ps
.\" .	sp -.70
.\" .	ps 12
.\" \\l'\\n[LL]u'
.\" .	ft
.\" .	ps
E 3
I 3
.if \\n%>1 \{\
.	sp -.1i
.	ps 14
.	ft 3
.	nr big 24
.	nr space \\w'XXX'
.	nr titlewid \\w'\\*[title]'
.	nr barwid (\\n[LL]-(\\n[titlewid]+(2*\\n[space])))/2
.	ds ln \\l'\\n[barwid]u'\\h'-\\n[barwid]u'\v'-.25'
.	ds bar \\s(\\n[big]\\*(ln\\*(ln\\*(ln\\*(ln\\*(ln\v'1.25'\\h'\\n[barwid]u'\\s0
.	ce 1
\\*[bar]\h'\\n[space]u'\v'-.15'\\*[title]\v'.15'\h'\\n[space]u'\\*[bar]
.	ps
.	sp -.70
.	ps 12
\\l'\\n[LL]u'
.	ft
.	ps
E 3
.\}
..
.\" Define a page bottom that looks cool
I 3
.\" HELLO CARL!  To turn this off, s/BT/oldBT/
I 22
D 53
.de BT
E 53
I 53
.de draftBT
E 53
D 51
.tl '\fBDRAFT\fP'Page %'\fBDRAFT\fP'
E 51
I 51
.\" .tl '\fBDRAFT\fP'Page %'\fBDRAFT\fP'
E 51
..
E 22
E 3
D 12
.de BT
E 12
I 12
D 53
.de oldBT
E 53
I 53
.de BT
E 53
E 12
D 3
.\" .	ps 9
.\" \v'-1'\\l'\\n(LLu'
.\" .	sp -1
.\" .	tl '\(co 1994 \\*[author]'\\*(DY'%'
.\" .	ps
E 3
I 3
.	ps 9
\v'-1'\\l'\\n(LLu'
.	sp -1
.	tl '\(co 1995 \\*[author]'\\*(DY'%'
.	ps
E 3
..
I 10
.de SP
D 48
.	sp .5
E 48
I 48
.	if t .sp .5
.	if n .sp 1
E 48
..
.de BU
.	SP
I 15
D 22
.	ne 3
E 22
I 22
.	ne 2
E 22
E 15
D 28
\(bu\ \ 
I 12
D 22
.	if \\n[.$] \fI\\$1\fP
E 22
I 22
.	if \\n[.$] \f(BI\\$1\fP\\$2
E 28
I 28
\(bu\ 
.	if \\n[.$] \fB\\$1\fP\\$2
E 28
E 22
E 12
..
.nr FIGURE 0
I 46
.nr TABLE 0
E 46
.nr SMALL .25i
.de TSTART
.	KF
.	if \\n[.$] \s(24\\l'\\n[pg@colw]u'\s0
I 33
.	ps -1
.	vs -1
E 33
I 30
D 32
.	ps -1
.	vs -1
E 32
E 30
..
.de TEND
I 33
.	ps +1
.	vs +1
E 33
I 30
D 32
.	ps +1
.	vs +1
E 32
E 30
.	if \\n[.$]=2 \{\
.	sp -.5
\s(24\\l'\\n[pg@colw]u'\s0 \}
.	sp .25
I 44
.	nr TABLE \\n[TABLE]+1
.	ce 1
\fBTable \\n[TABLE].\ \ \\$1\fP
.	SP
.	KE
..
.de FEND
.	ps +1
.	vs +1
.	if \\n[.$]=2 \{\
.	sp -.5
\s(24\\l'\\n[pg@colw]u'\s0 \}
.	sp .25
E 44
.	nr FIGURE \\n[FIGURE]+1
.	ce 1
\fBFigure \\n[FIGURE].\ \ \\$1\fP
.	SP
.	KE
..
I 24
D 25
.EQ
delim $$
.EN
E 25
E 24
E 10
.\" Configuration
D 3
.VARPS
E 3
D 12
.nr HM 1.0i
.nr FM 1i
D 3
.if t .nr PO .75i
.if t .nr LL 7.0i
E 3
I 3
.nr PO .75i
.po .75i
.nr LL 7.0i
E 12
I 12
D 15
.nr HM .9i
.nr FM .9i
.nr PO .9i
.po .9i
.nr LL 6.7i
E 15
I 15
.nr PI 3n
.nr HM .95i
.nr FM 1i
.nr PO .95i
D 48
.po .95i
E 48
I 48
.if t .po .95i
E 48
.nr LL 6.5i
E 15
E 12
E 3
D 48
.if n .nr PO .25i
.if n .nr LL 7.5i
E 48
I 48
.if n .nr PO 0i
.if n .nr LL 7.75i
E 48
D 3
.nr PS 9
.nr VS \n(PS+2
E 3
I 3
.nr PS 10
.nr VS \n(PS+1
E 3
D 14
.ds title Portable Tools for Performance Analysis
E 14
I 14
.ds title Portable tools for performance analysis
E 14
.ds author Larry McVoy
I 3
.ds lmbench \f(CWlmbench\fP
I 7
D 16
.ds getpid \f(CWgetpid\fP
E 7
I 4
.ds stream \f(CWstream\fP
E 16
I 16
.ds lmdd  \f(CWlmdd\fP
E 16
D 15
.ds bcopy \f(CWbcopy()\fP
.ds mmap \f(CWmmap(2)\fP
.ds memmove \f(CWmemmove()\fP
.ds uiomove \f(CWuiomove()\fP
D 12
.ds write \f(CWwrite(2)\fP
.ds read \f(CWread(2)\fP
.ds kill \f(CWkill(2)\fP
.ds fork \f(CWfork(2)\fP
.ds exit \f(CWexit(2)\fP
.ds connect \f(CWconnect(2)\fP
E 12
I 12
.ds write \f(CWwrite()\fP
.ds read \f(CWread()\fP
.ds kill \f(CWkill()\fP
.ds fork \f(CWfork()\fP
.ds exit \f(CWexit()\fP
.ds connect \f(CWconnect()\fP
E 12
.ds gettimeofday \f(CWgettimeofday()\fP
E 15
I 15
.ds bcopy \f(CWbcopy\fP
D 16
.ds mmap \f(CWmmap\fP
E 16
I 16
.ds connect \f(CWconnect\fP
.ds execlp  \f(CWexeclp\fP
.ds exit \f(CWexit\fP
.ds fork \f(CWfork\fP
.ds gcc \f(CWgcc\fP
.ds getpid \f(CWgetpid\fP
.ds getpid \f(CWgetpid\fP
.ds gettimeofday \f(CWgettimeofday\fP
.ds kill \f(CWkill\fP
E 16
.ds memmove \f(CWmemmove\fP
I 16
.ds mmap \f(CWmmap\fP
.ds popen  \f(CWpopen\fP
.ds read \f(CWread\fP
.ds stream \f(CWstream\fP
.ds system  \f(CWsystem\fP
E 16
.ds uiomove \f(CWuiomove\fP
.ds write \f(CWwrite\fP
D 16
.ds read \f(CWread\fP
.ds kill \f(CWkill\fP
.ds fork \f(CWfork\fP
.ds exit \f(CWexit\fP
.ds connect \f(CWconnect\fP
.ds gettimeofday \f(CWgettimeofday\fP
E 16
.ds yield  \f(CWyield\fP
D 16
.ds lmdd  \f(CWlmdd\fP
E 16
E 15
D 9
.R1
accumulate
database references
I 8
bracket-label \*([. \*(.] ", "
E 8
.R2
E 9
I 9
.\" References stuff
.de RN  \"Reference Name: .RN $1 -- prints the reference prettily
D 38
[\s-2\\$1\s+2]\\$2
E 38
I 38
.\"[\s-2\\$1\s+2]\\$2
D 48
[\\$1]\\$2
E 48
I 48
[\s-1\\$1\s0]\\$2
E 48
E 38
..
.\" .R1
.\" sort A+DT
.\" database references
.\" label-in-text
.\" label A.nD.y-2
.\" bracket-label \*([. \*(.] ", "
.\" .R2
I 25
D 26
.EQ
delim $$
.EN
E 26
E 25
E 9
E 4
E 3
.TL
D 15
lmbench:
.sp .5
\*[title]
E 15
I 15
D 54
\s(14lmbench: Portable tools for performance analysis\s0
E 54
I 54
\s(14lmbench: Portable tools for performance analysis\s0\**
E 54
E 15
D 3
.br
\s8Revision $Revision$ of $Date$\s0
E 3
.AU
D 21
\*[author]
.AI
.ps -2
D 3
lm@sgi.com\**
(415) 390-1804
E 3
I 3
D 12
Silicon Graphics Engineering\**
E 12
I 12
D 15
Silicon Graphics Engineering
E 15
I 15
Silicon Graphics, Inc.
E 15
E 12
E 3
.ps +2
E 21
I 21
D 48
\*[author]\**
E 48
I 48
\s+2\fR\*[author]\fP\s0
.AI
\fI\s+2Silicon Graphics, Inc.\s0\fP
E 48
D 24
.FS
lm@sgi.com, +1-415-933-1804, Silicon Graphics, Inc.
.FE
E 24
E 21
.AU
D 21
Carl Staelin
.AI
.ps -2
D 3
staelin@hpl.hp.com
(415) 857-6823
E 3
I 3
Hewlett Packard Laboratories
E 3
.ps +2
E 21
I 21
D 48
Carl Staelin\**
E 48
I 48
\s+2\fRCarl Staelin\fP
.AI
\s+2\fIHewlett-Packard Laboratories\s0\fP
E 48
D 24
.FS
staelin@hpl.hp.com, +1-415-857-6823, Hewlett Packard Laboratories.
.FE
E 24
E 21
D 15
.AB
E 15
I 15
.SP
D 54
.if t .MC 3.05i
I 24
D 48
.FS
Silicon Graphics, lm@sgi.com, 415-933-1804.
.FE
.FS
D 37
Hewlett Packard Laboratories, staelin@hpl.hp.com, 415-857-6823.
E 37
I 37
D 47
Hewlett\-Packard Laboratories, staelin@hpl.hp.com, 415-857-6823.
E 47
I 47
Hewlett-Packard Laboratories, staelin@hpl.hp.com, 415-857-6823.
E 47
E 37
.FE
E 48
E 24
.ce 1
.B \s+1Abstract\s0
.PP
E 54
I 54
.AB
E 54
E 15
D 3
lmbench is a set micro-benchmarks that measures important aspects
of system performance such as latency and bandwidth measurements 
for memory accesses.
The benchmarks also measures other important system operations such
as context switch, and in some instances provides comparative
measurements for similar data access methods, such as mmap'ed files
versus file reads.

There are a number of subtle issues associated with the construction
and analysis of data from several of the benchmarks.  For example,
it is very difficult to differentiate between the context switch time
and the overhead for the mechanism to force the context switches.  There
are also more mundane issues such as extracting accurate times (down to
nano-seconds) from systems which only provide rough granularity timers.

lmbench can provide a great deal of insight into many systems, and
users have discovered some system bugs that had gone previously undetected.
For example, it is usally possible to determine the effective size of
the memory cache(s) by analyzing the output from lmbench.  In at
least one system the effective cache size was dramatically smaller
than the real cache size due to a previously undetected bug in the 
memory management system.
E 3
I 3
D 12
\*[lmbench] is a set micro-benchmarks that measures important aspects
E 12
I 12
D 37
\*[lmbench] is a micro-benchmark suite which measures important aspects
E 37
I 37
D 49
\*[lmbench] is a micro-benchmark suite that measures important aspects
E 37
E 12
D 31
of system performance.  The benchmarks are designed to provide 
understanding and insight into the basic building blocks of many
E 31
I 31
D 32
of system performance.  The benchmarks are designed to focus
attention on the basic building blocks of many
E 32
I 32
D 33
of system performance.  The benchmarks are designed to provide 
understanding and insight into the basic building blocks of many
E 33
I 33
of system performance.  The benchmarks are designed to focus
E 49
I 49
\*[lmbench] is a micro-benchmark suite designed to focus
E 49
attention on the basic building blocks of many
E 33
E 32
E 31
common system applications, such as databases, simulations, 
D 12
software development, and networking applications.  In almost all
E 12
I 12
software development, and networking.  In almost all
E 12
cases, the individual tests are the result of analysis and isolation
D 37
of some customer's performance problem.  
E 37
I 37
of a customer's actual performance problem.  
E 37
D 12
.sp .5
E 12
I 12
.\" .SP
E 12
These tools can be, and currently are, used to compare different
D 16
system implementations; the point being to correct areas where 
E 16
I 16
D 37
system implementations; the point being to indicate areas where 
E 16
D 12
one vendor is substantially worse than another.  In several cases,
E 12
I 12
one vendor is substantially worse than another.  
E 37
I 37
D 47
system implementations in order to indicate areas where 
one vendor's product is substantially different than anothers.  
E 47
I 47
system implementations from different vendors.
E 47
E 37
D 16
The results have shown a strong
correlation between memory system performance and overall performance.
I 15
XXX - MP versus uniprocessors?
E 16
E 15
In several cases,
E 12
D 38
the benchmarks have uncovered bugs and design flaws that had not
D 12
yet been recognized as such.
.sp .5
Included in this paper are results from both uniprocessor and multi
D 4
processor systems current as of the summer of 1995.
E 4
I 4
processor systems current as of late 1995.
E 4
.sp .5
Finally, the tools form a useful toolkit from which new benchmarks
may be easily generated.  The software is ``copylefted'' which means
anyone may freely use the software for any reason.
E 12
I 12
D 16
yet been recognized as such.  
E 16
I 16
yet been discovered.
E 38
I 38
the benchmarks have uncovered previously unknown bugs and design flaws.
E 38
E 16
D 50
.\" .SP
I 38
.\" XXX Larry --- can we support this assertion?
E 50
E 38
I 16
The results have shown a strong
correlation between memory system performance and overall performance.
.\" XXX - MP versus uniprocessors?
E 16
D 24
Included in this paper is an extensible database of
E 24
I 24
D 38
Included with \*[lmbench] is an extensible database of
E 24
D 16
results from both uniprocessor and multi
processor systems current as of mid 1995.
E 16
I 16
D 37
results from both uniprocessor and multi-processor 
E 37
I 37
results from both uniprocessor and multiprocessor 
E 37
D 21
systems current as of mid 1995.
E 21
I 21
systems current as of late 1995.
E 38
I 38
\*[lmbench] includes an extensible database of
results from systems current as of late 1995.
I 54
.AE
.if t .MC 3.05i
.FS
This paper first appeared in the January 1996 Usenix conference proceedings.
The version you are reading has new results as well as some corrections.
.FE
E 54
E 38
E 21
E 16
E 12
E 3
D 15
.AE
.sp 2
.if t .2C
E 15
D 12
.FS
D 3
This work was mostly done while the author was an employee of Sun Microsystems
Computer Corporation.
E 3
I 3
This work was started while Larry McVoy was an employee of Sun Microsystems
Computer Corporation.  
E 3
.FE
E 12
.NH 1
Introduction
D 3
.LP
lmbench provides a standard set of measures for the most important aspect
of system performance, and
E 3
I 3
.PP
\*[lmbench]
provides a suite of benchmarks that attempt to measure the most commonly
D 37
found performance bottlenecks in a wide range of systems applications.
E 37
I 37
found performance bottlenecks in a wide range of system applications.
E 37
D 12
\*[lmbench]
measures each subsystem in isolation.  The idea is to calculate the fastest
speed at which one may complete an operation.  In many cases, observed times
may be longer due to cache, memory, bus, and/or processor contention.
E 12
I 12
D 16
The bottlenecks have been identified, isolated, and reproduced in a set
of small micro benchmarks.  
The benchmarks measure 
E 16
I 16
These bottlenecks have been identified, isolated, and reproduced in a set
of small micro-benchmarks, which measure 
E 16
D 37
system latency and bandwidth of data movement between 
E 37
I 37
system latency and bandwidth of data movement among 
E 37
the processor and memory, network, file system, and disk.  
E 12
D 16
.PP
D 12
\*[lmbench]
provides latency measures for system operations 
and both latency and bandwidth measures for data movement.
The measurements focus on system interfaces and facilities
available to the application, so it measures the performance
actually available to users applications.
D 4
The interfaces that
E 4
I 4
The system interfaces used by
E 4
\*[lmbench]
D 4
uses have been carefully chosen to be as portable 
E 4
I 4
have been carefully chosen to be as portable 
E 4
and standard as possible.  
E 12
I 12
The benchmarks are written using standard, portable 
system interfaces and facilities that are both available and commonly
used by applications.  The intent was to produce numbers that real
E 16
I 16
D 37
The intent was to produce numbers that real
E 16
applications could reproduce, rather than produce the frequently
E 37
I 37
The intent is to produce numbers that real
applications can reproduce, rather than the frequently
E 37
D 50
quoted and somewhat less reproduceable marketing performance numbers.
E 50
I 50
quoted and somewhat less reproducible marketing performance numbers.
E 50
E 12
D 16
.PP
E 16
D 12
\*[lmbench]
E 12
I 12
D 31
All readers should gain an intuition about performance
E 31
I 31
D 32
All readers should develop an intuition about performance
E 32
I 32
D 33
All readers should gain an intuition about performance
E 33
I 33
D 37
All readers should develop an intuition about performance
E 33
E 32
E 31
D 16
timing and the cost of system operations from this paper. 
Additionally, \*[lmbench]
E 12
should prove useful to systems architects looking for tests for new
system implementations, to systems engineers doing performance analysis and
regression testing, to customers doing procurement specifications, and
to operating systems researchers seeking data on the performance 
of current systems.
E 16
I 16
timing and the cost of many common system operations from this paper. 
E 37
E 16
.PP
I 12
The benchmarks focus on latency and bandwidth because
D 31
many performance issues are either a latency
E 31
I 31
D 32
most performance issues are either a latency
E 32
I 32
D 33
many performance issues are either a latency
E 33
I 33
D 37
most performance issues are either a latency
E 37
I 37
D 38
most performance issues are a latency
E 37
E 33
E 32
E 31
problem, a bandwidth problem, or some combination of the two.  Each benchmark
E 38
I 38
performance issues are usually caused by latency
problems, bandwidth problems, or some combination of the two.  Each benchmark
E 38
exists because it captures some unique performance problem present in 
one or more important applications.  
D 37
For example, the TCP latency benchmark is an accurate predictor of the 
Oracle distributed lock manager's performance, the memory latency benchmark
gives a strong indication of Verilog simulation performance, and the file
system
latency is frequently the critical path in software development.
E 37
I 37
For example, the TCP latency benchmark is an accurate predictor of the
Oracle distributed lock manager's performance, the memory latency
benchmark gives a strong indication of Verilog simulation performance,
and the file system latency benchmark models a critical path
in software development.
E 37
.PP
I 49
\*[lmbench] was developed to identify and evaluate system performance
D 50
bottlenecks present in many machines in 1993\-1995.  It is entirely
E 50
I 50
bottlenecks present in many machines in 1993-1995.  It is entirely
E 50
possible that computer architectures will have changed and advanced
enough in the next few years to render parts of this benchmark suite
obsolete or irrelevant.
.PP
E 49
D 31
\*[lmbench] is already in wide spread use and has detected unnoticed
D 16
system problems before  first customer ship.  For example, the memory
E 16
I 16
system problems before first customer ship.  For example, the memory
E 16
D 28
latency benchmark indicated that a Sun machine had a much smaller cache
E 28
I 28
latency benchmark indicated that a \s-1SUN\s0 machine had a much smaller cache
E 28
than it really had because of a problem with the memory management
software.
E 31
I 31
D 32
\*[lmbench] is already in widespread use at many sites by both end
users and system designers.  In some cases, \*[lmbench] has provided
the data necessary to discover and correct critical performance
problems before first customer ship.  One problem discovered by
\*[lmbench] was in Sun's memory management software.   There was a bug 
that made all pages map to the same location in the cache, effectively
turning a 512 kilobyte cache into a 4 kilobyte cache.
E 32
I 32
D 33
\*[lmbench] is already in wide spread use and has detected unnoticed
system problems before first customer ship.  For example, the memory
latency benchmark indicated that a \s-1SUN\s0 machine had a much smaller cache
than it really had because of a problem with the memory management
software.
E 33
I 33
\*[lmbench] is already in widespread use at many sites by both end
users and system designers.  In some cases, \*[lmbench] has provided
the data necessary to discover and correct critical performance
D 38
problems before first customer ship.  One problem discovered by
\*[lmbench] was in Sun's memory management software.   There was a bug 
E 38
I 38
D 50
problems before first customer ship.  \*[lmbench] uncovered a
E 50
I 50
problems that might have gone unnoticed.  \*[lmbench] uncovered a
E 50
problem in Sun's memory management software
E 38
that made all pages map to the same location in the cache, effectively
D 44
turning a 512 kilobyte cache into a 4 kilobyte cache.
E 44
I 44
turning a 512 kilobyte (K) cache into a 4K cache.
E 44
E 33
E 32
E 31
.PP
E 12
D 37
\*[lmbench]
is not a MIPS, MFLOPS,
D 14
throughput, saturation, stress, graphics, or multi processor test
E 14
I 14
throughput, saturation, stress, graphics, or multi-processor test
E 14
D 12
suite.\**
.FS
E 12
I 12
suite.  
I 14
D 15
So it does not measure system performance under heavy load, nor does
it measure system performance according to multi-programming level.
Also, it only measures a some system operations and a system's ability
E 15
I 15
It only measures a system's ability
E 37
I 37
\*[lmbench] measures only a system's ability
E 37
E 15
to transfer data between processor, cache, memory, network, and disk.
D 37
It does not measure other parts of the system, such as the graphics subsystem.
E 14
E 12
D 16
It is frequently run on multi processor systems to compare against
E 16
I 16
D 17
It is frequently run on multi-processor systems to compare against
E 17
I 17
It is frequently run on multi-processor (MP) systems to compare against
E 17
E 16
D 14
uniprocessor performance, but it does not take advantage of any multi processor
E 14
I 14
uniprocessor performance, but it does not take advantage of any multi-processor
E 14
D 12
features.
.FE
E 12
I 12
D 13
features.  Future release of the suite will include multi processor
E 13
I 13
D 16
features.  Future releases of the suite will include multi processor
E 16
I 16
features.  Future releases of the suite will include multi-processor
E 37
I 37
It does not measure other parts of the system, such as the graphics subsystem,
nor is it a MIPS, MFLOPS,
throughput, saturation, stress, graphics, or multiprocessor test suite.  
It is frequently run on multiprocessor (MP) systems to compare their performance
against
D 50
uniprocessor performance, but it does not take advantage of any multiprocessor
features.  Future releases of the suite will include multiprocessor
E 50
I 50
uniprocessor systems, but it does not take advantage of any multiprocessor
D 51
features.  Future releases of the suite may include multiprocessor
E 50
E 37
E 16
E 13
specific benchmarks.
E 51
I 51
features.  
E 51
E 12
.PP
I 16
The benchmarks are written using standard, portable 
D 28
system interfaces and facilities that are both available and commonly
used by applications.  
E 28
I 28
system interfaces and facilities commonly
used by applications, so 
E 28
E 16
\*[lmbench]
E 3
D 4
is both portable and comparable over a wide set of Unix systems.\**
.FS
E 4
I 4
D 28
is both portable and comparable over a wide set of Unix systems.
E 4
The tools have been run on at least
E 28
I 28
is portable and comparable over a wide set of Unix systems.
D 31
\*[lmbench] has been run on at least
E 31
I 31
D 32
\*[lmbench] has been run on 
E 32
I 32
D 33
\*[lmbench] has been run on at least
E 33
I 33
\*[lmbench] has been run on 
E 33
E 32
E 31
E 28
AIX,
BSDI,
HP-UX,
IRIX,
Linux,
I 3
FreeBSD,
E 3
NetBSD,
OSF/1,
Solaris,
and
SunOS.
I 3
Part of the suite has been run on Windows/NT as well.
E 3
D 4
.FE
E 4
D 3
lmbench provides latency measures for system operations 
and both latency and bandwidth measures for data movement.
The measurements focus on system interfaces and facilities
available to the application, so it measures the performance
actually available to users applications.
The interfaces that lmbench uses have been carefully chosen to be as portable 
and standard as possible.  It is an explicit intent to measure
widely available, standard interfaces.
lmbench is freely distributed under
E 3
I 3
.PP
\*[lmbench]
is freely distributed under
E 3
D 12
the GNU General Public License, with the additional restriction 
E 12
I 12
the Free Software Foundation's General Public License
.RN Stallman89 ,
with the additional restriction 
E 12
D 8
that results may be reported only if the benchmarks are unmodified.\**
.FS
E 8
I 8
that results may be reported only if the benchmarks are unmodified.
E 8
D 15
For example, the context switch benchmark may not use a \f(CWyield()\fP
E 15
I 15
D 37
For example, the context switch benchmark may not use a \*[yield]
E 15
primitive instead of pipes; the networking benchmarks must use the socket
interfaces, not TLI or some other interface.
E 37
D 8
.FE
E 8
D 12
.NH 2
D 3
Why is lmbench important?
.LP
lmbench is important because it is: relevant, accurate, and portable.
It is relevant because it measures how fast a computer can respond
to control events and how fast it can move or manipulate data.
so it can be used to predict maximum performance
of a wide variety of applications.
It is portable across a wide variety of machines, from PCs to Crays.
It is highly accurate, in some cases it is accurate down to nanoseconds.
E 3
I 3
Motivation for \*[lmbench]
E 12
E 3
D 16
.PP
D 3
The activity of many computer programs can be characterized as bi-modal
(especially in a client-server world): they are either transferring
control or moving data.  Similarly, network activity has been shown
to be bi-modal: lots of small (control) messages, and a few large
(data) messages.  Two examples of important applications which follow
this model are NFS and the Oracle Lock Manager.  The vast majority
of packets are small control or response messages for which latency
is important, but most data is sent in large messages where
bandwidth is crucial.  The Oracle Lock Manager activity consists
solely of small control packets where latency is critical to the
database's performance.
.PP
Obviously, there are some applications which do not fit this model,
particularly some scientific codes where program performance can 
largely be determined by the speed of floating point operations.
However, a significant fraction of applications and important
applications do fit this model and we can (often) predict the
maximum performance ("guaranteed not to exceed" speed) of those
E 3
I 3
D 4
In the author's experience, most performance issues are either a latency
E 4
I 4
D 12
Many performance issues are either a latency
E 4
problem, a bandwidth problem, or some combination of the two.  The set of
benchmarks in
\*[lmbench]
exist because each captures some unique performance problem present in 
one or more important application.  
For example, the TCP latency benchmark is an accurate predictor of the 
Oracle distributed lock manager's performance; the memory latency benchmark
gives a strong indication of Verilog simulation performance.
E 12
I 12
The outline of the rest of this paper is as follows:  \(sc2 describes
prior work, \(sc3 discusses benchmarking issues, \(sc4 describes the 
systems measured, \(sc5 describes bandwidth benchmarks and results,
\(sc6 describes latency benchmarks and results, and then we finsih up
with acknowledgments and conclusions.
E 16
.NH 1
Prior work
E 12
.PP
D 12
The activity of many computer programs can be characterized as bimodal
(especially in a client-server world): they are either transferring
control or moving data.  Similarly, network activity has been shown
to be bimodal: lots of small (control) messages, and a few large
(data) messages.  
.PP
Obviously, there are some applications which do not fit this model,
particularly some scientific codes where program performance can 
largely be determined by the speed of floating point operations.
However, a significant fraction of applications and important
applications do fit this model and we can (often) predict the
maximum performance ("guaranteed not to exceed" speed) of those
E 3
applications.
.NH 2
D 3
What does lmbench measure?
.LP
lmbench measures a systems ability to move data and some basic system
E 3
I 3
What can I learn from \*[lmbench]?
.PP
It is the author's hope that readers will gain an intuition about performance
timing and the cost of system operations from this paper. 
Many developers design
new software packages with little idea of the cost of systems facilities
that the packages use.  Frequently, the new software does not have the
expected performance; it is rare that the problem is that the software
D 4
is too fast.  By understanding what things cost, by developing an 
intuition about system performance, it is believed that more developers
will reach their performance goals.
E 4
I 4
is too fast.  \*[lmbench] can be used by programmers to develop an
intuition about the cost of various operations, so they can better
understand the tradeoffs involved in program design.  Also, they
can better understand the system-imposed limitations on program
performance.
E 4
.NH 2
What does \*[lmbench] measure?
.PP
\*[lmbench] measures a systems ability to move data and some basic system
E 3
operations.  
D 3
lmbench measures system latency and bandwidth for data movement between 
E 3
I 3
\*[lmbench] measures system latency and bandwidth for data movement between 
E 3
the processor and memory, network, file system, and disk.  
D 3
For some basic system operations, lmbench measures the operation latency.
The intent is for lmbench to provide performance information for the
E 3
I 3
For some basic system operations, \*[lmbench] measures the operation latency.
The intent is for \*[lmbench] to provide performance information for the
E 3
most important aspects of system performance.
E 12
I 12
D 31
There has been a great deal of work done on benchmarking and performance
analysis.  There are too many other benchmark suites to list all of
E 31
I 31
D 32
Benchmarking and performance analysis is not a new endeavor.
There are too many other benchmark suites to list all of
E 32
I 32
D 33
There has been a great deal of work done on benchmarking and performance
analysis.  There are too many other benchmark suites to list all of
E 33
I 33
Benchmarking and performance analysis is not a new endeavor.
There are too many other benchmark suites to list all of
E 33
E 32
E 31
D 37
them here.  Instead, we concentrate on the most similar, and show how
this work differentiates itself.
E 37
I 37
D 49
them here.  Instead, we compare \*[lmbench]
to the set of similar benchmarks.
E 49
I 49
them here.  We compare \*[lmbench]
to a set of similar benchmarks.
E 49
E 37
D 22
.BU "I/O (disk) benchmarks."
E 22
I 22
.BU "I/O (disk) benchmarks" :
E 22
IOstone
D 38
.RN Park90 ,
and IObench
E 38
I 38
.RN Park90 
D 50
is a simplistic benchmark that no longer measures any useful
feature of modern systems.
E 50
I 50
wants to be an I/O benchmark, but actually measures the memory
subsystem; all of the tests fit easily in the cache.
E 50
IObench
E 38
.RN Wolman89 
D 31
are attempts to provide disk and file benchmarks.  
A previous work,
E 31
I 31
D 32
are attempts at disk and file benchmarks.  
For previous work,
E 32
I 32
D 33
are attempts to provide disk and file benchmarks.  
A previous work,
E 33
I 33
D 38
are attempts at disk and file benchmarks.  
D 37
For previous work,
E 37
I 37
For previous work
E 38
I 38
D 50
is a systematic file system and disk benchmark, but it is too
E 50
I 50
is a systematic file system and disk benchmark, but it is 
E 50
complicated and unwieldy.
In
E 38
E 37
E 33
E 32
E 31
.RN McVoy91 
D 31
had occasion to review similar benchmarks.
All were found lacking in that they took too long to run and 
were too complex a solution to a fairly simple problem.  As a 
D 15
result, a simple I/O benchmark, \f(CWlmdd\fP, was written.
E 15
I 15
result, a simple I/O benchmark, \*[lmdd], was written.
E 31
I 31
D 32
we reviewed many I/O benchmarks and found all of them to be
lacking in that they took too long to run and 
were too complex a solution to a fairly simple problem.  We wrote a 
small, simple I/O benchmark, \*[lmdd].
E 32
I 32
D 33
had occasion to review similar benchmarks.
All were found lacking in that they took too long to run and 
were too complex a solution to a fairly simple problem.  As a 
result, a simple I/O benchmark, \*[lmdd], was written.
E 33
I 33
D 38
we reviewed many I/O benchmarks and found all of them to be
lacking in that they took too long to run and 
E 38
I 38
we reviewed many I/O benchmarks and found them all
lacking because they took too long to run and 
E 38
were too complex a solution to a fairly simple problem.  We wrote a 
D 34
small, simple I/O benchmark, \*[lmdd].
E 33
E 32
E 31
E 15
D 16
This benchmark, which is included in \f(CWlmbench\fP,
E 16
I 16
This benchmark, which is included in \*[lmbench],
E 34
I 34
D 38
small, simple I/O benchmark, \*[lmdd] that is part of \*[lmbench] and
E 38
I 38
small, simple I/O benchmark, \*[lmdd] that
E 38
E 34
E 16
D 31
measures I/O metrics (sequential and random) far 
E 31
I 31
D 32
measures sequential and random I/O far 
E 32
I 32
D 33
measures I/O metrics (sequential and random) far 
E 33
I 33
measures sequential and random I/O far 
E 33
E 32
E 31
D 15
faster than either IOstone or IObench.  The results from \f(CWlmdd\fP
E 15
I 15
D 50
faster than either IOstone or IObench.  The results from \*[lmdd]
E 15
D 31
have proven to be as accurate as any of the other benchmarks.
E 31
I 31
D 32
have proven to be as accurate as any of the other I/O benchmarks.
E 32
I 32
D 33
have proven to be as accurate as any of the other benchmarks.
E 33
I 33
D 37
have proven to be as accurate as any of the other I/O benchmarks.
E 37
I 37
have proven to be as accurate as those from any of the other I/O benchmarks.
E 50
I 50
faster than either IOstone or IObench.  As part of 
.RN McVoy91
the results from \*[lmdd] were checked against IObench (as well as some other
Sun internal I/O benchmarks).   \*[lmdd] proved to be more accurate than any
of the other benchmarks.
E 50
E 37
E 33
E 32
E 31
At least one disk vendor
D 15
routinely uses \f(CWlmdd\fP to do performance testing of their disk drives.
E 15
I 15
D 37
routinely uses \*[lmdd] to do performance testing of their disk drives.
E 37
I 37
routinely uses \*[lmdd] to do performance testing of its disk drives.
E 37
E 15
.SP
D 50
Chen and Patterson's work 
E 50
I 50
Chen and Patterson
E 50
D 20
.RN Chen94 ,
.RN Chen93 
E 20
I 20
D 37
.RN Chen93,Chen94
E 37
I 37
D 38
.RN Chen93 Chen94
E 38
I 38
.RN "Chen93, Chen94"
E 38
E 37
E 20
D 16
is a good effort towards serious I/O benchmarking.
We do not attempt to be as exhaustive as their work.  Our efforts were 
directed at measuring the maximum single stream I/O bandwidth and/or
the overhead cost of a single disk I/O.  
Peter Chen and David Patterson addressed many of the shortcomings
E 12
D 12
.NH 2
How the results can be used
D 3
.LP
The information provided by lmbench can be used for a variety of purposes.
E 3
I 3
.PP
The information provided by \*[lmbench] can be used for a variety of purposes.
E 3
For example, the network latency information can be used to estimate an 
upper bound on the performance of the Oracle lock manager whose performance
is limited by small-message network traffic.
D 3
lmbench has also detected system problems, for example the memory latency
E 3
I 3
\*[lmbench] has also detected system problems, for example the memory latency
E 3
benchmark indicated that a Sun machine had a much smaller cache than
it really had because of a problem with the memory management system.
D 3
Also, lmbench sometimes measures the performance of several mechanisms
E 3
I 3
Also, \*[lmbench] sometimes measures the performance of several mechanisms
E 3
for accomplishing the same task, and so it can be used to choose the
fastest mechanism.
D 4
.PP
D 3
Most importantly, lmbench can be used by programmers to develop an
E 3
I 3
Most importantly, \*[lmbench] can be used by programmers to develop an
E 3
intuition about the cost of various operations, so they can better
understand the tradeoffs involved in program design.  Also, they
can better understand the system-imposed limitations on program
performance.
E 4
I 3
D 8
.NH 2
Outline
E 8
.PP
I 4
D 6
XXX - this is bullshit.
E 6
I 6
D 8
XXX - this is bullshit.  Carl, please fix this.
E 6
.PP
E 4
The rest of this paper proceeds as follows: we separate the benchmarks into
bandwidth and latency categories.  For each benchmark, describe the 
subsystem being measured, explain why it is being measured,
explain how it is measured, and show results for
some current systems.  We also point out relevant comparisons to other 
metrics.
We finish by discussing some future extensions to
\*[lmbench]
The paper is followed by an appendix which is a ``cheat sheet'' showing all
results for all systems.
.PP
E 8
XXX - go through all the man pages and look for extra info there.
E 3
.NH 1
I 4
D 11
Prior work
E 11
I 11
Prior work - XXX rework this
E 11
.PP
There has been a great deal of work done on benchmarking and performance
D 8
analysis.  However, each of the existing systems suffers from one or
D 6
more flaws.
E 6
I 6
more flaws.  In most cases, the benchmarks measure things at to high of a 
E 8
I 8
analysis.  However, the existing benchmark suites tend to measure other
quantities, contain a subset of the metrics in \*[lmbench], or
suffer from one or more flaws.  
In most cases, the benchmarks measure things at to high of a 
E 8
level, and do not do enough to point out the real performance issues.
E 6
.PP
D 6
Contrast with Netperf.
E 6
I 6
D 8
Contrast with Netperf.  The main problems with netperf: it is just a 
networking benchmark, it is not even a complete networking benchmark, 
E 8
I 8
Netperf is a networking benchmark produced by Rick Jones of Hewlett-Packard.\**
.FS
D 9
netperf is available at \fCWhttp://www.cup.hp.com/netperf/NetperfPage.html\fP
E 9
I 9
for information on netperf, including source code, see:
\f(CWhttp://www.cup.hp.com/netperf/NetperfPage.html\fP
E 9
.FE
The main problem with netperf is that it just measures
networking benchmark.
It is not even a complete networking benchmark, since
E 8
it doesn't break the networking problems down enough to teach
you anything, and the results are organized in an incomprehensible 
D 8
fashion.
E 8
I 8
fashion.  It does measure bandwidth, latency, and CPU utilization
for TCP, UDP, DLPI, and ATM connections.  (XXX --- Larry is this true?)
E 8
E 6
.PP
D 6
Contrast with Ousterhout.
.NH 1
E 4
D 3
Data movement
.LP
E 3
I 3
Benchmarking notes
E 6
I 6
D 8
Contrast with Ousterhout.  Ousterhout's stuff is fairly naive.  He
uses \*[getpid] as a measure of kernel entry.  That is a poor choice
E 8
I 8
John Ousterhout 
E 12
I 12
of existing I/O benchmarks by measuring I/O performance under a
E 16
I 16
D 31
is a good effort towards serious I/O benchmarking, 
measuring I/O performance under a
E 31
I 31
D 32
measures I/O performance under a
E 32
I 32
D 33
is a good effort towards serious I/O benchmarking, 
measuring I/O performance under a
E 33
I 33
D 50
measures I/O performance under a
E 50
I 50
measure I/O performance under a
E 50
E 33
E 32
E 31
E 16
D 37
variety of workloads which are automatically varied to test the
E 37
I 37
variety of workloads that are automatically varied to test the
E 37
range of the system's performance.  
I 16
D 31
Our efforts were 
directed at measuring the maximum single stream I/O bandwidth and/or
the overhead cost of a single disk I/O.  
E 31
I 31
D 32
Our efforts differed in that we were more interested in the CPU overhead
of a single request, rather than the capacity of the system as a whole.
E 32
I 32
D 33
Our efforts were 
directed at measuring the maximum single stream I/O bandwidth and/or
the overhead cost of a single disk I/O.  
E 33
I 33
D 50
Our efforts differed in that we were more interested in the CPU overhead
E 50
I 50
Our efforts differ in that we are more interested in the CPU overhead
E 50
of a single request, rather than the capacity of the system as a whole.
E 33
E 32
E 31
E 16
D 22
.BU "Berkeley Software Distribution's microbench suite."
E 22
I 22
D 38
.BU "Berkeley Software Distribution's microbench suite" :
E 38
I 38
D 44
.BU "Berkeley Software Distribution's micro-benchmark suite" :
E 44
I 44
.BU "Berkeley Software Distribution's microbench suite" :
E 44
E 38
E 22
D 16
The BSD effort generated extensive set of 
E 16
I 16
The BSD effort generated an extensive set of 
E 16
test benchmarks to do regression testing (both quality and performance)
of the BSD releases.  
D 37
We did not use this as a basis for our work (although we stole ideas)
E 37
I 37
We did not use this as a basis for our work (although we used ideas)
E 37
for the following reasons:
D 37
(a) missing tests, such as memory latency,
E 37
I 37
D 50
(a) missing tests \- such as memory latency,
E 50
I 50
(a) missing tests \(em such as memory latency,
E 50
E 37
(b) too many tests, the results tended to be obscured under a mountain
of numbers,
D 37
and (c) wrong copyright; we wanted the
E 37
I 37
D 50
and (c) wrong copyright \- we wanted the
E 50
I 50
and (c) wrong copyright \(em we wanted the
E 50
E 37
Free Software Foundation's General Public License.
D 15
.BU "John Ousterhout's Operating System benchmark"
E 15
I 15
D 22
.BU "John Ousterhout's Operating System benchmark."
E 22
I 22
D 23
.BU "John Ousterhout's Operating System benchmark" :
E 23
I 23
.BU "Ousterhout's Operating System benchmark" :
E 23
E 22
E 15
E 12
D 9
.[
Ousterhout90
.]
E 9
I 9
.RN Ousterhout90
E 9
D 12
proposed several system benchmarks to measure system call
E 12
I 12
proposes several system benchmarks to measure system call
E 12
latency, context switch time, and file system performance.
D 12
Unfortunately, he
uses \*[getpid] as a measure of kernel entry, which is a poor choice
E 8
because it frequently does not reflect the real cost of entering the
kernel.  Many operating systems implement a series of simple (think
of them as read only) system calls as fast traps which do nothing but
D 8
grab the data needed and get back out again.  
E 8
I 8
grab the data needed and get back out again, and some systems have 
optimized \*[getpid] by putting the information in a read-only page
accessible from user-land turning \*[getpid] into a standard library
routine instead of a system call.  (XXX - reference?)
E 12
I 12
D 49
We used the same ideas as a basis for our work,  but we went
quite a bit farther.  We measured a more complete set of
E 49
I 49
We used the same ideas as a basis for our work,  while trying to
go farther.  We measured a more complete set of
E 49
D 16
primitives, including some hardware measurements, we went into greater depth
on some of the tests, such as context switching, and we went to great
E 16
I 16
D 37
primitives, including some hardware measurements, went into greater depth
on some of the tests, such as context switching, and went to great
E 37
I 37
primitives, including some hardware measurements; went into greater depth
on some of the tests, such as context switching; and went to great
E 37
E 16
D 49
lengths to make the benchmark portable and extensible so that others
could benefit from our work.
E 49
I 49
lengths to make the benchmark portable and extensible.
E 49
D 15
.BU "Networking benchmarks"
E 15
I 15
D 22
.BU "Networking benchmarks."
E 22
I 22
.BU "Networking benchmarks" :
E 22
E 15
D 38
\f(CWNetperf\fP is a networking benchmark produced by Rick Jones of
D 16
Hewlett-Packard It measures
E 16
I 16
Hewlett-Packard.  It measures
E 16
D 31
networking metrics such as bandwidth and latency.  We considered including this
E 31
I 31
D 32
networking bandwidth and latency.  We considered including this
E 32
I 32
D 33
networking metrics such as bandwidth and latency.  We considered including this
E 33
I 33
networking bandwidth and latency.  We considered including this
E 33
E 32
E 31
D 13
as part of \f(CWlmbench\fP but did not because it was too complex and was
E 13
I 13
D 15
as part of \f(CWlmbench\fP but did not because it was too complex and 
E 13
measured only networking.
E 15
I 15
D 16
as part of \f(CWlmbench\fP but did not because we were able to write a smaller,
less complex benchmark that produced the same results.
E 16
I 16
as part of \*[lmbench] but did not because we were able to write a smaller,
E 38
I 38
\f(CWNetperf\fP measures networking bandwidth and latency and 
D 44
was by Rick Jones of Hewlett-Packard.
E 44
I 44
was written by Rick Jones of Hewlett-Packard.
E 44
D 49
We considered including \f(CWnetperf\fP
in \*[lmbench] but did not because we were able to write a smaller,
E 38
less complex benchmark that produced similar results.
E 49
I 49
\*[lmbench] includes a smaller,
less complex benchmark that produces similar results.
E 49
E 16
E 15
.SP
D 49
\f(CWttcp\fP is a widely used benchmark in the Internet community.  It has
D 37
knowledge of \f(CWTCP\fP and is also a bit too complex for our tastes.
E 37
I 37
knowledge of \f(CWTCP\fP and is a bit too complex for our tastes.
E 49
I 49
\f(CWttcp\fP is a widely used benchmark in the Internet community.  
E 49
E 37
Our version of the same benchmark 
D 50
routinely delivers bandwidth numbers that are within 1-2% of the numbers 
E 50
I 50
routinely delivers bandwidth numbers that are within 2% of the numbers 
E 50
quoted by \f(CWttcp\fP.
I 23
D 24
.BU "McCalpin's streams benchmark" :
E 24
I 24
.BU "McCalpin's stream benchmark" :
E 24
.RN McCalpin95
has memory bandwidth measurements and results for a large number of
D 37
high end systems.  We did not use these because we discovered them after
E 37
I 37
D 38
high\-end systems.  We did not use these because we discovered them only after
E 37
D 24
we had results using our versions.  We will probably use McCalpin/s benchmarks 
E 24
I 24
we had results using our versions.  We will probably use McCalpin's benchmarks 
E 38
I 38
D 47
high\-end systems.  
E 47
I 47
high-end systems.  
E 47
I 44
We did not use these because we discovered them only after
we had results using our versions.
E 44
We will probably include McCalpin's benchmarks in \*[lmbench]
E 38
E 24
in the future.
E 23
E 12
E 8
E 6
.PP
D 6
Talk about timing issues, granularity, etc?  The most important is that
D 4
gettimeofday() can be as bad as 10 millisecond granularity - that means 
E 4
I 4
\*[gettimeofday] can be as bad as 10 millisecond granularity - that means 
E 4
that benchmarks should run at least 500 milliseconds in order to limit the 
D 4
gettimeofday() effects.
E 4
I 4
\*[gettimeofday] effects.
E 6
I 6
D 12
Ousterhout's context switch benchmark does not measure context switch
time, it measures context switch time plus the overhead of some system
calls.  In many of the systems measured by \*[lmbench], the overhead 
varies from 30% to 300% of the context switch time.  In addition, 
Ousterhout did not vary the number or the size of the processes
being measured.  Not only are his numbers fairly meaningless, given
that the amount of overhead time is not constant from system to
system, but they are also unrealistic - there are no real applications
that do nothing but context switch back and forth.  Real applications
do work before switching back.
E 12
I 12
D 37
In summary, we ``rolled our own'' because we wanted simple, portable
E 37
I 37
In summary, we rolled our own because we wanted simple, portable
E 37
D 31
benchmarks that accurately measured a wide variety of metrics that we
E 31
I 31
D 32
benchmarks that accurately measured a wide variety of operations that we
E 32
I 32
D 33
benchmarks that accurately measured a wide variety of metrics that we
E 33
I 33
benchmarks that accurately measured a wide variety of operations that we
E 33
E 32
E 31
D 37
considered crucial to performance on today's systems.  While portions of
other benchmark suites include similar work, none include all of it,
E 37
I 37
consider crucial to performance on today's systems.  While portions of
other benchmark suites include similar work, none includes all of it,
E 37
few are as portable, and almost all are far more complex.  Less filling,
tastes great.
.NH 1
Benchmarking notes
.NH 2
Sizing the benchmarks
E 12
E 6
E 4
.PP
D 6
XXX - I haven't checked this lately, I should go do that.
E 6
I 6
D 12
Ousterhout's file system benchmark may be interesting but it does nothing
to help systems designers identify the file system performance bottlenecks.
For example, if an engineer wishes to improve copy speed, is the problem
the memory copy, the networking, the disks, the file creation latency,
what?  In \*[lmbench], we attempt to present results for each of the
basic primitives that make up common, higher level operations.  An
intelligent engineer should be able to look at some subset of the 
\*[lmbench] results and predict file copy speed for both local and
remote operations.  Ousterhout was pointing fingers without doing the
D 8
hardware of isolating the performance problems.
E 8
I 8
hard work of isolating the performance problems.
E 12
I 12
D 16
One of the problems with benchmarking is the appropriate sizing of 
various benchmark parameters.  For example, memory to memory copy
E 16
I 16
The proper sizing of various benchmark parameters is crucial to ensure
that the benchmark is measuring the right component of system performance.  
D 37
For example, memory to memory copy
E 16
speeds are dramatically affected by the location of the data; if
E 37
I 37
D 47
For example, memory\-to\-memory copy
E 47
I 47
For example, memory-to-memory copy
E 47
speeds are dramatically affected by the location of the data: if
E 37
D 16
the data is in a cache, the performance may be as much as ten times
better than if the data is in memory.
On the other hand, if a system has limited memory, a memory to memory
copy benchmark may start paging to disk, and be slowed to such an extent
E 16
I 16
the size parameter is too small so
the data is in a cache, then the performance may be as much as ten times
faster than if the data is in memory.
On the other hand, if the memory size parameter is too big so the data
is paged to disk, then performance may be slowed to such an extent
E 16
D 37
that the benchmark appears to ``never finish.''
E 37
I 37
D 50
that the benchmark seems to ``never finish.''
E 50
I 50
that the benchmark seems to `never finish.'
E 50
E 37
E 12
E 8
E 6
.PP
D 4
XXX - on all of the bandwidth ones talk about how much data was moved
E 4
I 4
D 6
XXX - on all of the bandwidth talk about how much data was moved
E 4
and why that size was chosen.
E 6
I 6
D 8
Constrast with LADDIS?
E 8
I 8
D 12
LADDIS
D 9
.[
Shein89
.]
E 9
I 9
.RN Shein89
E 9
was developed as a benchmark of NFS performance.  It would
provide detailed statistics on the frequency and performace of each
NFS operation from the client's point of view.  It enabled users to 
compare NFS performance across platforms, and it gave system designers
some insights into the performance of each NFS operation.  However, it
did not provide further insight to the limits on NFS performance.
E 12
I 12
D 37
\*[lmbench] takes the following approach to handle the cache and memory
E 37
I 37
\*[lmbench] takes the following approach to the cache and memory
E 37
size issues:
.BU
D 16
All of the benchmarks that may be affected [XXX - effected?]
E 16
I 16
D 37
All of the benchmarks that may be affected
E 37
I 37
All of the benchmarks that could be affected
E 37
E 16
by cache size are run in a loop,
with increasing sizes (typically powers of two) until some maximum size
is reached.  The results may then be plotted to see where the benchmark
no longer fits in the cache.
.BU
The benchmark verifies that there is sufficient memory to run all of the
benchmarks in main memory.  A small test program allocates as much memory
as it can, clears the memory,
and then strides through that memory a page at a time, timing
each reference.  If any reference takes more than a few microseconds, the
D 37
page was not in memory.  The test program starts small and works forward
E 37
I 37
page is no longer in memory.  The test program starts small and works forward
E 37
until either enough memory is seen as present or the memory limit is reached.
.NH 2
Compile time issues
E 12
E 8
E 6
.PP
I 6
D 8
Constrast with IOstone?  If so, just steal what patterson wrote.
E 8
I 8
D 12
IOstone
D 9
.[
Park90
.]
E 9
I 9
.RN Park90
E 9
was originally developed by Arvin Park nearly ten years ago
as a simple benchmark of file system performance.  However, it has
not withstood the test of time, since it uses a relatively small
file to test I/O bandwidth.  Most modern systems can hold the entire
file in the file system buffer cache, so the benchmark is now roughly
equivalent to the file re-read bandwidth benchmark in \*[lmbench].
E 12
I 12
D 16
The compiler of choice is the GNU C compiler, \f(CWgcc\fP.  It consistently
E 16
I 16
D 37
The compiler of choice is the GNU C compiler, \*[gcc].  It consistently
E 16
D 14
produced the most reproduceable results.
E 14
I 14
D 15
produced the most reproduceable results across platforms.  Vendor
compilers vary widely, unless we use a standard compiler across all
the platforms it is hard to ensure that similar code is being
generated across all the platforms.  In some cases vendor compilers
generated much worse code than \f(CWgcc\fP unless high levels of 
optimization were used.  Sometimes the vendor compilers would optimize
out the (dummy) operations used by the benchmark.
E 15
I 15
produced the most reproduceable results across platforms.  
E 37
I 37
D 38
The compiler we chose is the GNU C compiler, \*[gcc].  It consistently
gave the most reproduceable results across platforms.  
E 38
I 38
D 44
The GNU C compiler, \*[gcc], is the compiler of choice because 
E 44
I 44
The GNU C compiler, \*[gcc], is the compiler we chose because 
E 44
D 50
it gave the most reproduceable results across platforms.  
I 44
D 49
If \*[gcc] was not present, we used the vendor suppiled \f(CWcc\fP.
E 49
I 49
When \*[gcc] was not present, we used the vendor suppiled \f(CWcc\fP.
E 50
I 50
it gave the most reproducible results across platforms.  
When \*[gcc] was not present, we used the vendor-supplied \f(CWcc\fP.
E 50
E 49
E 44
All of the benchmarks were compiled with optimization \f(CW-O\fP
except 
the benchmarks that calculate clock speed and the context switch times,
which must be compiled without optimization in order to produce
correct results.  No other optimization flags were enabled because
we wanted results that would be commonly seen by application writers.
E 38
E 37
E 15
E 14
E 12
E 8
.PP
D 8
Constrast with Chen/Patterson's stuff?
E 8
I 8
D 12
In 
D 9
.[
Chen94a
.]
E 9
I 9
.RN Chen94a
E 9
Peter Chen and David Patterson addressed many of the shortcomings
of existing I/O benchmarks by measuring I/O performance under a
variety of workloads which are automatically varied to test the
range of the system's performance.  This is the best pure I/O
benchmark currently available.  It has five workload parameters
and it automatically varies the parameters to explore the
system performance over the workload space.  However, it is only
an I/O benchmark and does not give any insights to other parts of
the system.
E 8
.NH 1
Benchmarking notes
E 12
I 12
D 16
All of the benchmarks were compiled with -O with the exception of the
E 16
I 16
D 38
All of the benchmarks were compiled with optimization \f(CW-O\fP with 
D 37
the exception of the
E 37
I 37
the exception of 
E 37
E 16
D 14
the benchmarks that calculate megahertz and the context switch times.
E 14
I 14
the benchmarks that calculate clock speed and the context switch times.
E 14
Those two must be compiled without optimization in order to produce
correct results.  No other optimization flags were turned on.  This was
D 28
in keeping with the desire to acheive results that would be commonly
E 28
I 28
in keeping with the desire to achieve results that would be commonly
E 28
seen by application writers.
E 12
.PP
I 12
D 37
All of the benchmarks were linked in whatever is the default manner of
the target system.  For most, maybe all, systems this meant that the
E 37
I 37
All of the benchmarks were linked in the default manner of
the target system.  For most if not all systems, this meant that the
E 37
binaries were linked with shared libraries.   
E 38
I 38
All of the benchmarks were linked using the default manner of
the target system.  For most if not all systems, the
binaries were linked using shared libraries.   
E 38
E 12
.NH 2
I 15
D 16
Multi processor issues
E 16
I 16
D 37
Multi-processor issues
E 37
I 37
Multiprocessor issues
E 37
E 16
.PP
D 16
All of the multi processor systems ran the benchmarks in the same way as
E 16
I 16
D 37
All of the multi-processor systems ran the benchmarks in the same way as
E 37
I 37
All of the multiprocessor systems ran the benchmarks in the same way as
E 37
E 16
D 39
the uniprocessor systems.  Some systems have ways of pinning processes
E 39
I 39
the uniprocessor systems.  Some systems allow users to pin processes
E 39
to a particular CPU, which sometimes results in better cache reuse.  We
D 39
did not do that because it defeats the usefulness of the MP scheduler.
E 39
I 39
do not pin processes because it defeats the MP scheduler.
E 39
D 16
XXX - I should do this on an IP19 and mark it as pinned.
E 16
I 16
.\" XXX - I should do this on an IP19 and mark it as pinned.
E 16
D 37
In certain cases, this decision yields interesting results.  More on this
later.
E 37
I 37
In certain cases, this decision yields interesting results discussed later.
D 44
.br
.di bigtable
.ev keep
.ps 8
.vs 9
.so systems.tbl
.ps \n[PS]
.vs \n[VS]
.nr FIGURE \n[FIGURE]+1
.ce 1
.SP
\fBFigure \n[FIGURE].\ \ System description\fP
.SP
.di
.ev
.nr WHEN \n[dn]+\n[FM]
.de print*table
.br
.nf
.bigtable
.ne 2
.wh -\n[WHEN]u skip*page
.fi
..
.de skip*page
.bp
.wh -\n[WHEN]u
..
.wh -\n[WHEN]u print*table
E 44
E 37
.NH 2
E 15
I 12
Timing issues
.LP
I 15
.sp -.5
E 15
D 22
.BU
D 15
\fIClock resolution\fP:
E 15
I 15
\fIClock resolution\fR:
E 22
I 22
.BU "Clock resolution" :
E 22
E 15
The benchmarks measure the elapsed time by reading the system clock via the
D 39
\*[gettimeofday] interface.  On some systems, this interface has a resolution
E 39
I 39
\*[gettimeofday] interface.  On some systems this interface has a resolution
E 39
of 10 milliseconds, a long time relative to many of the benchmarks which
D 15
have results measured in 10s to 100s of \(*mseconds.  To compensate for 
E 15
I 15
D 28
have results measured in 10s to 100s of microseconds.  To compensate for 
E 28
I 28
have results measured in tens to hundreds of microseconds.  To compensate for 
E 28
E 15
D 16
the coarse clock resolution, the benchmarks are hand tuned to run over
many clock ticks, such that any resolution problems become minimized.  
D 13
Typically, this is done timing the operation in a small loop, sometimes
E 13
I 13
Typically, this is done by timing the operation in a small loop, sometimes
E 13
unrolled if the operation is exceedingly fast, and then dividing out the
D 13
loop count from the resulting time delta.
E 13
I 13
loop count from the resulting time delta.  We run the loop long
enough to overcome any clock resolution problems.
E 16
I 16
D 44
the coarse clock resolution, the benchmarks are hand tuned to measure
D 39
many operations within a single time interval lasting for many clock ticks.
Typically, this is done by executing the operation in a small loop, sometimes
unrolled if the operation is exceedingly fast, and then dividing 
the loop time by the loop count.  
E 39
I 39
many operations within a single time interval lasting many clock ticks.
Typically, the benchmarks execute the operation in a small loop, sometimes
unrolling the loop if the operation is exceedingly fast, and then dividing 
the total time by the loop count.  
E 44
I 44
D 47
the coarse clock resolution, the benchmarks are hand\-tuned to measure
E 47
I 47
the coarse clock resolution, the benchmarks are hand-tuned to measure
E 47
many operations within a single time interval lasting for many clock ticks.
Typically, this is done by executing the operation in a small loop, sometimes
unrolled if the operation is exceedingly fast, and then dividing 
the loop time by the loop count.  
E 44
E 39
E 16
E 13
D 22
.BU
\fICaching\fP:
E 22
I 22
.BU Caching :
E 22
If the benchmark expects the data to be in the cache, the benchmark is 
D 44
typically run several times; only the last result recorded. 
E 44
I 44
typically run several times; only the last result is recorded. 
E 44
.SP
D 16
If the benchmark does not want the cache to be used, the size chosen is
large enough that any caches are defeated.  For example, the
E 12
D 12
Sizing the benchmarks
.PP
E 6
D 4
Talk about auto sizing to bigger cache machines.
E 4
I 4
One of the problems with benchmarking is the appropriate sizing of 
various benchmark parameters.  For example, memory to memory copy
D 8
speeds are daramatically affected by the location of the data; if
E 8
I 8
speeds are dramatically affected by the location of the data; if
E 8
the data is in a cache, the performance may be as much as ten times
better than if the data is in memory.
On the other hand, if a system has limited memory, a memory to memory
copy benchmark may start paging to disk, and be slowed to such an extent
that the benchmark appears to ``never finish.''
E 4
.PP
I 4
\*[lmbench] takes the following approach to handle the cache and memory
size issues:
D 6
.IP \(bu
E 6
I 6
D 10
.IP \(bu 2
E 10
I 10
.BU
E 10
E 6
All of the benchmarks that may be effected by cache size are run in a loop,
with increasing sizes (typically powers of two) until some maximum size
is reached.  The results may then be plotted to see where the benchmark
no longer fits in the cache.
D 10
.IP \(bu
E 10
I 10
.BU
E 10
The benchmark verifies that there is sufficient memory to run all of the
benchmarks in main memory.  A small test program allocates as much memory
as it can, clears the memory,
and then strides through that memory a page at a time, timing
each reference.  If any reference takes more than a few microseconds, the
page was not in memory.  The test program starts small and works forward
until either enough memory is seen as present or the memory limit is reached.
I 6
.NH 2
Compile time issues
E 6
.PP
E 4
D 6
Make sure that all of the figures give units.
E 6
I 6
The compiler of choice is the GNU C compiler, \f(CWgcc\fP.  It consistently
produced the most reproduceable results.
E 6
.PP
D 6
Talk about dynamic linking.
E 6
I 6
All of the benchmarks were compiled with -O with the exception of the
the benchmarks that calculate megahertz and the context switch times.
Those two must be compiled without optimization in order to produce
correct results.  On Silicon Graphics 64bit operating systems, the
binaries were compiled into 32bit binaries so as to be comparable 
with earlier SGI results.
E 6
.PP
D 6
Talk about cc flags.
E 6
I 6
All of the benchmarks were linked in whatever is the default manner of
the target system.  For most, maybe all, systems this meant that the
binaries were linked with shared libraries.   One of the test programs
twice, the second time it
is linked ``statically,'' i.e., without shared libraries.  That program
is used to show the shared library startup time overhead.
D 10
.PP
E 10
.NH 2
Timing issues
.PP
Most of the tests have implied knowledge of various Unix timing hard 
coded into the test.  
D 10
.IP \(bu 2
E 10
I 10
.BU
E 10
\fIClock resolution\fP:
The benchmarks measure the elapsed time by reading the system clock via the
\*[gettimeofday] interface.  On some systems, this interface has a resolution
of 10 milliseconds, a long time relative to many of the benchmarks which
D 7
have results measured in 10s to 100s of \*(mseconds.  To compensate for 
E 7
I 7
have results measured in 10s to 100s of \(*mseconds.  To compensate for 
E 7
the coarse clock resolution, the benchmarks are hand tuned to run over
many clock ticks, such that any resolution problems become minimized.  
Typically, this is done timing the operation in a small loop, sometimes
unrolled if the operation is exceedingly fast, and then dividing out the
loop count from the resulting time delta.
D 10
.IP \(bu
E 10
I 10
.BU
E 10
\fICaching\fP:
If the benchmark expects the data to be in the cache, the benchmark is 
typically run twice or even three times, based on experience, with 
only the last result recorded. 
D 10
.sp .5
E 10
I 10
.SP
E 10
D 8
If the benchmark does not want the cache to aid, the size choosen is
E 8
I 8
If the benchmark does not want the cache to aid, the size chosen is
E 8
large enough that any caches are defeated.  For example, the
E 12
\*[bcopy] benchmark by default copies 8 megabytes to 8 megabytes.
That largely defeats any second level cache today.  Note that the 
E 16
I 16
D 39
If the benchmark does not want the cache to be used, the size parameter is 
larger than the cache to defeat caching.  For example, the
E 39
I 39
If the benchmark does not want to measure cache performance it sets
the size parameter larger than the cache.  For example, the
E 39
\*[bcopy] benchmark by default copies 8 megabytes to 8 megabytes,
D 39
which largely defeats any second level cache today.  Note that the 
E 39
I 39
D 44
which usually defeats current second level caches.  Note that the 
E 44
I 44
D 47
which largely defeats any second\-level cache in use today.  (Note that the 
E 47
I 47
which largely defeats any second-level cache in use today.  (Note that the 
E 47
E 44
E 39
E 16
benchmarks are not trying to defeat the file or process page cache,
D 12
only the caches which are faster than main memory.
E 12
I 12
D 44
only the hardware caches.
E 44
I 44
only the hardware caches.)
E 44
E 12
D 10
.IP \(bu
E 10
I 10
D 22
.BU
E 10
\fIVariability\fP:
E 22
I 22
D 48
.BU Variability :
I 24
D 28
.PP
E 28
E 24
E 22
Some benchmarks, most notably the context switch benchmark, had a tendency
D 20
to vary quite a bit.  Why this is so we do not know; we suspect that the
E 20
I 20
D 44
to vary quite a bit.  We suspect that the
E 44
I 44
to vary quite a bit, up to 30%.  We suspect that the
E 44
E 20
operating system is not using the same set of physical
pages each time a process is created and we are seeing the effects of
collisions in the external caches.  We compensate by running the 
D 13
benchmark in a loop and taking the minimum result.  We run the loop long
enough to overcome any clock resolution problems.
E 13
I 13
D 44
benchmark in a loop and taking the minimum result.  
E 44
I 44
benchmark in a loop and taking the minimum result.  Users interested in
the most accurate data are advised to verify the results on their
own platforms.
E 44
I 28
.PP
Many of the results included in the database were donated by users
and were not created by the authors.
Good benchmarking hygiene suggests that one should run the benchmarks
as the only user of a machine, without other resource intensive
or unpredictable processes or daemons, such as X11 or httpd.
E 48
I 44
.br
.di bigtable
.ev keep
.ps 8
.vs 9
.so systems.tbl
.ps \n[PS]
.vs \n[VS]
.nr TABLE \n[TABLE]+1
.ce 1
.SP
D 48
\fBTable \n[TABLE].\ \ System description\fP
E 48
I 48
\fBTable \n[TABLE].\ \ System descriptions.\fP
E 48
.SP
.di
.ev
.nr WHEN \n[dn]+\n[FM]
.nr THT \n[dn]
.de print*table
D 51
.	nf
.	bigtable
E 51
I 51
'	sp .5
'	ev keep
'	nf
'	bigtable
E 51
.	ne 1
.	wh -\n[WHEN]u skip*page
D 51
.fi
E 51
I 51
.	fi
.	ev
E 51
..
.de skip*page
'	sp \n[THT]u
.	wh -\n[WHEN]u
..
.wh -\n[WHEN]u print*table
I 48
.BU Variability :
D 50
Some benchmarks, most notably the context switch benchmark, had a tendency
E 50
I 50
The results of some benchmarks, most notably the context switch benchmark, had a tendency
E 50
to vary quite a bit, up to 30%.  We suspect that the
operating system is not using the same set of physical
pages each time a process is created and we are seeing the effects of
collisions in the external caches.  We compensate by running the 
benchmark in a loop and taking the minimum result.  Users interested in
the most accurate data are advised to verify the results on their
own platforms.
.PP
Many of the results included in the database were donated by users
and were not created by the authors.
D 49
Good benchmarking hygiene suggests that one should run the benchmarks
E 49
I 49
Good benchmarking practice suggests that one should run the benchmarks
E 49
as the only user of a machine, without other resource intensive
D 49
or unpredictable processes or daemons, such as X11 or httpd.
E 49
I 49
or unpredictable processes or daemons.
E 49
E 48
E 44
E 28
E 13
I 12
.NH 2
D 28
Using the \*[lmbench] database
E 28
I 28
Using the \f(CBlmbench\fP database
E 28
.PP
D 16
\*[lmbench] provides a database of results included with the source.  
The database is useful for comparison purposes.  It is quite easy to
E 16
I 16
D 44
\*[lmbench] includes a database of results which
E 44
I 44
\*[lmbench] includes a database of results that
E 44
is useful for comparison purposes.  It is quite easy to
E 16
D 40
build the source, run the benchmark, and produce a table of results
E 40
I 40
D 44
build the program, run the benchmark, and produce a table of results
E 40
that includes your run.  All of the tables in this paper were produced
E 44
I 44
build the source, run the benchmark, and produce a table of results
that includes the run.  All of the tables in this paper were produced
E 44
D 16
from the database which is included in \*[lmbench].  This paper is also
D 15
included with \*[lmbench] and may be reproduced with your results included.  
E 15
I 15
included with \*[lmbench] and may be reproduced with new results included.  
E 16
I 16
from the database included in \*[lmbench].  This paper is also
included with \*[lmbench] and may be reproduced incorporating new results.
I 24
D 28
For more information, consulte the file \f(CWlmbench-HOWTO\fP in the 
E 28
I 28
For more information, consult the file \f(CWlmbench-HOWTO\fP in the 
E 28
\*[lmbench] distribution.
E 24
E 16
E 15
E 12
E 6
.NH 1
D 4
Benchmark systems
E 4
I 4
Systems tested
E 4
.PP
D 24
Describe each of the systems measured either here or at the end.
I 12
People want to know the approximate cost of each machine.  Try and
D 15
get SpecInt numbers for each machine as well.
E 15
I 15
get SpecInt numbers for each machine as well.  Note which are MP
and which are UP.
E 24
I 24
D 32
Describe each of the systems measured.  We need SpecInt, Cost, Year introduced,
MP or UP.
E 32
I 32
D 49
\*[lmbench] has been run on a wide variety of platforms, but this
E 49
I 49
\*[lmbench] has been run on a wide variety of platforms. This
E 49
paper includes results from a representative subset of machines and
operating systems.
Comparisons between similar hardware running different operating
systems can be very illuminating, and we have included a few examples
in our results.
E 32
E 24
I 20
.PP
D 32
The machine entry contains three pieces of information: operating system, 
E 32
I 32
D 44
The systems are briefly characterized in Figure 1.  
E 44
I 44
D 47
The systems are briefly characterized in Table 1.  
E 44
Each system is characterized by the operating system, 
E 32
D 24
processor, and processor clock speed.  There are seven major Unix variants
included in the results, two of which (Linux and AIX) were run on 
multiple processor architectures.
E 24
I 24
processor, and processor clock speed.  
I 32
For each system we also provide the year of introduction, SPECint92,
and approximate list price.
E 47
I 47
D 50
The systems are briefly characterized in Table 1.  Please note that list prices
E 50
I 50
The systems are briefly characterized in Table 1.  Please note that the list prices
E 50
are very approximate as is the year of introduction.
The SPECInt92 numbers are a little suspect since 
some vendors have been ``optimizing'' for certain parts of SPEC.  We try and
D 48
quote the original SPECInt92 numbers were we can.
E 48
I 48
quote the original SPECInt92 numbers where we can.
E 48
E 47
E 32
E 24
E 20
E 15
E 12
I 6
D 10
.LP
E 10
I 10
D 37
.TSTART
E 10
D 30
.so ../Results/tmp/misc_mhz.tbl
E 30
I 30
.so systems.tbl
E 30
I 10
.TEND "System details"
E 37
I 28
.NH 2
Reading the result tables
.PP
Throughout the rest of this paper, we present tables of results for many of the
benchmarks.  All of the tables are sorted, from best to worst.  Some tables
have multiple columns of results and those tables are sorted on only one of
the columns.  The sorted column's heading will be in \fBbold\fP.
E 28
E 10
D 8
This is just a placeholder for the table.  It needs specint.
E 8
I 8
D 12
XXX --- This is just a placeholder for the table.  It needs specint.
E 8
E 6
I 4
.NH 2
D 6
hardware
.NH 2
Software
E 6
I 6
Processor cycle time
.PP
\*[lmbench] includes a program which attempts to calculate
the megahertz and clock speed of the processor.  This is the
standard loop in which a series of interlocked operations are timed,
and then the megahertz is derived from the timing.  The operations 
are purposefully interlocked to overcome any super scalar features of the
system under test.
.PP
There are actually three versions of mhz, a generic one that works on
most systems, and two specific versions for SuperSPARC and rs6000
systems.
.PP
It turns out that the
SuperSPARC processor has two ALU's that are run at twice the clock rate,
allowing two interlocked operations to complete in one processor clock.\**
.FS
Credit and thanks to John Mashey of SGI/MIPS fame, who kindly took the
time to out why the benchmark wasn't working on SuperSPARC
systems.  He explained the SuperSPARC pipeline and the solution to the
problem.
.FE
Fortunately, the ALU's are asymmetric and can not do two shifts in
one processor clock.  Shifts are used on SuperSPARC systems.
.PP
IBM rs6000 systems have a C compiler that does not honor the
``register'' directive in unoptimized code.  The IBM loop looks
like it is doing half as many instructions as the others.  This
is on purpose, each add on the IBM is actually two instructions
(I think it is a load/add/store or something like that).
E 12
E 6
E 4
.NH 1
Bandwidth benchmarks
.PP
E 3
D 4
Ken Okin\** once noted that ``Unix is Swahili for bcopy().''
I believe that he was indicating his belief that the operating system spent
most of its time moving data from one place to another, via various means.
I tend to agree and have measured the various ways that data can be moved.
D 3
The ways that are measured are: through pipes, TCP sockets, library bcopy()
and hand unrolled bcopy(), the read() interface, through the mmap() interface,
and direct memory read and write (no copying).
E 3
I 3
The ways that are measured are: 
library bcopy(),
hand unrolled bcopy(),
E 4
I 4
By bandwidth, we mean the rate at which a particular facility can move
D 51
data.  This operation is so common that
D 29
Ken Okin\** once noted that ``Unix is Swahili for \*[bcopy].''
This observation came after yet another crisis management situation in
E 29
I 29
D 32
Ken Okin\** once noted that ``Unix is Swahili for copy.''
This observation came after a situation in
E 32
I 32
D 33
Ken Okin\** once noted that ``Unix is Swahili for \*[bcopy].''
This observation came after yet another crisis management situation in
E 33
I 33
Ken Okin\** once noted that ``Unix is Swahili for copy.''
This observation came after a situation in
E 33
E 32
E 29
D 8
which a customer was furious because of the acheived \*[bcopy] rates on
his shiney new Sun multi processor system.  
E 8
I 8
D 12
which a customer was furious because of the achieved \*[bcopy] rates on
his shiny new Sun multi processor system.  
E 12
I 12
D 15
which a customer was underwhelmed with the achieved \*[bcopy] rates on
E 15
I 15
D 48
which a customer was underwhelmed with the achieved \*[bcopy]\** rates on
E 48
I 48
which a customer was underwhelmed with achieved \*[bcopy]\** rates on
E 48
E 15
D 16
his new Sun multi processor system.  
E 16
I 16
D 28
his new Sun multi-processor system.  
E 28
I 28
D 37
his new \s-1SUN\s0 multi-processor system.  
E 37
I 37
D 44
his new \s-1SUN\s0 multiprocessor system.  
E 37
E 28
E 16
I 15
.FS
One of Larry McVoy's former managers at Sun Microsystems, currently the VP
of desktop systems.
.FE
.FS
\*[bcopy] is the BSD interface that is used for memory to memory copy.
.FE
E 44
I 44
D 50
a new \s-1SUN\s0 multiprocessor system.  
E 50
I 50
a new \s-1Sun\s0 multiprocessor system.  
E 50
.PP
E 51
I 51
data.  
E 51
E 44
E 15
E 12
E 8
We attempt to measure the data movement ability of a number of
different facilities:
library \*[bcopy],
D 44
hand unrolled \*[bcopy],
E 4
D 15
direct memory read and write (no copying).
E 15
I 15
direct memory read and write (no copying),
E 44
I 44
D 47
hand\-unrolled \*[bcopy],
direct\-memory read and write (no copying),
E 47
I 47
hand-unrolled \*[bcopy],
direct-memory read and write (no copying),
E 47
E 44
E 15
pipes,
TCP sockets,
D 4
the read() interface,
E 4
I 4
the \*[read] interface,
E 4
and
D 4
the mmap() interface.
E 4
I 4
the \*[mmap] interface.
I 44
D 51
.FS
One of Larry McVoy's former managers at Sun Microsystems, currently the VP
of desktop systems.
.FE
.FS
\*[bcopy] is the BSD interface that is used for memory to memory copy.
.FE
E 51
E 44
E 4
E 3
D 15
.FS
D 3
One of Larry McVoy's former managers at Sun Microsystems.
E 3
I 3
One of Larry McVoy's former managers at Sun Microsystems, currently the VP
of desktop systems.
E 3
.FE
E 15
D 3

.NH 1
Latency measurements
.LP
The latency measurements included in this suite are process creation times
(including address space extension via mmap()),
basic operating system entry cost, context switching, inter process
communication, file system latency, 
disk latency (you must be the super user to get
disk latency results), and memory latency.
E 3
I 3
.NH 2
D 15
Bcopy bandwidth
E 15
I 15
Memory bandwidth
E 15
E 3
.PP
D 3
Process benchmarks are used to measure the basic process primitives,
such as creating a new process, running a different program, and context 
switching.  Process creation benchmarks are of particular interest
to distributed systems since many remote operations include the creation
of a remote process to shepherd the remote operation to completion.
Context switching is important for the same reasons.
E 3
I 3
D 4
bcopy() is the BSD interface for moving data.  The System V interface is
memmove().  Both interfaces are used to move data from one place to another.
A modified version of bcopy(), frequently called uiomove(), is used to 
E 4
I 4
D 15
\*[bcopy] is the BSD interface that is used for memory to memory copy.
A modified version of \*[bcopy], frequently called \*[uiomove], is used to 
E 4
move data from/to the user application to/from kernel buffers.  
E 15
I 15
Data movement is fundamental to any operating system.  
In the past, performance
D 40
was frequently measured by MFLOPS.  Building floating point units was a 
complex enough problem that it was rare that microprocessor systems were
limited by memory bandwidth.  Today, the floating point units are much
E 40
I 40
D 49
was frequently measured by MFLOPS because floating point units were
E 49
I 49
was frequently measured in MFLOPS because floating point units were
E 49
slow enough that microprocessor systems were
D 50
rarely limited by memory bandwidth.  Today, the floating point units are much
E 50
I 50
rarely limited by memory bandwidth.  Today, floating point units are usually much
E 50
E 40
D 16
faster than memory bandwidth.  Many current MFLOP ratings can not be maintained
from memory, they are cache only ratings.
E 16
I 16
faster than memory bandwidth, so many current MFLOP ratings can not be 
D 44
maintained using memory resident data; they are ``cache only'' ratings.
E 44
I 44
D 47
maintained using memory\-resident data; they are ``cache only'' ratings.
E 47
I 47
maintained using memory-resident data; they are ``cache only'' ratings.
E 47
E 44
E 16
E 15
E 3
.PP
D 3
Inter process communication latency is important because many operations
are control messages that tell another process (frequently on another
system) to do something.  The latency of telling the remote process to
do something is pure overhead and is frequently in the critical path
of important functions, such as distributed databases.\**
.FS
The performance of the TCP latency benchmark has proven to be a good
estimate of the performance of the Oracle database lock manager.
.FE
.PP
The inter process communication latency benchmarks are roughly the same
idea: pass a small message (a byte or so) back and forth between two
processes.  The reported results are always the microseconds it takes
to do one round trip.  If you are interested in a one way timing, then
about half the round trip is right (however, the CPU cycles tend to be
somewhat asymmetric for a one trip).  
.NH 2
Process forks/exits
.LP
Create a child process which does nothing but 
terminate.  Results are reported in creations per second.  
The benchmark is measuring how fast the OS can create a new address
space and process context.
The child process is spawned via the \f(CBfork\fP() interface,
not the \f(CBvfork\fP() interface. 
.NH 2
Simple process creates I
.LP
Create a child process which then runs a new program that does nothing
but print ``hello world'' and exit.  The difference between this 
benchmark and the previous is the running of a new program.  The
time difference between this and the previous benchmark is the cost
of starting a new (simple) program.  That cost is especially noticeable
on (some) systems that have shared libraries.  Shared libraries can
introduce a substantial (10s of milliseconds) start up cost.  This 
benchmark is intended to quantify the time/space tradeoff of shared
libraries.
.NH 2
Simple process creates II
.LP
Create a child process which runs the same new program except that the
program is started by the system shell.  This is a clone of the C
library \f(CBsystem\fP() interface.  The intent is to educate users
about the cost of this interface.  I have long felt that using the
Bourne shell, especially a dynamically linked Bourne shell, to start up
processes is over kill; perhaps these numbers will convince others of the
same thing.  A better choice would be Plan 9's \f(CBrc\fP shell (which
is, by the way, free software).
.NH 2
Memory mapping 
.LP
Memory mapping is the process of making a file part of a process' address
space, allowing direct access to the file's pages.  It is an alternative
to the traditional read and write interfaces.  Memory mapping is extensively
used for linking in shared libraries at run time.  This benchmark measures
the speed at which mappings can be created as well as removed.  Results
are reported in mappings per second, and the results can be graphed as the
test is run over a series of different sizes.
.NH 2
Context switches
.LP
Measures process context switch time.\**  A context switch is defined as 
the time it takes to save the state of one process and restore the state
of another process.
Typical context switch benchmarks measure just the minimal context switch
time, i.e., the time to switch between two processes that are doing nothing
but context switching.  That approach is misleading because systems may
have multiple active processes and the processes typically have more state
(hot cache lines) than just the code required to force another context
switch.  This benchmark takes that into consideration and varies both
the number and the size of the processes.
E 3
I 3
D 4
Data movement is fundamental to any operating system.  The various bcopy()
E 4
I 4
D 15
Data movement is fundamental to any operating system.  The various \*[bcopy]
E 4
implementations are some of the most heavily optimized code in any operating
system and are constantly being revised to work with the latest hardware.
E 15
I 15
D 16
We measured memory performance in several ways.  We measured the ability to
E 16
I 16
D 44
We measured the ability to
E 44
I 44
We measure the ability to
E 44
E 16
D 24
copy, read, and write data over a varying set of sizes and alignments.  
E 24
I 24
copy, read, and write data over a varying set of sizes.
E 24
There are too many results to report all of them here, so we concentrate on 
D 16
large memory movement.  
E 16
I 16
large memory transfers.
E 16
E 15
.PP
D 4
We measure bcopy() bandwidth two ways.  The first is the user level library
E 4
I 4
D 15
We measure \*[bcopy] bandwidth two ways.  The first is the user level library
E 4
interface.  The second is a hand unrolled loop that loads and stores
aligned 64bit (8 byte) words.  The point of doing both is to see if the
general interface could use any improvement.
.PP
Both ways copy an 8 megabyte\** chunk to another 8 megabyte chunk.  The copy is
repeated eight times and the average is reported.  The intent of the size 
is to test memory, not cache bandwidth.  As secondary caches reach 16 megabytes,
E 15
I 15
D 44
We measure copy bandwidth two ways.  The first is the user level library
E 44
I 44
D 47
We measure copy bandwidth two ways.  The first is the user\-level library
E 47
I 47
We measure copy bandwidth two ways.  The first is the user-level library
E 47
E 44
\*[bcopy] interface.
D 44
The second is a hand unrolled loop that loads and stores
D 28
aligned 64bit (8 byte) words.  
E 28
I 28
aligned 64 bit (8 byte) words.  
E 44
I 44
D 47
The second is a hand\-unrolled loop that loads and stores
aligned 8\-byte words.  
E 47
I 47
The second is a hand-unrolled loop that loads and stores
aligned 8-byte words.  
E 47
E 44
E 28
In both cases, we took care to
D 16
insure that the source and destination locations would not map to the same
E 16
I 16
ensure that the source and destination locations would not map to the same
E 16
D 44
lines if the any of the caches were direct mapped.  
E 44
I 44
D 47
lines if the any of the caches were direct\-mapped.  
E 47
I 47
lines if the any of the caches were direct-mapped.  
E 47
E 44
D 16
Both cases copy an 8 megabyte\** chunk to another 8 megabyte chunk.  
The intent of the size is to test memory, not cache bandwidth. 
As secondary caches reach 16 megabytes,
E 15
this benchmark will be modified to move enough more than the cache size that
the cache effects are negligible.
E 16
I 16
In order to test memory bandwidth rather than cache bandwidth, 
D 28
both benchmarks copy an 8 megabyte\** chunk to another 8 megabyte chunk.  
E 28
I 28
D 44
both benchmarks copy an 8 megabyte\** area to another 8 megabyte area.  
E 28
As secondary caches reach 16 megabytes, these benchmarks will have to
be resized to reduce caching effects.
E 44
I 44
both benchmarks copy an 8M\** area to another 8M area.  
(As secondary caches reach 16M, these benchmarks will have to
be resized to reduce caching effects.)
E 44
E 16
E 3
.FS
D 3
A previous version of this benchmark included several system calls
in addition to the context switch, resulting in grossly over inflated
context switch times.
E 3
I 3
D 4
Some of the PCs had only 16 megabytes of memory - those machines copied 4 
megabytes.
E 4
I 4
D 8
Some of the PCs had only less than 16 megabytes of availablememory -
E 8
I 8
D 13
Some of the PCs had only less than 16 megabytes of available memory -
E 13
I 13
D 44
Some of the PCs had less than 16 megabytes of available memory -
E 13
E 8
those machines copied 4 megabytes. 
E 44
I 44
Some of the PCs had less than 16M of available memory;
those machines copied 4M. 
E 44
D 15
In the future, this will be increased to 16MB to defeat large second level
caches.
E 15
E 4
E 3
.FE
D 5
.PP
D 3
The benchmark is a ring of two to twenty processes that are connected
with Unix pipes.  A token is passed from process to process, forcing
context switches.  The benchmark measures the time it takes to pass
the token two thousand times from process to process.  Each hand off
of the token has two costs: (a) the context switch, and (b) the cost 
of passing the token.  In order to get just the context switching time,
the benchmark first measures the cost of passing the token through a 
ring of pipes in a single process.  This time is defined as the cost 
of passing the token and is not included in the reported context switch
time.
E 3
I 3
XXX - results go here, do a side by side table of system, libc, unrolled
E 3
.PP
E 5
D 3
When the processes are larger than the default baseline of ``zero''
(where zero means just big enough to do the benchmark), the cost
of the context switch includes the cost of restoring user level
state (cache lines).  This is accomplished by having the process
allocate an array of data and sum it as a series of integers
after receiving the token but before passing the token to the
next process.  Note that the overhead mentioned above includes 
the cost of accessing the data but because it is measured in 
just one address space, the cost is typically the cost with hot
caches.  So the context switch time does not include anything
other than the context switch provided that all the processes
fit in the cache.  If there are cache misses (as is common), the
cost of the context switch includes the cost of those cache misses.
E 3
I 3
D 15
The number reported actually
represents 1/2 to 1/3 of the actual memory bandwidth used to obtain those
results.  The reason is that the bcopy is at least a read followed by a
D 13
write, resulting in twice as much memory bandwidth used.  in the bcopy loop,
E 13
I 13
write, resulting in twice as much memory bandwidth used.  Within the bcopy loop,
E 13
the store to memory is usually smaller in size than a cache line.  That means
that the cache line has to first be read from memory, completely filled in,
and then stored back to memory.  In that case, the bcopy results represent
1/3 of the memory traffic.  Some systems architects have recognized this
problem and provided special store instructions that do not cause a read
of the cache line when writing a partial cache line.  XXX - reference 
UltraSPARC?
I 10
.TSTART
E 10
I 5
.so ../Results/tmp/bw_bcopy.tbl
E 5
E 3
D 10
.PP
E 10
I 10
D 11
.TEND "Bcopy bandwidth in MB/sec"
E 11
I 11
.TEND "Bcopy bandwidth in MB/second"
E 11
E 10
I 4
D 12
The results reported here may be correlated with John McCalpin's \*[stream]
E 12
I 12
The results reported in Figure \n[FIGURE]
E 15
I 15
.PP
D 28
The copy results actually
D 16
represent 1/2 (or less) of the actual memory bandwidth used to obtain those
E 16
I 16
represent 1/2 to 1/3 of the actual memory bandwidth used to obtain those
E 16
results since we are reading and writing memory.  If the cache line size is
larger than the 8 byte store, then the written
cache line must be read before it is written.
In that case, the amount of memory being moved is three times the 
\*[bcopy] bandwidth.
E 28
I 28
D 44
The copy results actually represent 1/2 to 1/3 of the actual memory
E 44
I 44
D 47
The copy results actually represent one\-half to one\-third of the memory
E 47
I 47
The copy results actually represent one-half to one-third of the memory
E 47
E 44
bandwidth used to obtain those results since we are reading and writing
memory.  If the cache line size is larger than the word stored, then
the written cache line will typically be read before it is written.  The
actual amount of memory bandwidth used varies because some architectures
have special instructions specifically designed for the \*[bcopy]
function.  Those architectures will move twice as much memory as
D 49
reported by this benchmark; less intelligent architectures move three
E 49
I 49
reported by this benchmark; less advanced architectures move three
E 49
times as much memory: the memory read, the memory read because it is
about to be overwritten, and the memory written.
E 28
.PP
D 44
The \*[bcopy] results reported in Figure 2
E 44
I 44
The \*[bcopy] results reported in Table 2
E 44
E 15
may be correlated with John McCalpin's \*[stream]
E 12
D 9
.[
McCalpin95
.]
E 9
I 9
.RN McCalpin95
E 9
benchmark results in the following manner:
D 15
the \*[stream] benchmark reports all of the memory moved (XXX - how?)
E 15
I 15
D 44
the \*[stream] benchmark reports all of the memory moved 
E 44
I 44
the \*[stream] benchmark reports all of the memory moved
E 44
E 15
whereas the \*[bcopy] benchmark reports the bytes copied.  So our
D 12
numbers should be approximately 1/2 of his numbers.  Other differences
are probably accounted for by caching vagaries.  XXX
E 12
I 12
D 15
numbers should be approximately 1/2 of his numbers.  
E 15
I 15
D 16
numbers should be approximately 1/2 of his numbers.
E 16
I 16
D 44
numbers should be approximately 1/2 to 1/3 of his numbers.
I 28
Systems that have to load cache lines from memory before a 
write will typically get 1/3 of the \*[stream] bandwidth.
E 44
I 44
D 47
numbers should be approximately one\-half to one\-third of his numbers.
E 47
I 47
numbers should be approximately one-half to one-third of his numbers.
E 47
E 44
E 28
E 16
E 15
E 12
.PP
E 4
D 3
Results for an HP system running at 100 mhz are shown below.  
This is a particularly nice system for this benchmark because the
results are quite close to what is expected from a machine with a
256KB cache.  As the size and number of processes are both increased,
processes start falling out of the cache, resulting in higher context
switch times.
.LP
.so ctx.pic
E 3
I 3
D 12
It is helpful, when reading this for the first time, to pick one machine and
E 12
I 12
D 15
It is helpful, when reading this paper for the first time, to pick one machine and
E 12
remember that machine's results as you proceed from one section to the
next.  The reason is that many of the other bandwidth results are limited by
D 4
bcopy() speed and/or are a multiple of bcopy() speed.  The AIX machine is
E 4
I 4
D 12
\*[bcopy] speed and/or are a multiple of \*[bcopy] speed.  The AIX machine is
E 4
interesting, especially considering it has stellar memory performance.\**
E 12
I 12
\*[bcopy] speed and/or are a multiple of \*[bcopy] speed.  The IBM 990 machine
is an interesting machine to watch.  It has a very good memory subsystem\**
and we think that that memory subsystem is the main reason that other benchmarks 
performance well on this system.
E 12
.FS
D 8
Someone, I think it was John Mashey, described this machine as a $1000
E 8
I 8
Someone, we think it was John Mashey, described this machine as a $1000
E 8
processor on a $99,000 memory subsystem.  Seems right.
.FE
E 3
.NH 2
E 15
I 15
D 44
Memory reading is measured by an unrolled loop which sums up a series of
E 15
D 3
Null system calls
.LP
Measures the cost of entering and exiting (without pausing) the
operating system.  This is accomplished by repeatedly writing one byte
to \f(CB/dev/null\fP, a pseudo device driver that does nothing but
discard the data.  Results are reported as system calls per second.
E 3
I 3
D 15
Memory read bandwidth
E 3
.PP
I 12
XXX - STATE - start here and work forward.
.PP
E 12
D 3
It is important to note that the system call chosen actually does the
work on all systems, to the best of my knowledge.  There are some
systems that optimized trivial system calls, such as \f(CBgetpid\fP(),
to return the answer without a true entry into the OS proper.  Writing
to \f(CB/dev/null\fP has not been optimized.
E 3
I 3
D 8
Memory reading is a straightforward benchmark.  XXX - Carl, what do I say here?
.PP
E 8
Memory reading is fundamental to many applications.  In the past, performance
was frequently measured by MFLOPS.  Building floating point units was a 
complex enough problem that it was rare that microprocessor systems were
limited by memory bandwidth.  Today, the floating point units are much
faster than memory bandwidth.  Many current MFLOP ratings can not be maintained
from memory, they are cache only ratings.
.PP
Memory reading is measured by an unrolled loop which sums up a series of
E 15
D 4
integers.  On most systems, the DEC alpha is known exception, the integer
E 4
I 4
integers.  On most systems measured (maybe all of them) the integer
E 44
I 44
Memory reading is measured by an unrolled loop that sums up a series of
integers.  On most (perhaps all) systems measured the integer
E 44
E 4
size is 4 bytes.  The loop is unrolled such that most compilers generate
D 44
code that use an constant offset with the load, resulting in a load and
E 44
I 44
code that uses a constant offset with the load, resulting in a load and
E 44
an add for each word of memory.  The add is an integer add that completes
D 44
in one cycle on all of the processors.  Given that today's processor cycles
D 4
typically at 10 or less nanoseconds and that memory is typically 200 nanoseconds or
more per cache line, the result reported here should be dominated by the 
E 4
I 4
typically at 10 or less nanoseconds and that memory is typically 200-1000
D 16
nanoseconds per cache line, the result reported here should be dominated by the 
E 16
I 16
nanoseconds per cache line, the result reported here should be dominated by the
E 44
I 44
in one cycle on all of the processors.  Given that today's processor 
typically cycles at 10 or fewer nanoseconds (ns) and that memory is typically 200-1,000
ns per cache line, the results reported here should be dominated by the
E 44
E 16
E 4
memory subsystem, not the processor add unit.
.PP
I 8
D 10
The kernel of the memory read benchmark is roughly:
.DS
.nf
.ft CW
mem = (unsigned int)malloc(bytes);
end = mem + bytes / sizeof(unsigned int) - 200;

for (p = mem; p < end; ) {
	sum += p[0]+p[1]+...+p[200];
	p += 200;
}
.ft
.fi
.DE
.PP
E 10
E 8
D 44
The memory is added up because almost all C compilers (a) would generate
far too many instructions without optimization, and (b) would optimize 
out the whole loop when you turned on optimization.  The solution was to
E 44
I 44
D 50
The memory is added up because almost all C compilers
E 50
I 50
The memory contents are added up because almost all C compilers
E 50
would optimize out the whole loop when optimization was turned on, and
would generate far too many instructions without optimization.
D 50
The solution was to
E 50
I 50
The solution is to
E 50
E 44
add up the data and pass the result as an unused argument to the 
D 15
``finish timing'' function.  When compilers get smart enough to defeat that, 
D 13
I may have to resort to assembler.
E 13
I 13
we may have to resort to assembly code.
E 15
I 15
``finish timing'' function.  
E 15
E 13
.PP
D 15
In the future, when all systems support 64bit longs, this benchmark will use
64bit loads instead of the current 32 bit loads.
D 10
.PP
E 10
I 10
D 11
.TSTART
E 10
D 5
XXX - results go here.
E 5
I 5
.so ../Results/tmp/bw_mem_rdsum.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "Memory read bandwidth in MB/sec"
E 11
I 11
.\" .TSTART
.\" .so ../Results/tmp/bw_mem_rdsum.tbl
.\" .TEND "Memory read bandwidth in MB/second"
.PP
E 15
E 11
E 10
D 4
Memory reads represent about 1/2 to 1/3 of the bcopy() work, and we expect
that pure reads should run at roughly twice the speed of the bcopy().
E 4
I 4
D 44
Memory reads represent about 1/2 to 1/3 of the \*[bcopy] work, and we expect
E 44
I 44
D 47
Memory reads represent about one\-third to one\-half of the \*[bcopy] work, and we expect
E 47
I 47
Memory reads represent about one-third to one-half of the \*[bcopy] work, and we expect
E 47
E 44
D 17
that pure reads should run at roughly twice the speed of the \*[bcopy].
E 17
I 17
that pure reads should run at roughly twice the speed of \*[bcopy].
E 17
E 4
Exceptions to this rule should be studied, for exceptions indicate a bug
D 4
in the benchmarks, a problem in bcopy(), or some unusual hardware.  
E 4
I 4
in the benchmarks, a problem in \*[bcopy], or some unusual hardware.  
I 8
D 15
For example, the rs6000-990 can bcopy data faster than it can read it,
which is quite startling.  Also, the FreeBSD i586@100 has significantly
better \*[bcopy] performance than AIX powerpc@134 while both have similar
memory read performance.
E 15
I 11
.TSTART
D 15
.so ../Results/tmp/bw_mem.tbl
.TEND "Memory bandwidth in MB/second"
E 11
D 10
.PP
.so memrd_bcopy_comp.pic
E 10
E 8
E 4
E 3
.NH 2
D 3
Pipe latency
.LP
This benchmark measures the OS; there is almost no code executed at
user level.  The benchmark measures the round trip time of a small message
being passed back and forth between two processes through a pair of
Unix pipes.
E 3
I 3
Memory write bandwidth
E 15
I 15
.so ../Results/tmp/bw_allmem.tbl
D 47
.TEND "Memory bandwidth in MB/s"
E 47
I 47
.TEND "Memory bandwidth (MB/s)"
E 47
E 15
.PP
D 8
Memory writing is a straightforward benchmark.  XXX - Carl, what do I say here?
.PP
E 8
D 15
Memory writing is fundamental to many applications.  It has also become a
D 8
greater performance bottleneck due to the increase in processor speed
E 8
I 8
performance bottleneck due to the increase in processor speed
E 8
relative to memory speed.  
.PP
E 15
Memory writing is measured by an unrolled loop that stores a value into
an integer (typically a 4 byte integer) and then increments the pointer.
The processor cost of each memory operation is approximately the same
as the cost in the read case.
I 8
D 10
.DS
.nf
.ft CW
for (p = mem; p < end; ) {
	*p++=1;
	*p++=1;
	...
	*p++=1;
}
.ft
.fi
.DE
E 8
.PP
E 10
I 10
D 11
.TSTART
E 10
D 5
XXX - results go here.
E 5
I 5
.so ../Results/tmp/bw_mem_wr.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "Memory write bandwidth in MB/sec"
E 10
D 4
Memory write results may be different than read results.  Several factors
may cause this:  what the fuck are they?  Store buffers?  Multiple store
units?  Multiple paths to external cache?
E 4
I 4
The IBM 990 has remarkable write performance.  This may be in part due to the
fact that the 990 has two independent paths from the processor to the cache.
E 11
I 11
D 15
.\" .TSTART
.\" .so ../Results/tmp/bw_mem_wr.tbl
.\" .TEND "Memory write bandwidth in MB/second"
Notice that the IBM 990 has remarkable write performance.  This may be
in part due to the fact that the 990 has two independent paths from the
processor to the cache.
E 15
I 15
.PP
D 48
It is helpful, when reading this paper for the first time, to pick one
machine and remember that machine's results as you proceed from one
D 17
section to the next.  The reason is that many of the other bandwidth
E 17
I 17
section to the next because many of the other bandwidth
E 17
results are limited by \*[bcopy] speed and/or are a multiple of
D 24
\*[bcopy] speed.  The IBM 990 machine is an interesting machine to
E 24
I 24
D 28
\*[bcopy] speed.  The BUM 990 machine is an interesting machine to
E 28
I 28
D 44
\*[bcopy] speed.  The \s-1IBM\s0 990 machine is an interesting machine to
E 28
E 24
watch.  It has a very good memory subsystem\** and we think that that
memory subsystem is the main reason that other benchmarks performance
E 44
I 44
\*[bcopy] speed.  The \s-1IBM Power2\s0 machine is an interesting machine to
watch.  It has a very good memory subsystem,\** and we think that 
this subsystem is the main reason that other benchmarks perform
E 44
well on this system.  
E 48
I 48
The numbers reported in Table \n[TABLE]
are not the raw hardware speed in some cases.
D 49
The Power2 is capable of up to 800M/sec read rates 
E 49
I 49
The Power2\** is capable of up to 800M/sec read rates 
.FS
Someone described this machine as a $1,000 processor on a $99,000 memory
subsystem.
.FE
E 49
.RN McCalpin95
and HP PA RISC (and other prefetching)
systems also do better if higher levels of code optimization used
and/or the code is hand tuned.
.PP
The Sun libc bcopy in Table \n[TABLE] 
is better because they use a hardware specific bcopy
routine that uses instructions new in SPARC V9 that were added specifically
for memory movement.
.PP
D 49
The PentiumPro read rate in Table \n[TABLE] is much higher than the write rate because,
E 49
I 49
The Pentium Pro read rate in Table \n[TABLE] is much higher than the write rate because,
E 49
according to Intel, the write transaction turns into a read followed by
a write to maintain cache consistency for MP systems.
E 48
D 49
.FS
D 44
Someone described this machine as a $1000
E 44
I 44
D 48
Someone described this machine as a $1,000
E 44
D 20
processor on a $99,000 memory subsystem.  Seems right. 
E 20
I 20
processor on a $99,000 memory subsystem.
E 48
I 48
Someone described this machine as a $1,000 processor on a $99,000 memory
subsystem.
E 48
E 20
.FE
E 49
E 15
E 11
E 4
E 3
.NH 2
D 3
TCP/IP latency
.LP
This benchmark measures the OS
networking code and the driver code; there is almost no code executed at
user level.  The benchmark measures the round trip time of a small message
being passed back and forth between two processes through an AF_INET
socket.  Note that both remote and local results may be reported.
.NH 2
UDP/IP latency
.LP
This benchmark measures the OS
networking code and the driver code; there is almost no code executed at
user level.  The benchmark measures the round trip time of a small message
being passed back and forth between two processes through an AF_INET socket.
Note that both remote
and local results may be reported.
.LP
It is interesting to note that the TCP performance is sometimes
greater than the UDP performance.  
This is contrary to expectations since
the TCP protocol is a reliable, connection oriented protocol, and as such
is expected to carry more overhead.
Why this is so is an exercise left to the
reader.
.NH 2
RPC latency (TCP and UDP)
.LP
Actually two latency benchmarks: Sun RPC over TCP/IP and over UDP/IP.
This benchmark consists of the user level RPC code layered over the TCP
or UDP sockets.  The benchmark measures the round trip time of a small
message being passed back and forth between two processes.  Note that
both remote and local results may be reported.
.LP
Using the TCP or the UDP benchmarks as a baseline, it 
is possible to see how much the RPC code is costing.  
.NH 2
TCP/IP connect latency
.LP
This benchmarks measures the time it takes to get a TCP/IP socket and
connect it to a remote server.
.NH 2
E 3
I 3
D 15
Pipe bandwidth
E 15
I 15
IPC bandwidth
E 15
.PP
I 15
D 17
Inter process communication bandwidth is frequently a performance issue.
E 17
I 17
D 37
Inter-process communication bandwidth is frequently a performance issue.
E 37
I 37
Interprocess communication bandwidth is frequently a performance issue.
E 37
E 17
D 49
Many Unix applications are really several applications communicating
E 49
I 49
Many Unix applications are composed of several processes communicating
E 49
D 44
through pipes or TCP sockets.  Examples include the groff documentation
system that prepared this paper, the X window system, remote file access,
D 17
world wide web access, etc.
E 17
I 17
D 31
and world wide web access.
E 31
I 31
D 32
and World Wide Web access.
E 32
I 32
D 33
and world wide web access.
E 33
I 33
and World Wide Web access.
E 44
I 44
through pipes or TCP sockets.  Examples include the \f(CWgroff\fP documentation
system that prepared this paper, the \f(CWX Window System\fP, remote file access,
D 50
and \f(CWWorld Wide Web\fP access.
E 50
I 50
and \f(CWWorld Wide Web\fP servers.
E 50
E 44
E 33
E 32
E 31
E 17
.PP
E 15
Unix pipes are an interprocess communication mechanism implemented as 
D 44
a one way byte stream; each end of the stream has an associated file
descriptor, one is the write descriptor and the other is the read
E 44
I 44
D 47
a one\-way byte stream. Each end of the stream has an associated file
E 47
I 47
a one-way byte stream. Each end of the stream has an associated file
E 47
descriptor; one is the write descriptor and the other the read
E 44
descriptor.
I 20
TCP sockets are similar
to pipes except they are bidirectional and can cross machine 
D 50
boundaries because they are a networking facility.
E 50
I 50
boundaries.
E 50
E 20
.PP
I 15
D 20
TCP sockets are a interprocess communication mechanism similar
to pipes.  TCP sockets differ in that they are a networking
facility, so they can cross machine boundaries, and are
also bidirectional.
.PP
E 20
E 15
D 13
Unix pipes are perhaps the simplest way to move data between processes.
Many Unix commands are actually a series of programs communicating in a
pipeline.  For example, the command to print this document is:
E 3
D 3
File system latency
.LP
A benchmark that measures how fast the file system can do basic, common 
operations, such as creates and deletes of small files.  
E 3
I 3
.DS
gsoelim lmb.ms | gpic | gtbl | groff -mgs | lpr
.DE
.PP
E 13
D 17
We measure pipe bandwidth by creating two processes, a writer and a reader,
and having the writer write 50 megabytes of data in 64 kilobyte chunks to the reader.
Note that the reader prints the timing results; that guarantees that all
E 17
I 17
Pipe bandwidth is measured by creating two processes, a writer and a
D 28
reader, which transfer 50 megabytes of data in 64 kilobyte chunks.
E 28
I 28
D 44
reader, which transfer 50 megabytes of data in 64 kilobyte transfers.
E 44
I 44
reader, which transfer 50M of data in 64K transfers.
E 44
D 49
The transfer size was chosen such that the overhead of system calls
E 49
I 49
The transfer size was chosen so that the overhead of system calls
E 49
and context switching would not dominate the benchmark time.
E 28
The reader prints the timing results, which guarantees that all
E 17
data has been moved before the timing is finished.
D 10
.PP
E 10
I 10
D 11
.TSTART
E 10
D 5
XXX - pipe results go here.
E 5
I 5
.so ../Results/tmp/bw_pipe.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "Pipe bandwidth in MB/sec"
E 11
I 11
D 15
.\" .TSTART
.\" .so ../Results/tmp/bw_pipe.tbl
.\" .TEND "Pipe bandwidth in MB/second"
E 11
E 10
It is interesting to contrast these results with the TCP bandwidth as well
D 4
as the bcopy() bandwidth.  
E 4
I 4
as the \*[bcopy] bandwidth.  
E 15
E 4
.PP
I 15
D 17
We measure TCP bandwidth in a similar fashion to the pipe benchmark.  In
the TCP case, the writer uses 1 megabyte I/O requests.  Both the
E 17
I 17
D 50
TCP bandwidth is measured similarly, except the data is transferred
D 28
in 1 megabyte chunks instead of 64 kilobyte chunks.  Both the
E 28
I 28
D 44
in 1 megabyte transfers instead of 64 kilobyte transfers.  Both the
E 44
I 44
in 1M transfers instead of 64K transfers.  Both the
E 44
E 28
E 17
sending and receiving I/O buffers are page aligned.  
If the TCP implementation supports it, the send and receive socket
D 44
buffers are enlarged to 1 megabyte, instead of the default 4-60
kilobytes.
E 44
I 44
buffers are enlarged to 1M, instead of the default 4-60K.
E 50
I 50
TCP bandwidth is measured similarly, except the data is transferred in
1M page aligned transfers instead of 64K transfers.  If the TCP
implementation supports it, the send and receive socket buffers are
enlarged to 1M, instead of the default 4-60K.  We have found that
setting the transfer size equal to the socket buffer size produces the
greatest throughput over the most implementations.
E 50
E 44
.TSTART
.so ../Results/tmp/bw_ipc.tbl
D 47
.TEND "Pipe and local TCP bandwidth in MB/s"
E 47
I 47
.TEND "Pipe and local TCP bandwidth (MB/s)"
E 47
.PP
D 44
The \*[bcopy] results in Figure \n[FIGURE] are the C library results
reported in Figure 2. 
E 44
I 44
D 49
The \*[bcopy] results in Table \n[TABLE] are the C library results
D 48
reported in Table 2. 
E 48
I 48
taken from Table 2. 
E 49
E 48
E 44
E 15
D 4
Bcopy() is important to this test because the 
pipe write/read is typically implemented as a bcopy() into the kernel
from the writer, and then a bcopy() from the kernel to the reader.  In 
E 4
I 4
\*[bcopy] is important to this test because the 
pipe write/read is typically implemented as a \*[bcopy] into the kernel
D 44
from the writer, and then a \*[bcopy] from the kernel to the reader.  In 
E 4
an ideal world, these results would be approximately one half of the 
E 44
I 44
from the writer and then a \*[bcopy] from the kernel to the reader.  
D 47
Ideally, these results would be approximately one\-half of the 
E 47
I 47
Ideally, these results would be approximately one-half of the 
E 47
E 44
D 4
bcopy() results.
E 4
I 4
D 28
\*[bcopy] results.
E 28
I 28
\*[bcopy] results.  It is possible for the kernel \*[bcopy]
to be faster than the C library \*[bcopy] since the kernel may have 
access to \*[bcopy] hardware unavailable to the C library.
E 28
E 4
.PP
D 13
Comparing with TCP is interesting because the TCP benchmark is doing 
E 13
I 13
D 15
Comparing pipes with TCP is interesting because the TCP benchmark is doing 
E 13
exactly the same thing, it is just going through the networking subsystem.
E 15
I 15
D 28
Note that a SGI uniprocessor does better than an SGI MP machine on the pipe
E 28
I 28
D 44
Note that a \s-1SGI\s0 uniprocessor does better than an \s-1SGI\s0 MP machine on the pipe
E 44
I 44
D 48
Note that an \s-1SGI\s0 uniprocessor does better than an \s-1SGI\s0 MP machine on the pipe
E 44
E 28
D 17
bandwidth benchmark.  The reason is that the MP machine schedules the
two processes on different CPUs which implies different caches.  The 
E 17
I 17
bandwidth benchmark.  The MP machine schedules the
D 24
two processes on different CPUs so data has to cross the memory bus
E 24
I 24
two processes on different CPUs so data has to cross the bus
E 24
to reach the reader's cache, while the 
E 17
D 28
uniprocessor is moving data through the pipes in 64 kilobyte chunks
E 28
I 28
D 44
uniprocessor is moving data through the pipes in 64 kilobyte buffers
E 28
which fit nicely in the cache.  In other words, the uniprocessor case
is a cache benchmark but the MP case is a cache to cache benchmark.
E 44
I 44
uniprocessor is moving data through the pipes in 64K buffers
that fit nicely in the cache.  In other words, the uniprocessor case
D 47
is a cache benchmark but the MP case is a cache\-to\-cache benchmark.
E 47
I 47
is a cache benchmark but the MP case is a cache-to-cache benchmark.
E 47
E 44
.PP
D 28
Comparing pipes with TCP is interesting because the TCP benchmark is identical
to the pipe benchmark except for the transport mechanism.
E 15
In an ideal world, the TCP bandwidth would be as good as the pipe
D 15
bandwidth.  
I 11
.TSTART
.so ../Results/tmp/bw_pipe_tcp.tbl
.TEND "Pipe and local TCP bandwidth in MB/second"
E 15
I 15
D 17
bandwidth.  Frequently, this is the case for loopback networking.  The
reason for the good loopback performance is that many vendors have
E 17
I 17
bandwidth.  Frequently, this is the case for loopback networking because
many vendors have
E 17
realized there is no reason to use small packets or to do TCP checksums
in the loopback case.
When comparing against the pipe benchmark, it is easy to see that Solaris
has done this and that OSF and AIX have not.  
I 17
.\" HP-UX???
E 17
.PP
E 28
D 44
The TCP results reported are in ``loop back'' mode, i.e., both ends of
E 44
I 44
The TCP results reported are in loopback mode \- that is both ends of
E 44
D 28
the socket are on the same machine.   It would have been more interesting
D 17
to present machine to machine numbers, but we 
were unable to because it was impossible to
E 17
I 17
to present machine-to-machine numbers, but 
it was impossible to
E 17
get results for all the machines included in this paper.  We are interested
in receiving results for identical machines with a dedicated network connecting
them.  
E 28
I 28
the socket are on the same machine.   It would have been more
interesting to present machine-to-machine numbers, but it was impossible
to get results for all the machines included in this paper.  We are
D 47
interested in receiving results for identical machines with a dedicated
network connecting them.
E 47
I 47
interested in receiving more results for identical machines with a dedicated
network connecting them.  The results we have for TCP bandwidth are
shown below.
.TSTART
.so tcp_bw.tbl
.TEND "TCP network bandwidth (MB/s)"
E 47
I 32
D 33
.\" XXX - I have FreeBSD, Ultrasparc, and IRIX numbers - I should publish them.
E 33
E 32
.PP
E 48
D 31
XXX - I have FreeBSD, Ultrasparc, and IRIX numbers - I should publish them.
.PP
E 31
D 49
Comparing pipes with TCP is interesting because the TCP benchmark is
E 49
I 49
It is interesting to compare pipes with TCP because the TCP benchmark is
E 49
D 44
identical to the pipe benchmark except for the transport mechanism.  In
an ideal world, the TCP bandwidth would be as good as the pipe
E 44
I 44
identical to the pipe benchmark except for the transport mechanism.  
Ideally, the TCP bandwidth would be as good as the pipe
E 44
D 49
bandwidth.  It is not widely known, except to networking types, that the
E 49
I 49
bandwidth.  It is not widely known that the
E 49
D 44
majority of the TCP cost is in the checksum and the network interface
driver.  Both of these costs may be safely eliminated in the loopback
E 44
I 44
majority of the TCP cost is in the \*[bcopy], the checksum,
and the network interface driver. 
D 50
Both of these costs may be safely eliminated in the loopback
E 50
I 50
The checksum and the driver may be safely eliminated in the loopback
E 50
E 44
case and if the costs have been eliminated, then TCP should be just as
D 44
fast as pipes.  When comparing against the pipe benchmark, it is easy to
E 44
I 44
D 51
fast as pipes.  From the pipe and TCP results, it is easy to
E 51
I 51
fast as pipes.  From the pipe and TCP results in Table \n[TABLE], it is easy to
E 51
E 44
see that Solaris and HP-UX have done this optimization.
I 48
.PP
Bcopy rates in Table \n[TABLE] can be lower than pipe rates because the
pipe transfers are done in 64K buffers, a size that frequently fits in
caches, while the bcopy is typically an 8M-to-8M copy, which does not
fit in the cache.
.PP
In Table \n[TABLE], the SGI Indigo2, a uniprocessor, does better than
the SGI MP on pipe bandwidth because of caching effects - in the UP
case, both processes share the cache; on the MP, each process is
communicating with a different cache.
.PP
D 50
All of the TCP results in Table \n[TABLE] are in loopback mode \- that
E 50
I 50
All of the TCP results in Table \n[TABLE] are in loopback mode \(em that
E 50
is both ends of the socket are on the same machine.  It was impossible
to get remote networking results for all the machines included in this
paper.  We are interested in receiving more results for identical
machines with a dedicated network connecting them.  The results we have
for over the wire TCP bandwidth are shown below.
.TSTART
.so tcp_bw.tbl
.TEND "Remote TCP bandwidth (MB/s)"
.PP
The SGI using 100MB/s Hippi is by far the fastest in Table \n[TABLE].
The SGI Hippi interface has hardware support for TCP checksums and 
D 50
the IRIX operating system has virtual memory tricks to avoid copying 
E 50
I 50
the IRIX operating system uses virtual memory tricks to avoid copying 
E 50
data as much as possible.
For larger transfers, SGI Hippi has reached 92MB/s over TCP.
.PP
100baseT is looking quite competitive when compared to FDDI in Table
\n[TABLE], even though FDDI has packets that are almost three times
larger.  We wonder how long it will be before we see gigabit ethernet
interfaces.
E 48
E 28
E 15
E 11
E 3
.NH 2
D 3
Page fault latency
.LP
A benchmark that measures how fast the file system can pagefault in a
page that is not in memory.
.NH 2
Disk latency
.LP
A benchmark that is designed to measure the overhead of a disk
operation.  Results are reported as operations per second.
E 3
I 3
D 15
TCP socket bandwidth
E 15
I 15
Cached I/O bandwidth
E 15
E 3
.PP
D 3
The benchmark is designed with SCSI disks in mind.  It actually simulates
a large number of disks in the following way.  The benchmark reads 512 byte
chunks sequentially from the raw disk device (raw disks are unbuffered
and are not read ahead by Unix).  The benchmark ``knows'' that most
disks have read ahead buffers that read ahead the next 32-128 kilobytes.
Furthermore, the benchmark ``knows'' that the disks rotate and read ahead
faster than the processor can request the chunks of data.\**
.FS
This may not always be true - a processor could be fast enough to make the
requests faster than the rotating disk.  If we take 3MB/sec to be disk
speed, a fair speed, and divide that by 512, that is 6144 IOs/second, or
163 microseconds per IO.  I don't know of any processor/OS/io controller
combinations that can do an
IO in 163 microseconds.
.FE
So the benchmark is basically reading small chunks of data from the
disks track buffer.  Another way to look at this is that the benchmark 
is doing memory to memory transfers across a SCSI channel.
E 3
I 3
D 15
TCP sockets may be viewed as interprocess communication mechanism similar
to pipes.  The major differences are that TCP sockets are a networking
facility, so they can cross machine boundaries, and are bidirectional.
E 15
I 15
D 20
Experience has shown us that reusing data that is in the file system
E 20
I 20
Experience has shown us that reusing data in the file system
E 20
D 31
page cache is an important performance metric.  This 
section measures that metric through two interfaces, \*[read] and
E 31
I 31
D 32
page cache can be a performance issue.  This 
section measures that operation through two interfaces, \*[read] and
E 32
I 32
D 33
page cache is an important performance metric.  This 
section measures that metric through two interfaces, \*[read] and
E 33
I 33
page cache can be a performance issue.  This 
section measures that operation through two interfaces, \*[read] and
E 33
E 32
E 31
\*[mmap].   
D 44
The benchmark here is not an I/O benchmark, in that no disk activity is
involved (unless the machine is incapable of caching the data - and we size
the data such that it will be cached).  We wanted to measure the overhead
E 44
I 44
The benchmark here is not an I/O benchmark in that no disk activity is
involved.
We wanted to measure the overhead
E 44
D 49
of reusing data, an overhead that is CPU intensive, not disk intensive.
E 49
I 49
of reusing data, an overhead that is CPU intensive, rather than disk intensive.
E 49
E 15
E 3
.PP
D 3
No matter how you look at it, the resulting number represents a 
\fBlower\fP bound on the overhead of a disk I/O.  In point of fact,
the real numbers will be higher on SCSI systems.  Most SCSI controllers
will not disconnect if the request can be satisfied immediately; that is
the case here.  In practice, the overhead numbers will be higher because
the processor will send the request, disconnect, get interrupted,
reconnect, and transfer.
E 3
I 3
D 15
The results reported here are in ``loop back'' mode, i.e., both ends of
the socket are on the same machine.  The point of measuring local
instead of remote is twofold: first, we want to compare TCP
to pipes, and second, we are interested in the software overhead
of TCP, not the speed of the transmission media.  In addition, TCP
sockets are very frequently used in local communication even though
a pair of pipes would be lighter weight.  
E 15
I 15
The \*[read] interface copies data from the kernel's file system page cache into the
D 28
process' buffer.  The difference between the \*[bcopy] and \*[read] benchmarks
E 28
I 28
D 44
process' buffer, using 64 kilobyte buffers.  The transfer size was chosen 
E 44
I 44
process's buffer, using 64K buffers.  The transfer size was chosen 
E 44
D 49
so as to minimize the overhead of repeatedly entering the kernel while 
remaining a realistically sized.
E 49
I 49
to minimize the kernel entry overhead while
remaining realistically sized.
E 49
.PP
D 49
The difference between the \*[bcopy] and \*[read] benchmarks
E 49
I 49
The difference between the \*[bcopy] and the \*[read] benchmarks
E 49
E 28
is the cost of the file and virtual memory system overhead.  In most
systems, the \*[bcopy] speed should be faster than the \*[read] speed.  The
D 32
exceptions are typically systems that have hardware specifically designed
D 17
for the \*[bcopy] function.  That hardware is typically available only to
the operating system, so in some cases, \*[read] can be faster than \*[bcopy].
E 17
I 17
D 28
for the \*[bcopy] function, but it is typically available only to
E 28
I 28
for the \*[bcopy] function, hardware available only to
E 32
I 32
D 35
exceptions are typically have hardware specifically designed
E 35
I 35
D 44
exceptions typically have hardware specifically designed
E 35
for the \*[bcopy] function and available only to
E 44
I 44
exceptions usually have hardware specifically designed
for the \*[bcopy] function and that hardware may be available only to
E 44
E 32
E 28
D 50
the operating system.
E 50
I 50
the operating system.  
E 50
E 17
E 15
E 3
.PP
D 3
It is possible to generate loads of upwards of 500 IOPs on a single
SCSI disk using this technique.  It is useful to do that to figure out
how many drives could be supported on a system before there are no
more processor cycles to handle the load.  Using this trick, you 
do not have to hook up 30 drives, you simulate them.
E 3
I 3
D 15
We measure TCP bandwidth by creating two processes, a writer and a reader,
and having the writer write a variable amount of data (default is 3 megabytes)
to the reader.  The writer writes in 1 megabyte chunks.  Both the sending and
D 13
receiving buffer are page aligned; this makes a noticeable difference on
E 13
I 13
receiving buffer are page aligned, which makes a noticeable difference on
E 13
some systems.   
E 15
I 15
The \*[read] benchmark is implemented by rereading a file
D 44
(typically 8 megabytes) in 64 kilobyte
D 28
chunks.  Each chunk is summed in the user process.  The summing is
E 28
I 28
buffers.  Each buffer is summed in the user process.  The summing is
E 28
done for two reasons: (a) the memory mapped benchmark has to touch the
D 32
data so that benchmark sums which means this benchmark has to sum in
order to be an apples to apples comparison, and (b) it is possible that
E 32
I 32
data for an apples to apples comparison, and (b) it is possible that
E 32
the file system can DMA the data into memory but not into the processor
cache.  
D 17
SGI's XFS can move data
into memory at rates in excess of 500 megabytes per second.  
E 17
I 17
D 28
For example, SGI's XFS can move data
E 28
I 28
For example, \s-1SGI\s0's XFS can move data
E 28
into memory at rates in excess of 500 megabytes per second, but it
D 24
can only move data into cache at XXX megabytes per second.
.\" XXX --- Larry please fill in the XXX above...  Carl
E 24
I 24
D 28
can only move data into cache at 67 megabytes per second.
E 28
I 28
can only move data into cache at 68 megabytes per second.
E 28
E 24
E 17
The intent here is to measure performance delivered to the application,
not DMA performance to memory.
E 44
I 44
(typically 8M) in 64K
buffers.  Each buffer is summed as a series of integers in the user
D 49
process.  The summing is done for two reasons: (a) we want the
D 47
memory\-mapped benchmark to touch all the data for an apples\-to\-apples
E 47
I 47
memory-mapped benchmark to touch all the data for an apples-to-apples
E 47
comparison so we summed in that benchmark, and (b) it is possible that
the file system can DMA the data into memory at much higher rates than the
E 49
I 49
process.  The summing is done for two reasons: for an apples-to-apples
comparison the memory-mapped benchmark needs to touch all the data,
and the file system can sometimes transfer data into memory faster than the
E 49
processor can read the data.
For example, \s-1SGI\s0's XFS can move data into memory at
rates in excess of 500M per second, but it can move data into
D 49
cache at only 68M per second.  The intent here is to measure performance
E 49
I 49
D 52
cache at only 68M per second.  The intent is to measure performance
E 52
I 52
the cache at only 68M per second.  The intent is to measure performance
E 52
E 49
delivered to the application, not DMA performance to memory.
E 44
.TSTART
.so ../Results/tmp/bw_reread2.tbl
D 47
.TEND "File vs. memory bandwidth in MB/s"
E 47
I 47
.TEND "File vs. memory bandwidth (MB/s)"
E 47
E 15
.PP
D 15
Many socket implementations have a concept of socket 
buffers.  The typical (BSD based) default is 4 kilobytes each of send and
receive buffers.
Tuning the amount of socket buffer results in higher performance
in many cases.
This test attempts to set the sending sockets send buffers
to 1 megabyte, and the receiving sockets receive buffers to 1 megabyte.  If there is
not that much space available, it backs off 16 kilobyte steps until the set socket
buffer ioctl() succeeds.
E 15
I 15
The \*[mmap] interface provides a way to access the kernel's file cache 
D 17
without copying the data.  This promotes data sharing between processes
and saves the copy overhead.
E 15
.PP
E 17
I 17
without copying the data.  
E 17
D 15
As with the pipe test, the reader generates the timing
results (however, the writer prints them after getting them from the
reader through another socket).
D 10
.PP
E 10
I 10
D 11
.TSTART
E 10
D 5
XXX - results go here.
E 5
I 5
.so ../Results/tmp/bw_tcp_local.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "Loopback TCP bandwidth in MB/sec"
E 11
I 11
.\" .TSTART
.\" .so ../Results/tmp/bw_tcp_local.tbl
.\" .TEND "Loopback TCP bandwidth in MB/second"
E 11
E 10
D 4
It is interesting to compare these results with the bcopy() and pipe
E 4
I 4
It is interesting to compare these results with the \*[bcopy] and pipe
E 4
D 14
results.  Remember that the test is going to move the data into the
E 14
I 14
results.  For example, the AIX rs6000-990@71 has \*[bcopy], pipe, and TCP
bandwidths of 170.64MB/s, 84.19MB/s, and 10.49MB/s respectively.
Remember that the test is going to move the data into the
E 14
kernel from one process and back out of the kernel to another process.
In a naive implementation of loop back TCP, the additional overhead, beyond
byte copying, includes the TCP/IP protocol stack which includes a TCP checksum
that runs over all of the data.  Many vendors are realizing that the loop back
interface does not need to do that much work and have improved local TCP
performance by (a) setting the loop back MTU (packet or transfer size) to 
8 kilobytes or more, and (b) eschewing the TCP checksum in the loop back case.
When comparing against the pipe benchmark, it is easy to see that Solaris
has done this and that OSF and AIX have not.  
E 15
I 15
D 44
The \*[mmap] benchmark is implemented by mapping the entire file (typically 8 megabytes)
E 44
I 44
The \*[mmap] benchmark is implemented by mapping the entire file (typically 8M)
E 44
into the
D 44
process' address space.  The file is then summed, to force the data
E 44
I 44
process's address space.  The file is then summed to force the data
E 44
into the cache.
E 15
I 13
.PP
D 15
We do not include non-local TCP results because we could not get a
set of results for the machines included in this paper.  For the most
common transport layer, ethernet, the results are largely
uninteresting because nearly all the machines shown here can saturate
an ethernet.  For faster transport layers, such as Hippi, ATM or Myrinet,
we do not have access to enough different machines to generate
meaningful comparisons.  Note that to do a full non-local TCP test we
require two identical machines with a dedicated network connecting
them.  However, it is interesting to compare the measured bandwidth
as seen by applications over these high speed links and compare them
to the bandwidths advertised by the proponents of the various competing
network standards.
E 13
E 3
.NH 2
D 3
Memory read latency
.LP
This is perhaps the most interesting benchmark in the suite.  The
entire memory hierarchy is measured, including onboard cache latency
and size, external cache latency and size, main memory latency, and TLB
miss latency.  
E 3
I 3
Read bandwidth
E 3
.PP
D 3
The benchmark varies two parameters, array size and array stride.  
For each size, a list of pointers is created for all of the different 
strides.  Then the list is walked like so
.DS
.ft CB
mov  r0,(r0)  # C code: p = *p;
.DE
The time to do about fifty thousand loads (the list wraps) is measured and
reported.  The time reported is pure latency time and may be zero even though
the load instruction does not execute in zero time.  Zero is defined as one
clock cycle; in other words, the time reported is \fBonly\fP memory latency
time, as it does not include the instruction execution time.  It is assumed
that all processors can do a load instruction (not counting stalls) in one
processor cycle.  In other words, if the processor cache load time
is 60 nanoseconds on a 20 nanosecond processor, the load latency reported
would be 40 nanoseconds, the missing 20 seconds is for the load instruction
itself.  Processors that can manage to get the load address out to the 
address pins before the end of the load cycle get some free time in this
benchmark (I don't think any processors can do that).
E 3
I 3
D 4
The read(2) interface moves data from the kernel's file data cache into the
E 4
I 4
The \*[read] interface moves data from the kernel's file data cache into the
E 4
process' buffer.  
E 3
.PP
D 3
Note that this benchmark has been validated by logic analyzer measurements
on an SGI indy. The
clever reader might realize that last few nanoseconds of inaccuracy could be
rounded off by realizing that the latency is always going to be a multiple
of the processor clock rate.
E 3
I 3
The benchmark is measuring reread performance, in other words, the data
is first faulted into the cache.
The benchmark here is explicitly not an I/O benchmark, no disk activity is
D 4
involved (unless the machine is incapable of caching 8 megabytes of data).  
E 4
I 4
involved (unless the machine is incapable of caching the data - note that 
the data cached is auto sized to fit in memory).  
E 4
E 3
.PP
D 3
The raw data is a series of data sets.  Each data set is a stride size,
with array size varied from about one kilobyte up to eight megabytes.
When these data sets are all plotted together (using a log base 2 scale
for the size variable), the data will be seen to contain a series of 
horizontal plateaus.  The first is the onboard data cache latency (if there
is an onboard cache).  The point where the lines start to go up marks the
size of the cache.  The second is the external cache, the third is the
main memory, and the last is main memory plus TLB miss cost.  In addition
to this information, the cache line size can be derived by noticing which
strides are faster than main memory times.  The first stride that is
main memory speed is likely to be the cache line size.  The reason is
that the strides that are faster than memory indicate that the benchmark is
getting more than one hit per cache line.  Note that prefetching may confuse
you.
E 3
I 3
D 4
The benchmark is implemented by rereading an 8 megabyte file in 64 kilobyte
E 4
I 4
The benchmark is implemented by rereading a file (typically 8 megabytes) in 
64 kilobyte
E 4
chunks.  Each chunk is summed in the user process.  The summing is
done for two reasons: (a) the memory mapped benchmark has to touch the
data so that benchmark sums which means this benchmark has to sum in
order to be an apples to apples comparison, and (b) it is possible that
the file system can DMA the data into memory but not into the processor
cache.  Some file systems, such as SGI's XFS, can DMA at data rates
far in excess of what a processor could read.  SGI's XFS can move data
into memory at rates in excess of 500 megabytes per second.  The processors
used on that machine do not have the memory bandwidth, to one processor,
to actually do anything with that data.  Contrast the 500 megabytes per
second number with the memory read rates in a previous benchmark.
The intent here is to measure performance delivered to the application,
not DMA performance to memory.
E 3
D 10
.PP
E 10
I 10
D 11
.TSTART
E 10
D 3
The graph below shows a particularly nicely made machine, a DEC alpha.
This machine is nice because (a) it shows the latencies and sizes of
the on chip level 1 and motherboard level 2 caches, and (b) because it
has the best all around numbers, especially considering it can support a
4MB level 2 cache.  Nice work, DEC.
.so mem.pic
.NH 1
Bandwidth measurements
.LP
One of my former managers\** once noted that ``Unix is Swahili for bcopy().''
I believe that he was indicating his belief that the operating system spent
E 3
I 3
D 5
XXX results go here.
E 5
I 5
.so ../Results/tmp/bw_reread.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "File reread bandwidth in MB/sec"
E 11
I 11
.\" .TSTART
.\" .so ../Results/tmp/bw_reread.tbl
.\" .TEND "File reread bandwidth in MB/second"
.PP
E 11
E 10
D 4
It is interesting to compare these results with the bcopy() results.  A reread
E 4
I 4
It is interesting to compare these results with the \*[bcopy] results.  A reread
E 4
is basically a bcopy plus the overhead of traversing the operating system's
page cache.  The difference in performance reflects the cost of the
operating file and/or virtual memory systems.  In an ideal world, the results
here would be the same or better than the bcopy results.  The reason that the
reread results may be better is that the kernel can typically bcopy faster
than a user application - block copy hardware is usually only accessible from
the kernel.
.PP
War story about Galaxy read performance.
I 11
.TSTART
.so ../Results/tmp/bw_reread2.tbl
.TEND "File reread bandwidth in MB/second"
E 11
E 3
D 3
most of its time moving data from one place to another, via various means.
I tend to agree and have measured the various ways that data can be moved.
The ways that are measured are: through pipes, TCP sockets, library bcopy()
and hand unrolled bcopy(), the read() interface, through the mmap() interface,
and direct memory read and write (no copying).
E 3
I 3
.NH 2
Memory mapped read bandwidth
.PP
D 4
The mmap(2) interface provides a way to access the kernel's file cache 
E 4
I 4
The \*[mmap] interface provides a way to access the kernel's file cache 
E 4
without copying the data.  This promotes data sharing between processes
and saves the copy overhead.
.PP
The benchmark is measuring reread performance, in other words, the data
is first faulted into the cache.
The benchmark here is explicitly not an I/O benchmark, no disk activity is
D 4
involved (unless the machine is incapable of caching 8 megabytes of data).  
E 4
I 4
involved (unless the machine is incapable of caching the data - note that 
the data cached is auto sized to fit in memory).  
E 4
.PP
D 4
The benchmark is implemented by mapping the entire 8 megabyte file into the
E 4
I 4
The benchmark is implemented by mapping the entire file (typically 8 megabytes)
into the
E 4
process' address space.  The file is then summed, to force the data
into the cache.
D 10
.PP
E 10
I 10
D 11
.TSTART
E 10
D 5
XXX results go here.
E 5
I 5
.so ../Results/tmp/bw_mmap.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "Mmap reread bandwidth in MB/sec"
E 11
I 11
.\" .TSTART
.\" .so ../Results/tmp/bw_mmap.tbl
.\" .TEND "Mmap reread bandwidth in MB/second"
.PP
E 11
E 10
It is interesting to compare these results with the reread results.  
You can detect whether or not the operating system has done a reasonable
virtual memory page cache and file buffer cache integration.  In those 
systems with a reasonable FS/VM subsystem, this benchmark should result
in better results than the reread results.   The AIX system at 106 megabytes
per second
clearly has some problems (although a 106 megabytes per second
is still the best result,
it is not as good as the corresponding read result).
E 15
I 15
D 28
In Figure \n[FIGURE], contrast C library \*[bcopy] speed with \*[read] and 
memory read speed (\f(CWmemrd\fP) with \*[mmap] read speed.  Remember that
the \*[read] interface has access to block copy hardware and may be faster.
I 24
.\" XXX - fix that paragraph when I change the table.
E 28
I 28
D 32
In Figure \n[FIGURE], what we are looking for is that the \fIFile read\fP
column is as fast as (or faster than) the \fILibc bcopy\fP column.  
The file reread can be faster because the kernel may have access to
hardware \*[bcopy] assist not available to the C library.  The other
goal is that \fIFile mmap\fP reread approaches \fIMemory read\fP
performance.  From the results, this looks to be a harder problem
E 32
I 32
D 35
In Figure \n[FIGURE], the \fILibc bcopy\fP is usually faster than
the \fIFile read\fP column, but
file reread can be faster because the kernel may have access to
E 35
I 35
D 44
In Figure \n[FIGURE], 
E 44
I 44
In Table \n[TABLE], 
E 44
a good system will have \fIFile read\fP as fast as (or even faster than)
\fILibc bcopy\fP because as the file system overhead goes to zero, the
D 49
file reread case is virtually the same as the library bcopy case.
E 49
I 49
file reread case is virtually the same as the library \*[bcopy] case.
E 49
However, file reread can be faster because the kernel may have access to
E 35
\*[bcopy] assist hardware not available to the C library.  
D 35
Also, \fIFile mmap\fP performance should approach \fIMemory read\fP
E 35
I 35
Ideally, \fIFile mmap\fP performance should approach \fIMemory read\fP
E 35
D 49
performance, but mmap is often dramatically worse.  
E 49
I 49
performance, but \*[mmap] is often dramatically worse.  
E 49
D 50
Judging by the results, this looks to be a hard problem
E 32
or a potential area for operating system improvements.
E 50
I 50
Judging by the results, this looks to be a 
potential area for operating system improvements.
E 50
I 48
.PP
In Table \n[TABLE] the Power2 does better on file reread than bcopy because it takes
full advantage of the memory subsystem from inside the kernel.  
The mmap reread is probably slower because of the lower clock rate;
the page faults start to show up as a significant cost.   
.PP
It is surprising that the Sun Ultra1 was able to bcopy at the high
rates shown in Table 2 but did not show those rates for file reread
in Table \n[TABLE].
HP has the opposite problem, they get file reread faster than bcopy,
perhaps because the kernel \*[bcopy] has access to hardware support.
.PP
The Unixware system has outstanding mmap reread rates, better than
systems of substantially higher cost.  Linux needs to do some work on
the \f(CWmmap\fP code.
E 48
E 28
E 24
E 15
I 8
D 10
.NH 2
Bandwidth comparisons
.PP
It is interesting to compare the various bandwidth measurements for the
various machines.
.PP
.so bw.pic
.PP
E 10
E 8
.NH 1
Latency measurements
.PP
D 15
Latency measurements are an interesting area.  Latency is an often overlooked
E 15
I 15
D 44
Latency is an often overlooked
E 44
I 44
D 47
Latency is an often\-overlooked
E 47
I 47
Latency is an often-overlooked
E 47
E 44
E 15
D 18
area of performance problems.  This may be because resolving latency issues
E 18
I 18
area of performance problems, possibly because resolving latency issues
E 18
is frequently much harder than resolving bandwidth issues.  For example,
D 44
memory bandwidth may increased by making wider cache lines and increasing
E 44
I 44
memory bandwidth may be increased by making wider cache lines and increasing
E 44
D 22
memory ``width'' and interleave.\**
E 3
.FS
D 3
Ken Okin
E 3
I 3
This often referred to as ``throwing money at the problem.''
E 3
.FE
E 22
I 22
D 23
memory ``width'' and interleave.
E 22
I 3
D 18
Latency is much more difficult.  Latency tends to creep in nanosecond by
E 18
I 18
Latency can only be improved by shortening paths or increasing
E 23
I 23
memory ``width'' and interleave,
D 44
but memory latency can only be improved by shortening paths or increasing
E 44
I 44
but memory latency can be improved only by shortening paths or increasing
E 44
E 23
(successful) prefetching.  
D 23
Latency tends to creep in nanosecond by
E 18
nanosecond, line of code by line of code.  However, the first steps 
towards improving latency are understanding the current latencies of
your system.
E 23
I 23
D 44
The first step towards improving latency is understanding the 
current latencies in your system.
E 44
I 44
The first step toward improving latency is understanding the 
current latencies in a system.
E 44
E 23
.PP
The latency measurements included in this suite are
I 17
D 44
memory latency.
E 44
I 44
memory latency,
E 44
E 17
basic operating system entry cost,
signal handling cost,
process creation times,
context switching,
D 44
inter process communication,
virtual memory system latency,
E 44
I 44
interprocess communication,
.\" virtual memory system latency,
E 44
file system latency, 
D 17
disk latency (optionally),
and memory latency.
E 17
I 17
D 44
and disk latency (optional).
E 44
I 44
and disk latency.
E 44
.NH 2 
Memory read latency background
.PP
I 22
D 24
XXX - this section needs to be trimmed down.
.PP
E 24
D 44
We expend considerable effort to define the different memory
latencies and to explain and justify our benchmark in this section.
E 44
I 44
In this section, we expend considerable effort to define the different memory
latencies and to explain and justify our benchmark.
E 44
The background is a bit tedious but important, since we believe the
memory
D 44
latency measurements to be one of the most thought provoking and useful
E 44
I 44
D 47
latency measurements to be one of the most thought\-provoking and useful
E 47
I 47
latency measurements to be one of the most thought-provoking and useful
E 47
E 44
measurements in \*[lmbench].  
D 44
Readers already familiar with memory latency definitions may skip over
this background information.
E 44
I 44
D 49
(Readers already familiar with memory latency definitions may skip over
this background information.)
E 49
E 44
.PP
E 22
D 49
The most basic latency measurement is memory latency because most of
E 49
I 49
The most basic latency measurement is memory latency since most of
E 49
the other latency measurements can be expressed in terms of memory
latency.  For example, context switches require saving the current
D 44
process state and loading the next process's state.  However, memory
D 22
latency is rarely measured and is poorly understood by most software
developers.
E 22
I 22
latency is rarely measured and frequently misunderstood.
E 44
I 44
process state and loading the state of the next process.  However, memory
latency is rarely accurately measured and frequently misunderstood.
E 44
E 22
.PP
D 44
Memory read latency has many definitions because there are many different
numbers all defined as some sort of read latency.  The most common definitions,
E 44
I 44
Memory read latency has many definitions;
the most common,
E 44
in increasing time order,
D 44
are memory chip cycle time, processor pins to memory and back time, load
in a vacuum time, and back to back load time.  
E 44
I 44
D 47
are memory chip cycle time, processor\-pins\-to\-memory\-and\-back time,
load\-in\-a\-vacuum time, and back\-to\-back\-load time.  
E 47
I 47
are memory chip cycle time, processor-pins-to-memory-and-back time,
load-in-a-vacuum time, and back-to-back-load time.  
E 47
E 44
D 22
.BU
\fIMemory chip cycle latency\fP
Memory chips are rated in nanoseconds, typical speeds are around 60 nanoseconds.
D 18
This is sometimes referred to as the chip cycle time.  That is a bit misleading,
since the chips are operated as a two dimensional array and need one cycle
for each of the RAS (row access strobe) and CAS (column access strobe) 
before the memory is available.  This means that 60 nanosecond memory
chips really are more like 120 nanosecond memory chips.  Some systems
E 18
I 18
This is sometimes referred to as the chip latency.  The chip cycle
time is the chip latency plus the time required to restore the data
in the capacitors which is often referred to as the precharge time.
This means that 60 nanosecond memory
chips really are more like 100 nanosecond memory chips.  Some systems
E 18
operate memory in ``page mode'' or ``static column'' memory systems
hold either RAS or CAS and allow subsequent accesses in the same row
or column in one cycle instead of two.
.BU
\fIPin to pin latency\fP
In addition to the memory cycle time, there is the time that it takes for the
memory request to travel from the processor's pins to the memory subsystem
and back again.  The paper describing the DEC 8400 quotes memory latencies of
265 nanoseconds; these numbers are the pin to pin values.
E 22
I 22
.BU "Memory chip cycle latency" :
D 44
Memory chips are rated in nanoseconds, typical speeds are around 60
D 24
nanoseconds.  While the chips themselves may return a single datum
in 60 nanoseconds, that performance is rarely, if ever, reached in real
systems.  For example, most systems offer a wide range of memory 
capacity, from 64MB to 1GB or more.  If 64MB simms are used, the number
of simms range from 1 to 16.  The more simms there are, the more 
capacitance there is in the memory subsystem.  More capacitance means
longer setup times for the fully populated memory subsystem.  System
designers have to allow time for this setup.
For more details, consult [XXX - reference on DRAM].
E 24
I 24
D 28
nanoseconds.  The 60 nanosecond time is the shortest amount of time 
that it would take to read one item from the chip.
E 28
I 28
nanoseconds.  The specific information we describe here is from 
E 44
I 44
D 49
Memory chips are rated in nanoseconds; typical speeds are around 60 ns.
E 49
I 49
Memory chips are rated in nanoseconds; typical speeds are around 60ns.
E 49
D 48
The specific information we describe here is from 
E 44
.RN Toshiba94 , 
E 48
I 48
D 50
General overview on DRAM architecture may be found in
.RN Hennessy96 ,
and the 
E 50
I 50
A general overview on DRAM architecture may be found in
.RN Hennessy96 .
The 
E 50
specific information we describe here is from 
.RN Toshiba94  
E 48
and pertains to the \s-1THM361020AS-60\s0 module and \s-1TC514400AJS\s0
D 44
\s-1DRAM\s0 used in \s-1SGI\s0 workstations.  The 60 nanoseconds time is the
E 44
I 44
D 49
\s-1DRAM\s0 used in \s-1SGI\s0 workstations.  The 60 ns time is the
E 49
I 49
\s-1DRAM\s0 used in \s-1SGI\s0 workstations.  The 60ns time is the
E 49
E 44
I 33
time from 
E 33
I 29
D 32
time from 
E 32
E 29
.ps -1
.nr width \w'R\&A\&S'
.nr height \n[rst]+1000
RAS\v'-\n[height]u'\h'-\n[width]u'\fB\l'\n[width]u'\fP\v'\n[height]u'
.ps
D 29
pulse time, or the time that a signal must be held constant in order for
the data to be available on the \s-1DRAM\s0 pins.  While it is possible
E 29
I 29
D 32
assertion to the when 
the data to be available on the \s-1DRAM\s0 pins (assuming 
.ps -1
.nr width \w'C\&A\&S'
.nr height \n[rst]+1000
CAS\v'-\n[height]u'\h'-\n[width]u'\fB\l'\n[width]u'\fP\v'\n[height]u'
.ps
access time requirements were met).
While it is possible
E 32
I 32
D 33
pulse time, or the time that a signal must be held constant in order for
the data to be available on the \s-1DRAM\s0 pins.  While it is possible
E 33
I 33
assertion to the when 
D 48
the data to be available on the \s-1DRAM\s0 pins (assuming 
E 48
I 48
the data will be available on the \s-1DRAM\s0 pins (assuming 
E 48
.ps -1
.nr width \w'C\&A\&S'
.nr height \n[rst]+1000
CAS\v'-\n[height]u'\h'-\n[width]u'\fB\l'\n[width]u'\fP\v'\n[height]u'
.ps
access time requirements were met).
While it is possible
E 33
E 32
E 29
D 44
to get data out of a \s-1DRAM\s0 in 60 nanoseconds, that is not all of
E 44
I 44
D 49
to get data out of a \s-1DRAM\s0 in 60 ns, that is not all of
E 49
I 49
to get data out of a \s-1DRAM\s0 in 60ns, that is not all of
E 49
E 44
the time involved.  There is a precharge time that must occur after
every access.
.RN Toshiba94
D 44
quotes 110 nanoseconds as the random read or write cycle time, and this
E 44
I 44
D 49
quotes 110 ns as the random read or write cycle time, and this
E 49
I 49
D 50
quotes 110ns as the random read or write cycle time, and this
E 50
I 50
quotes 110ns as the random read or write cycle time and this
E 50
E 49
E 44
time is more representative of the cycle time.  
E 28
.\" For example, most systems offer a wide range of memory 
.\" capacity, from 64MB to 1GB or more.  If 64MB simms are used, the number
.\" of simms range from 1 to 16.  The more simms there are, the more 
.\" capacitance there is in the memory subsystem.  More capacitance means
.\" longer setup times for the fully populated memory subsystem.  System
.\" designers have to allow time for this setup.
.\" For more details, consult [XXX - reference on DRAM].
E 24
.\" This is sometimes referred to as the chip latency.  The
.\" chip cycle time is the chip latency plus the time required to restore
.\" the data in the capacitors which is often referred to as the precharge
.\" time.  This means that 60 nanosecond memory chips really are more like
.\" 100 nanosecond memory chips.  Some systems operate memory in ``page
.\" mode'' or ``static column'' memory systems hold either RAS or CAS and
.\" allow subsequent accesses in the same row or column in one cycle instead
.\" of two.
D 44
.BU "Pin to pin latency" :
In addition to the memory cycle time, there is the time that it takes
E 44
I 44
D 47
.BU "Pin\-to\-pin latency" :
E 47
I 47
.BU "Pin-to-pin latency" :
E 47
This number represents the time needed
E 44
for the memory request to travel from the processor's pins to the memory
D 44
subsystem and back again.  Many vendors have used the pin to pin
E 44
I 44
D 47
subsystem and back again.  Many vendors have used the pin\-to\-pin
E 47
I 47
subsystem and back again.  Many vendors have used the pin-to-pin
E 47
E 44
D 28
definition of memory latency in their reports.  For example, the DEC
E 28
I 28
D 49
definition of memory latency in their reports.  For example, the \s-1DEC\s0
E 28
8400 paper 
E 49
I 49
definition of memory latency in their reports.  For example, 
E 49
D 44
.RN Fenwick95 ,
D 31
quotes memory latencies of 265; a careful
E 31
I 31
D 32
quotes memory latencies of 265 nanoseconds; a careful
E 32
I 32
D 33
quotes memory latencies of 265; a careful
E 33
I 33
quotes memory latencies of 265 nanoseconds; a careful
E 33
E 32
E 31
reading of that paper shows that these are pin to pin numbers.  In spite
E 44
I 44
.RN Fenwick95 
D 49
quotes memory latencies of 265 ns; a careful
E 49
I 49
while describing the \s-1DEC\s0 8400
quotes memory latencies of 265ns; a careful
E 49
D 47
reading of that paper shows that these are pin\-to\-pin numbers.  In spite
E 47
I 47
reading of that paper shows that these are pin-to-pin numbers.  In spite
E 47
E 44
of the historical precedent in vendor reports, this definition of memory
latency is misleading since it ignores actual delays seen when a load
instruction is immediately followed by a use of the data being loaded.
The number of additional cycles inside the processor can be significant
and grows more significant with today's highly pipelined architectures.
I 28
.PP
D 44
It is worth noting that the pin to pin numbers 
E 44
I 44
D 47
It is worth noting that the pin\-to\-pin numbers 
E 47
I 47
It is worth noting that the pin-to-pin numbers 
E 47
E 44
include the amount of time it takes to charge
the lines going to the \s-1SIMM\s0s, a time that increases with the
(potential) number of \s-1SIMM\s0s in a system.  More \s-1SIMM\s0s mean
more capacitance which requires in longer charge times.  This is one reason
why personal computers frequently have better memory latencies than
workstations: the PCs typically have less memory capacity.
E 28
E 22
D 24
.\" XXX Larry
.\" Processor pins or cache pins?  DEC 8400 spends 140ns getting data
.\" from the processor pins into the processor?  (or 70ns getting
.\" address off and 70ns getting data back?)
D 22
.BU
\fILoad in a vacuum latency\fP
E 22
I 22
.\" Larry's reply: yeah, that's the cost of getting the load to the pins,
.\" stalling the pipeline, getting the data back from the pins, and restarting
.\" the pipeline.
E 24
D 44
.BU "Load in a vacuum latency" :
E 22
A load in vacuum is the time that the processor will wait for one load that
must be fetched from main memory, i.e., is a cache miss.  The vacuum part
E 44
I 44
D 47
.BU "Load\-in\-a\-vacuum latency" :
E 47
I 47
.BU "Load-in-a-vacuum latency" :
E 47
A load in a vacuum is the time that the processor will wait for one load that
must be fetched from main memory (i.e., a cache miss).  The ``vacuum'' 
E 44
means that there is no other activity on the system bus, including no other
D 22
loads.  Systems architects frequently refer to this value as the memory
latency.
.BU
\fIBack to back load latency\fP
Back to back load latency is the time that each load takes, assuming that the
instruction before and after was also a cache missing load.  Back to back
loads may take longer than loads in a vacuum for the following reason: 
many systems implement something known as \fIcritical word first\fP, 
which means that the sub block of the cache line that contains the word being
loaded is delivered to the processor before the entire cache line has been
brought into the cache.  If another load occurs quickly enough after the 
processor gets restarted from the current load, the second load may get
stalled because the cache is still busy filling the cache line for the 
previous load.  On some unnamed future systems, the difference between
back to back and load in a vacuum will be about 35%.
E 22
I 22
loads.  
D 44
While this is frequently quoted as the memory latency, it is not a very
useful number.  It is basically a ``not to exceed number'' important only for
E 44
I 44
While this number is frequently used as the memory latency, it is not very
useful.  It is basically a ``not to exceed'' number important only for
E 44
marketing reasons.
D 44
Some architects will point out that since most processors implement non\-blocking
loads (the load does not cause a stall until the data is used) the perceived
E 44
I 44
Some architects point out that since most processors implement nonblocking
loads (the load does not cause a stall until the data is used), the perceived
E 44
load latency may be much less that the real latency.  When pressed, however,
D 44
most will admit that cache misses occur in bursts, which result in perceived
latencies of at least the load in a vacuum latency.
.BU "Back to back load latency" :
Back to back load latency is the time that each load takes, assuming
that the instruction before and after was also a cache missing load.
Back to back loads may take longer than loads in a vacuum for the
E 44
I 44
most will admit that cache misses occur in bursts, resulting in perceived
D 47
latencies of at least the load\-in\-a\-vacuum latency.
.BU "Back\-to\-back\-load latency" :
Back\-to\-back\-load latency is the time that each load takes, assuming
that the instructions before and after were also cache\-missing loads.
Back\-to\-back loads may take longer than loads in a vacuum for the
E 47
I 47
latencies of at least the load-in-a-vacuum latency.
.BU "Back-to-back-load latency" :
Back-to-back-load latency is the time that each load takes, assuming
D 50
that the instructions before and after were also cache-missing loads.
E 50
I 50
that the instructions before and after are also cache-missing loads.
E 50
Back-to-back loads may take longer than loads in a vacuum for the
E 47
E 44
following reason: many systems implement something known as \fIcritical
D 44
word first\fP, which means that the sub block of the cache line that
E 44
I 44
word first\fP, which means that the subblock of the cache line that
E 44
contains the word being loaded is delivered to the processor before the
entire cache line has been brought into the cache.  If another load
occurs quickly enough after the processor gets restarted from the
D 50
current load, the second load may get stalled because the cache is still
E 50
I 50
current load, the second load may stall because the cache is still
E 50
busy filling the cache line for the previous load.  On some systems,
D 44
such as the current implementation of Ultrasparc, 
E 44
I 44
such as the current implementation of UltraSPARC, 
E 44
the difference between back to back and load in a vacuum is about 35%.
E 22
D 24
.PP
E 24
I 24
D 28
.BU "Stall analysis" .
E 24
The multiplicity of memory latency measures has caused some systems 
architects a bit of frustration
when customers measure back to back memory latency and get numbers
worse than what the
systems architects said they would get.  
D 20
\*[lmbench] uses the back to back load latency definition.
We are sticking with this definition
E 20
I 20
\*[lmbench] uses the back to back load latency definition
E 20
because we feel that it is what most software people consider to be memory
D 24
latency.  Consider the following code fragment:
E 24
I 24
latency.  Consider the following C code fragment:
E 24
.DS
.nf
.ft CW
p = head;
while (p->p_next)
	p = p->p_next;
.ft
.fi
.DE
On a DEC Alpha, the loop part turns into three instructions, including the
load.  A 300Mhz processor has a 3.33 nanosecond cycle time, so the loop
could execute in slightly less than 10 nanoseconds.  However, the load itself
will take 400 nanoseconds on a 300Mhz DEC 8400.  In other words, the 
instructions cost 10 nanoseconds but the load stalled for 400.  Another
way to look at it is that there would need to be 400/3.3 or 121 non-dependent,
D 20
non-loading
instructions following the load to hide the load latency.  Super scalar 
processors are even worse off since since they really want to be doing
D 18
more than one thing per cycle.
E 18
I 18
more than one operation per cycle.
E 20
I 20
non-loading instructions following the load to hide the load latency.  
Because super scalar processors typically execute multiple operations
per clock cycle, they need even more useful operations between cache
misses to keep the processor from stalling.
I 24
.BU "Justification for back to back load benchmark" .
As as microbenchmark, the back to back load latency is interesting
because it reflects the overall goodness of the implementation of large
or small cache lines.  Architects like large cache lines, up to 64
bytes or so, with large caches, because the prefetch effect causes a
higher hit rate, if there is reasonable spatial locality of data
references.
E 24
E 20
E 18
.PP
I 22
D 24
As as microbenchmark, the back to back load latency is interesting because it reflects the  
overall goodness of the implementation of large or small cache lines.  
Architects like large cache lines, up to 64 bytes or so, with large  
caches, because the prefetch effect causes a higher hit rate, if there  
is reasonable spatial locality of data references.  
.PP
This benchmark, for the larger strides goes against this assumption of  
E 24
I 24
This benchmark, for the larger strides, goes against this assumption of  
E 24
spatial locality. So it provides insight into the negative effects of  
large line prefetch.  Namely:  
.BU 
Multi-cycle fill operations are typically atomic events at the 
caches, and sometimes block other cache accesses until they
complete.
.BU
Caches are typically single-ported. Having a large line prefetch
of unused data, at the very least, causes extra bandwidth
demands at the cache, and can cause increased access latency for 
normal cache accesses.
E 28
I 28
D 31
.BU "\f(CBlmbench\fB chooses back to back loads"
E 31
I 31
D 32
.BU "\f(CBlmbench\fB measures back to back loads."
E 31
.ft R
As a microbenchmark, the back to back load latency is interesting
because it reflects the overall goodness of the implementation of large
or small cache lines.  Architects like large cache lines, up to 64
bytes or so, with large caches, because the prefetch effect causes a
higher hit rate, if there is reasonable spatial locality of data
references.
E 32
E 28
.PP
I 28
D 32
This benchmark, for the larger strides, goes against this assumption of  
spatial locality. So it provides insight into the negative effects of  
large line prefetch.  Namely:  
.BU 
Multi-cycle fill operations are typically atomic events at the 
caches, and sometimes block other cache accesses until they
complete.
.BU
Caches are typically single-ported. Having a large line prefetch
of unused data, at the very least, causes extra bandwidth
demands at the cache, and can cause increased access latency for 
normal cache accesses.
.PP
A practical reason for choosing back to back loads is that it is the
only measurement that may be easily measured from software.
In addition, \*[lmbench] uses the back to back load latency definition
E 32
I 32
D 44
\*[lmbench] measures back to back load latency because it is the
E 44
I 44
D 47
\*[lmbench] measures back\-to\-back\-load latency because it is the
E 47
I 47
\*[lmbench] measures back-to-back-load latency because it is the
E 47
E 44
only measurement that may be easily measured from software and
E 32
D 44
because we feel that it is what most software people consider to be memory
E 44
I 44
because we feel that it is what most software developers consider to be memory
E 44
latency.  Consider the following C code fragment:
.DS
.nf
.ft CW
p = head;
while (p->p_next)
	p = p->p_next;
.ft
.fi
.DE
On a \s-1DEC\s0 Alpha, the loop part turns into three instructions, including the
D 44
load.  A 300Mhz processor has a 3.33 nanosecond cycle time, so the loop
could execute in slightly less than 10 nanoseconds.  However, the load itself
will take 400 nanoseconds on a 300Mhz \s-1DEC\s0 8400.  In other words, the 
instructions cost 10 nanoseconds but the load stalled for 400.  Another
way to look at it is that there would need to be 400/3.3 or 121 non-dependent,
non-loading instructions following the load to hide the load latency.  
Because super scalar processors typically execute multiple operations
E 44
I 44
D 49
load.  A 300 Mhz processor has a 3.33 ns cycle time, so the loop
could execute in slightly less than 10 ns.  However, the load itself
takes 400 ns on a 300 Mhz \s-1DEC\s0 8400.  In other words, the 
instructions cost 10 ns but the load stalls for 400.  Another
E 49
I 49
load.  A 300 Mhz processor has a 3.33ns cycle time, so the loop
could execute in slightly less than 10ns.  However, the load itself
takes 400ns on a 300 Mhz \s-1DEC\s0 8400.  In other words, the 
instructions cost 10ns but the load stalls for 400.  Another
E 49
way to look at it is that 400/3.3, or 121, nondependent,
nonloading instructions following the load would be needed 
to hide the load latency.  
Because superscalar processors typically execute multiple operations
E 44
per clock cycle, they need even more useful operations between cache
misses to keep the processor from stalling.
.PP
I 32
This benchmark illuminates the tradeoffs in processor cache design.
Architects like large cache lines, up to 64 bytes or so, because 
the prefetch effect of gathering a whole line increases
hit rate given reasonable spatial locality.
Small stride sizes have high spatial locality and should have higher
performance, but large stride sizes have poor spatial locality causing
the system to prefetch useless data.
D 44
So the benchmark provides insight into both the positive and negative 
effects of large line prefetch.  Namely:  
E 44
I 44
So the benchmark provides the following insight into negative 
effects of large line prefetch:
E 44
.BU 
Multi-cycle fill operations are typically atomic events at the 
caches, and sometimes block other cache accesses until they
complete.
.BU
Caches are typically single-ported. Having a large line prefetch
D 49
of unused data, at the very least, causes extra bandwidth
E 49
I 49
of unused data causes extra bandwidth
E 49
demands at the cache, and can cause increased access latency for 
normal cache accesses.
.PP
E 32
E 28
D 24
Even though the strides are larger for this micro benchmark, the  
incrementing address pattern can make drams operating in "page mode"  
work well. The successive cache misses have spatial locality, in the  
dram's view, since they can go to the same row. Some vendors optimize  
the assignment of physical address bits to dram row addresses, to  
increase the likelihood that cache misses for incrementing addresses  
occur on the same row.  
.PP
However, address traces for operating system and user applications
seem to indicate that the  
actual patterns of addresses to memory are more random, and use of  
page mode between transactions can be a disadvantage. A future version  
of this benchmark may have a more random addressing pattern to avoid  
this ``false positive'' indication.  
.PP
Arguably, a memory system optimized for this microbenchmark, would
have no cache, and only return 4 bytes of data for every load, with  
the dram operating in page mode.  Sub-100 nanosecond latency numbers would be  
reported. However, such a memory system would do poorly on other  
microbenchmarks in \*[lmbench].  
.PP
The latency number reported by \*[lmbench] is useful in understanding the  
overall tradeoffs made in a memory design. It has some aspects of a  
throughput measurement in it, which is desireable. There has been too  
much history of reporting memory access delays that misrepresent the  
performance of applications with large data sets, and also, too much history  
of reporting large bandwidth numbers that don't reflect back on good  
application performance.  
.PP
High bandwidth is great, but not at the expense of latency, or  
difficulty of use.  Requiring multiple processors or vectorization to  
achieve the reported peak bandwidths has historically not served the  
interests of the average user application.  
.PP
E 24
E 22
D 18
In summary, we believe that processors are more than fast enough that the
load latency seen on average for loads that miss, will be closer to the
back to back load number than the load in a vacuum number.
E 18
I 18
In summary, we believe that processors are so fast that the average
load latency for cache misses will be closer to the
D 24
``back to back'' load number than the ``load in a vacuum'' number.
E 18
.NH 3 
E 24
I 24
D 28
back to back load number than the load in a vacuum number and
that is why we implemented a back to back load latency benchmark.
E 28
I 28
D 44
back to back load number than the load in a vacuum number.  We are
E 44
I 44
D 47
back\-to\-back\-load number than to the load\-in\-a\-vacuum number.  We are
E 47
I 47
back-to-back-load number than to the load-in-a-vacuum number.  We are
E 47
E 44
hopeful that the industry will standardize on this definition of
memory latency.
E 28
.NH 2 
E 24
Memory read latency
.PP
D 32
This is perhaps the most interesting benchmark in the suite.  The
D 28
entire memory hierarchy may be measured, including on board cache latency
D 24
and size, external cache latency and size, main memory latency, and TLB
miss latency.  
E 24
I 24
and size, external cache latency and size, and main memory latency.
E 28
I 28
entire memory hierarchy may be measured, including on board data cache latency
and size, external data cache latency and size, and main memory latency.
E 32
I 32
D 44
The entire memory hierarchy may be measured, including on board data 
E 44
I 44
D 47
The entire memory hierarchy can be measured, including on\-board data 
E 47
I 47
The entire memory hierarchy can be measured, including on-board data 
E 47
E 44
cache latency and size, external data cache latency and size, and 
main memory latency.
E 32
Instruction caches are not measured.
E 28
D 44
TLB miss latency could be measured, as in 
E 44
I 44
TLB miss latency can also be measured, as in 
E 44
D 28
.RN Saavedra92 .
E 28
I 28
.RN Saavedra92 ,
but we stopped at main memory.  Measuring TLB miss time is problematic
because different systems map different amounts of memory with their 
TLB hardware.
E 28
E 24
.PP
The benchmark varies two parameters, array size and array stride.  
For each size, a list of pointers is created for all of the different 
D 44
strides.  Then the list is walked like so
E 44
I 44
strides.  Then the list is walked thus:
E 44
.DS
.ft CW
D 48
mov  r0,(r0)  # C code: p = *p;
E 48
I 48
mov  r4,(r4)  # C code: p = *p;
E 48
.ft
.DE
D 44
The time to do about fifty thousand loads (the list wraps) is measured and
E 44
I 44
The time to do about 1,000,000 loads (the list wraps) is measured and
E 44
reported.  The time reported is pure latency time and may be zero even though
the load instruction does not execute in zero time.  Zero is defined as one
clock cycle; in other words, the time reported is \fBonly\fP memory latency
time, as it does not include the instruction execution time.  It is assumed
D 49
that all processors can do a load instruction (not counting stalls) in one
processor cycle.  In other words, if the processor cache load time
D 44
is 60 nanoseconds on a 20 nanosecond processor, the load latency reported
D 28
would be 40 nanoseconds, the missing 20 seconds is for the load instruction
E 28
I 28
would be 40 nanoseconds, the missing 20 nanoseconds is for the load instruction
E 28
itself.  Processors that can manage to get the load address out to the 
E 44
I 44
is 60 ns on a 20 ns processor, the load latency reported
would be 40 ns, the missing 20 ns is for the load instruction
E 49
I 49
that all processors can do a load instruction in one processor cycle 
(not counting stalls).  In other words, if the processor cache load time
is 60ns on a 20ns processor, the load latency reported
would be 40ns, the additional 20ns is for the load instruction
E 49
itself.\**
.FS
In retrospect, this was a bad idea because we calculate the clock
rate to get the instruction execution time.  If the clock rate is off,
so is the load time.
.FE
Processors that can manage to get the load address out to the 
E 44
address pins before the end of the load cycle get some free time in this
D 18
benchmark (I don't think any processors can do that).
E 18
I 18
D 24
benchmark (We don't think any processors do that).
E 24
I 24
D 44
benchmark (we don't think any processors do that).
E 44
I 44
D 49
benchmark (we don't know of any processors do that).
E 49
I 49
benchmark (we don't know of any processors that do that).
E 49
E 44
E 24
E 18
.PP
D 18
Note that this benchmark has been validated by logic analyzer measurements
on an SGI indy. The
clever reader might realize that last few nanoseconds of inaccuracy could be
rounded off by realizing that the latency is always going to be a multiple
of the processor clock rate.
E 18
I 18
This benchmark has been validated by logic analyzer measurements
D 24
on an SGI indy. 
E 24
I 24
D 28
on an SGI Indy. 
E 24
The clever reader might realize that last few nanoseconds of
D 23
inaccuracy could be rounded off by realizing that the latency is
E 23
I 23
inaccuracy could be rounded off because the latency is
E 23
always going to be a multiple of the processor clock rate.
E 28
I 28
on an \s-1SGI\s0 Indy by Ron Minnich while he was at the Maryland Supercomputer
Research Center.
E 28
E 18
.TSTART 1
.so mem.pic
D 44
.TEND "Memory latency" 1
E 44
I 44
.FEND "Memory latency" 1
E 44
.PP
D 18
The raw data is a series of data sets.  Each data set is a stride size,
with array size varied from about one kilobyte up to eight megabytes.
When these data sets are all plotted together (using a log base 2 scale
for the size variable), the data will be seen to contain a series of 
horizontal plateaus.  The first is the on board data cache latency (if there
is an on board cache).  The point where the lines start to go up marks the
size of the cache.  The second is the external cache, the third is the
main memory, and the last is main memory plus TLB miss cost.  In addition
to this information, the cache line size can be derived by noticing which
strides are faster than main memory times.  The first stride that is
main memory speed is likely to be the cache line size.  The reason is
that the strides that are faster than memory indicate that the benchmark is
E 18
I 18
D 50
Each curve represents a stride size,
E 50
I 50
Results from the memory latency benchmark are plotted as a series of data sets
as shown in Figure \n[FIGURE].
Each data set represents a stride size,
E 50
D 24
with the array size varied from about one kilobyte up to eight megabytes.
E 24
I 24
D 44
with the array size varied from about 1 kilobyte up to 8 megabytes.
E 44
I 44
D 49
with the array size varying from about 1K up to 8M.
E 49
I 49
with the array size varying from 512 bytes up to 8M or more.
E 49
E 44
E 24
The curves contain a series of 
horizontal plateaus, where each plateau represents a level in the
memory hierarchy.  
D 44
The point where each plateau ends and the line starts to go up indicates the
size of that bit of the memory hierarchy (e.g. external cache).  
E 44
I 44
D 49
The point where each plateau ends and the line starts up indicates the
size of that bit of the memory hierarchy (e.g., external cache).  
E 49
I 49
The point where each plateau ends and the line rises marks the
end of that portion of the memory hierarchy (e.g., external cache).  
E 49
E 44
D 23
While machines have different memory hierarchies,
most machines have similar hierarchies: on-board cache, external
D 20
cache, external level-2 cache, main memory, and main memory plus TLB
miss costs.  Some processors don't have on-board cache, while many
machines don't have external level-2 cache.
E 20
I 20
cache, main memory, and main memory plus TLB
miss costs.  Some processors don't have on-board cache, while others
don't have external cache.
E 23
I 23
Most machines have similar memory hierarchies:
on-board cache, external cache, main memory, and main memory plus TLB
miss costs.  
D 44
There are variations, some processors are missing a cache, while at
E 44
I 44
D 49
There are variations: some processors are missing a cache, while at
E 44
least one processor adds another cache in the hierarchy.
E 49
I 49
There are variations: some processors are missing a cache, while 
others add another cache to the hierarchy.
E 49
.\" XXX Larry please double-check this; I am going on dim memory...
D 44
For example, the Alpha 8400 has two on-board caches, one 8KB
and the other 90KB.
E 44
I 44
For example, the Alpha 8400 has two on-board caches, one 8K
D 50
and the other 90K.
E 50
I 50
and the other 96K.
E 50
E 44
.PP
E 23
E 20
The cache line size can be derived by comparing curves and noticing which
strides are faster than main memory times.  The smallest stride that is
the same as main memory speed is likely to be the cache line size because
the strides that are faster than memory are
E 18
D 23
getting more than one hit per cache line.  Note that prefetching may confuse
D 20
the issue because a demand read may stall behind a prefetch load.
E 20
I 20
the issue because a demand read may stall behind a prefetch load.  This
may cause cache lines to appear twice as large as they are.
E 23
I 23
D 24
getting more than one hit per cache line.  Prefetching may confuse
the issue because a demand read may stall behind a prefetch load,
causing cache lines to appear twice as large as they are.
E 24
I 24
getting more than one hit per cache line.  
.\" Prefetching may confuse
.\" the issue because a demand read may stall behind a prefetch load,
.\" causing cache lines to appear twice as large as they are.
E 24
E 23
E 20
I 18
.\" XXX
I 23
.\" Larry --- can we use prime modulus arithmetic to set up pointer 
.\" loops which might appear random but which really aren't and which
.\" hit every stride once before looping?
.\"
.\" XXX
.\" Larry --- is there any way we can defeat/disable prefetching
.\" so the cache line size can be more accurately determined?
.\"
.\" XXX
E 23
.\" Larry --- can we create a benchmark for TLB misses?
.\" I think it was Tom Rokicki who suggested that we create a
.\" benchmark where the data fits in the cache, but the pages don't
.\" fit in the TLB.  
.\"
.\" XXX
.\" Larry --- is the description of the memory hierarchy correct?
.\" I am not sure I haven't added an extra level of external cache...
I 28
.EQ
delim $$
.EN
E 28
E 18
.PP
D 18
The graph XXX shows a particularly nicely made machine, a DEC alpha.
E 18
I 18
D 24
Figure XXX shows a particularly nicely made machine, a DEC alpha.
E 24
I 24
D 28
Figure \n[FIGURE] shows a nicely made machine, a DEC alpha.
E 28
I 28
D 44
Figure \n[FIGURE] shows a nicely made machine, a \s-1DEC\s0 alpha.
E 44
I 44
Figure \n[FIGURE] shows memory latencies on a nicely made machine,
a \s-1DEC\s0 Alpha.
E 44
E 28
E 24
E 18
We use this machine as the example 
D 44
because (a) it shows the latencies and sizes of
the on chip level 1 and motherboard level 2 caches, and (b) because it
D 24
has the best all around numbers, especially considering it can support a
E 24
I 24
has good all around numbers, especially considering it can support a
E 24
4 megabyte level 2 cache.
I 28
You can see that the on-board cache is $2 sup 13$ bytes or 8 kilobytes, while the
external cache is $2 sup 19$ bytes or 512 kilobytes.
E 44
I 44
because it shows the latencies and sizes of
D 47
the on\-chip level 1 and motherboard level 2 caches, and because it
has good all\-around numbers, especially considering it can support a
E 47
I 47
the on-chip level 1 and motherboard level 2 caches, and because it
has good all-around numbers, especially considering it can support a
E 47
4M level 2 cache.
The on-board cache is $2 sup 13$ bytes or 8K, while the
external cache is $2 sup 19$ bytes or 512K.
E 44
E 28
I 26
.EQ
D 28
delim $$
.EN
E 26
I 18
D 24
You can see that the on-board cache is $2^13$ or 8KB, while the
D 20
external cache is $2^18$ or 256KB.
E 20
I 20
external cache is $2^19$ or 512KB.
E 24
I 24
You can see that the on-board cache is $2 sup 13$ or 8KB, while the
external cache is $2 sup 19$ or 512KB.
I 26
.EQ
E 28
delim off
.EN
I 36
.TSTART
.so lat_allmem.tbl
D 47
.TEND "Cache and memory latency"
E 47
I 47
.TEND "Cache and memory latency (ns)"
I 48
.nr MEMTABLE \n[TABLE]
E 48
E 47
.PP
D 44
In  Figure \n[FIGURE], we have gone through all of the memory latency
E 44
I 44
D 49
In  Table \n[TABLE], we have gone through all of the memory latency
E 44
graphs and extracted the cache sizes and latency as well as main memory
latency.  If readers wish to see the actual graphs, the data as well as
tools for extracting the data are included with \*[lmbench].  It is
worthwhile to plot all of the graphs and examine them since we can not
show as much detail as we would like here.  We do not show the
\s-1DEC\s0 Alpha 8400 processor's second on chip cache of about 96
D 44
kilobytes, nor do we show the typical differences between workstation
E 44
I 44
KB, nor do we show the typical differences between workstation
E 44
and personal computer second level caches (the workstations typically
do better).
E 49
I 49
Table \n[TABLE] shows the cache size, cache latency, and main memory
latency as extracted from the memory latency graphs.  
The graphs and the tools for extracting the data are 
included with \*[lmbench].  
It is worthwhile to plot all of the graphs and examine them since the
table is missing some details, such as the
D 51
\s-1DEC\s0 Alpha 8400 processor's second 96K on-chip cache
E 51
I 51
\s-1DEC\s0 Alpha 8400 processor's second 96K on-chip cache.
E 51
E 49
I 48
.PP
We sorted Table \n[TABLE] on level 2 cache latency because we think
that many applications will fit in the level 2 cache.  The HP and IBM
systems have only one level of cache so we count that as both level 1
and level 2.  Those two systems have remarkable cache performance for
caches of that size.  In both cases, the cache delivers data in one
D 50
clock cycle after the load instruction.  HP designs usually focus on
large caches as close as possible to the processor.  Another HP
multiprocessor system, the 9000/890 had a 4M, split I&D, 2 way set
D 49
associative cache, accessable in one clock (16 ns).  That system was
E 49
I 49
associative cache, accessable in one clock (16ns).  That system was
E 49
primarily a database server.  The IBM focus is on low latency, high
E 50
I 50
clock cycle after the load instruction.  
.PP
HP systems usually focus on
large caches as close as possible to the processor.  A older HP
D 54
multiprocessor system, the 9000/890, has a 4M, split I&D, 2 way set
associative cache, accessible in one clock (16ns).  That system is
E 54
I 54
multiprocessor system, the 9000/890, has a 4M, split I&D, direct mapped
cache with a 2K victim cache, accessible in one clock (16ns).\**  That system is
E 54
primarily a database server.  
I 54
.FS
The Usenix version of this paper had this as a set associate cache; that was
incorrect.
.FE
E 54
.PP
The IBM focus is on low latency, high
E 50
bandwidth memory.  The IBM memory subsystem is good because all of
memory is close to the processor, but has the weakness that it is
D 50
extremely difficult to evale that design to a multiprocessor.
E 50
I 50
extremely difficult to evolve the design to a multiprocessor system.
E 50
.PP 
The 586 and PowerPC motherboards have quite poor second level caches,  
the caches are not substantially better than main memory.
.PP
D 49
The PentiumPro and Sun Ultra second level caches are of medium speed 
E 49
I 49
The Pentium Pro and Sun Ultra second level caches are of medium speed 
E 49
at 5-6 clocks latency each.  5-6 clocks seems fast until it is compared
against the HP and IBM one cycle latency caches of similar size.  
D 49
Given the tight integration of the PentiumPro level 2 cache, it is 
E 49
I 49
Given the tight integration of the Pentium Pro level 2 cache, it is 
E 49
surprising that it has such high latencies.
.PP
The 300Mhz DEC Alpha has a rather high 22 clock latency to the second
level cache which is probably one of the reasons that they needed a 96K
level 1.5 cache.  SGI and DEC have used large second level caches
to hide their long latency from main memory.
.PP
E 48
E 36
E 26
E 24
E 20
D 28
.\" XXX
.\" Larry --- these numbers sound funny to me, but that's what they
.\" look like reading the graph
E 28
E 18
E 17
E 3
.NH 2
D 3
Pipe bandwidth
.LP
Bandwidth measurement between two local processes communicating through
a Unix pipe.  Results are in megabytes per second.
E 3
I 3
Operating system entry
.PP
Entry into the operating system is required for many system facilities.  
D 44
When calculating the cost of some facility, it is useful to know how
expensive it is to perform a non-trivial entry into the operating system.
E 44
I 44
When calculating the cost of a facility, it is useful to know how
expensive it is to perform a nontrivial entry into the operating system.
E 44
.PP
D 44
We measure non-trivial entry into the system by repeatedly writing one
E 44
I 44
We measure nontrivial entry into the system by repeatedly writing one
E 44
D 15
word to \f(CB/dev/null\fP, a pseudo device driver that does nothing but
E 15
I 15
word to \f(CW/dev/null\fP, a pseudo device driver that does nothing but
E 15
discard the data.   This particular entry point was chosen because it has
never been optimized in any system that we have measured.  Other entry
D 16
points, typically getpid() and gettimeofday(), are heavily used and also
E 16
I 16
D 18
points, typically \*[getpid] and \*[gettimeofday], are heavily used and also
E 16
D 15
heavily optimized.  A write to the \f(CB/dev/null\fP driver will go
E 15
I 15
heavily optimized.  A write to the \f(CW/dev/null\fP driver will go
E 18
I 18
points, typically \*[getpid] and \*[gettimeofday], are heavily used,
D 44
heavily optimized, and sometimes are implemented as user-level library
E 44
I 44
heavily optimized, and sometimes implemented as user-level library
E 44
routines rather than system calls.  
A write to the \f(CW/dev/null\fP driver will go
E 18
E 15
D 4
through the system call table, to write(), verify the user area as
E 4
I 4
D 44
through the system call table, to \*[write], verify the user area as
E 44
I 44
through the system call table to \*[write], verify the user area as
E 44
E 4
readable, look up the file descriptor to get the vnode, call the vnode's
D 44
write function, and then return.  As noted below, this involves more work 
D 18
that is obvious at first.
E 18
I 18
than is obvious at first.
E 44
I 44
write function, and then return.  
E 44
E 18
D 10
.PP
I 4
Contrast with ousterhout?
.PP
E 10
I 10
.TSTART
E 10
E 4
D 5
XXX - results go here.
E 5
I 5
.so ../Results/tmp/lat_nullsys.tbl
E 5
D 10
.PP
E 10
I 10
D 15
.TEND "Simple system call time in \(*mseconds"
E 15
I 15
D 47
.TEND "Simple system call time in microseconds"
E 47
I 47
.TEND "Simple system call time (microseconds)"
I 48
.PP
Linux is the clear winner in the system call time.  The reasons are
twofold: Linux is a uniprocessor operating system, without any
MP overhead, and Linux is a small operating system, without all
of the ``features'' accumulated by the commercial offers.
.PP
Unixware and Solaris are doing quite well, given that they are both fairly
large, commercially oriented operating systems with a large accumulation
of ``features.''
E 48
E 47
E 15
E 10
D 20
It is interesting to note that Solaris, normally slower than most
D 18
systems, is faster than most here.  System calls have to save a fair
amount of state, and on some processors, that state can be expensive to
save.  The state only needs to be saved in the case that the process
will either go to sleep in the kernel waiting on some event, or if the
process is preempted.  The Solaris kernel team noticed that the default
entry to the kernel did the state saving work on every entry to the
system but most of the time, the process did not stay in the kernel.  So
they turned the entry ``upside down,'' and did more work in the uncommon
case but less in the common case.  This helped their system call cost
quite a bit and it shows.
E 18
I 18
systems, is faster than most here.  
During system calls most UNIXes save a fair
amount of process state, which can be expensive on some processors.
However, the state only needs to be saved when the process
goes to sleep waiting on some event, or when the
process is preempted.  The Solaris kernel team noticed that during most
kernel entries the processes were not preempted or blocked, and
it was not necessary to save the process state.
So, the Solaris kernel only saves the process state when the process
is preempted or blocks which speeds up the average system call 
quite a bit.
E 20
E 18
E 3
.NH 2
D 3
TCP/IP socket bandwidth
.LP
Bandwidth measurement using TCP/IP sockets.  Results are reported in megabytes
per second.  
Results are reported for local, ethernet, FDDI, and ATM, where possible.  
Results range from 1-10+ megabytes per second.  Any system delivering 
more than 10 MB/second over TCP is doing very well by 1994 standards.
E 3
I 3
Signal handling cost
E 3
.PP
D 3
Note that for local measurements, the system is actually moving 
twice as much data, since the data is being moved to/from the same host.
E 3
I 3
Signals in Unix are a way to tell another process to handle an event.  They
are to processes as interrupts are to the CPU.
E 3
.PP
D 3
Local bandwidths are (sometimes) useful for determining the overhead of the
protocol stack (as well as other OS tasks, such as context switching).  
Note, however, that some implementations (such as Solaris 2.x) have 
``fast pathed'' loopback IP which skews the results.  The fast path
uses a larger MTU and does not do checksums.
E 3
I 3
D 32
Signal handling becomes critical to layered systems.  Some ``applications''
E 32
I 32
D 49
Signal handling is often critical to layered systems.  Some ``applications''
E 49
I 49
Signal handling is often critical to layered systems.  Some applications,
E 49
E 32
D 18
such as databases, software development environments, threading libraries,
etc., essentially provide an operating system like layer on top of the
operating system.  Whether this is a good idea or not is an exercise left to
the reader.  Good idea or bad, signal handling becomes one of the critical
paths in this many of these applications.
E 18
I 18
D 43
such as databases, software development environments, and threading libraries,
provide an operating system like layer on top of the operating system,
making signal handling a critical path in this many of these applications.
E 43
I 43
such as databases, software development environments, and threading libraries
D 44
provide an operating system-like layer on top of the operating system,
E 44
I 44
D 47
provide an operating system\-like layer on top of the operating system,
E 47
I 47
provide an operating system-like layer on top of the operating system,
E 47
E 44
making signal handling a critical path in many of these applications.
E 43
E 18
E 3
.PP
I 15
D 49
We measure both signal installation and signal dispatching in two separate
E 49
I 49
\*[lmbench] measure both signal installation and signal dispatching in two separate
E 49
D 32
loops, both within the context of one process.
E 32
I 32
loops, within the context of one process.
E 32
E 15
D 3
The sockets are configured to use the largest receive/send buffers that the OS
will allow.  This is done to allow maximum bandwidth.  Sun's 4.x TCP/IP
subsystem (and probably BSD's as well) default to 4KB send/receive buffers,
which is too small.  (It would be better if the OS noted that this was a
high volume / high bandwidth connection and automatically grew the buffers.
Hint, hint.)
E 3
I 3
D 49
We measure signal handling by installing a signal handler and then repeatedly
D 4
sending ourselves the signal.   As the kill(2) system call is returning, the
kernel will notice that we have a pending system call and call our signal
E 4
I 4
D 15
sending ourselves the signal.   As the \*[kill] system call is returning, the
kernel will notice that we have a pending signal and call our signal
E 4
handler instead of completing the return from system call.  After the handler
D 4
is done, the kill(2) system call will return.  We define the signal handler
cost as the total time less the time it takes to do the kill(2) call.
E 4
I 4
is done, the \*[kill] system call will return.  We define the signal handler
cost as the total time less the time it takes to do the \*[kill] call.
E 15
I 15
sending ourselves the signal.   
E 49
I 49
It measures signal handling by installing a signal handler and then repeatedly
sending itself the signal.   
E 49
.TSTART
.so ../Results/tmp/lat_signal.tbl
D 43
.TEND "Signal times in microseconds"
E 43
I 43
D 44
.TEND "Signal times (microseconds)"
E 44
I 44
D 47
.TEND "Signal times in microseconds"
E 47
I 47
.TEND "Signal times (microseconds)"
E 47
E 44
E 43
E 15
E 4
.PP
I 44
Table \n[TABLE] shows the signal handling costs.  
E 44
D 18
Note that we are not context switching at all in our test, the signal is
going to the same process that generated the signal.  This is not normal,
most of the time the signal will be going to another process, which implies
E 18
I 18
D 49
Note there are no context switches in this benchmark; the signal is
D 32
going to the same process that generated the signal.  In applications
E 32
I 32
going to the same process that generated the signal.  In real applications
E 32
the signals will usually be going to another process, which implies
E 49
I 49
Note that there are no context switches in this benchmark; the signal goes
to the same process that generated the signal.  In real applications,
the signals usually go to another process, which implies
E 49
E 18
that the true cost of sending that signal is the signal overhead plus the
D 18
context switch overhead.  We wanted to separate the two since context
switch times vary widely between different operating systems.
D 10
.PP
E 10
I 10
D 15
.TSTART
E 10
D 5
XXX - results go here
E 5
I 5
.so ../Results/tmp/lat_signal.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "Signal handler time in \(*mseconds"
E 15
E 10
XXX  - any results I had were wrong, need to redo.
E 18
I 18
context switch overhead.  We wanted to measure signal and context
switch overheads separately since context
D 43
switch times vary widely between operating systems.
E 43
I 43
switch times vary widely among operating systems.
I 48
.PP
D 51
SGI does very well on signal processing in Table \n[TABLE],
E 51
I 51
SGI does very well on signal processing,
E 51
especially since their hardware is of an older generation than
many of the others.  
.PP
D 51
The Linux/Alpha signal handling numbers in Table \n[TABLE] are so poor
that we suspect that this is a bug, especially given that the Linux/ix86
E 51
I 51
The Linux/Alpha signal handling numbers are so poor
that we suspect that this is a bug, especially given that the Linux/x86
E 51
numbers are quite reasonable.
E 48
D 44
.PP
Table \n[TABLE] shows the signal handling costs.  For most machines
the signal handling costs are smaller than the context switch
overhead, but they are not negligible.
E 43
.\" XXX  - any results I had were wrong, need to redo.
E 18
I 4
.PP
D 18
XXX - it sure would be nice to compare this to interrupt dispatch cost on
some machines.
E 18
I 18
.\" XXX - it sure would be nice to compare this to interrupt dispatch 
.\" cost on some machines.
E 44
E 18
E 4
E 3
.NH 2
D 3
bcopy bandwidths
.LP
A simple benchmark that measures how fast data can be copied.  A hand
unrolled version and the C library version are tested.  Results are
reported in megabytes per second.  Note that a typical system is actually
moving about three times as much memory as the reported result.  A copy
is actually a read, a write which causes a cache line read, and a write
back.
.NH 2
Read bandwidth
.LP
Most VM system cache file pages for reuse.  This benchmark measures the
speed at which those pages can be reused.  It is important to notice
that this is not a disk read measurement, it is a memory read measurement.
Results are reported in megabytes per second.
.NH 2
Mmap read bandwidth
.LP
The same measurement as the previous benchmark except that it maps the
file, avoiding the copy from kernel to user buffer.  
Results are reported in megabytes per second.
.NH 2
Memory read bandwidth
.LP
A large array is repeatedly read sequentially.
Results reported in megabytes per second.
.NH 2
Memory write bandwidth
.LP
A large array is repeatedly written sequentially.
Results reported in megabytes per second.
.NH 1
Other measurements
.LP
.NH 2
Processor cycle time
mhz
.LP
Calculates the megahertz and clock speed of the processor.  This is the
standard loop in which a series of interlocked operations are timed,
and then the megahertz is derived from the timing.  The operations 
are purposefully interlocked to overcome any super scalerness of the
system under test.
E 3
I 3
Process creation costs
E 3
.PP
D 3
There are actually three versions of mhz, a generic one that works on
most systems, and two specific versions for SuperSPARC and rs6000
systems.
E 3
I 3
Process benchmarks are used to measure the basic process primitives,
such as creating a new process, running a different program, and context
switching.  Process creation benchmarks are of particular interest
D 50
to distributed systems since many remote operations include the creation
E 50
I 50
in distributed systems since many remote operations include the creation
E 50
of a remote process to shepherd the remote operation to completion.
Context switching is important for the same reasons.
D 24
.NH 3
Simple process creation
E 3
.PP
E 24
I 24
.BU "Simple process creation" .
E 24
D 3
It turns out that the
SuperSPARC processor has two ALU's that are run at twice the clock rate,
allowing two interlocked operations to complete in one processor clock.\**
.FS
Credit and thanks to John Mashey of SGI/MIPS fame, who kindly took the
time to out why the benchmark wasn't working on SuperSPARC
systems.  He explained the SuperSPARC pipeline and the solution to the
E 3
I 3
D 4
The Unix process creation primitive is fork(2).  This primitive takes what
E 4
I 4
D 18
The Unix process creation primitive is \*[fork].  This primitive takes what
E 4
was one process and creates a (virtually) exact copy of that process.
E 18
I 18
The Unix process creation primitive is \*[fork], which
creates a (virtually) exact copy of the calling process.
I 19
D 28
Unlike VMS and some other operating systems, Unix starts any new task
E 28
I 28
Unlike VMS and some other operating systems, Unix starts any new process
E 28
with a \*[fork].  
D 28
Consequently, \*[fork] should be fast and ``light,'' facts which
many have been ignoring for some time.
E 28
I 28
Consequently, \*[fork] and/or \f(CWexecve\fP should be fast and
D 43
``light,'' facts which many have been ignoring for some time.
E 43
I 43
``light,'' facts that many have been ignoring for some time.
E 43
E 28
E 19
E 18
.PP
D 19
Creating processes is how Unix starts any new task.  This is different than
some other operating systems, such as VMS.  It imposes a certain ``lightness''
constraint on the process creation primitives, a constraint that many have
been ignoring for some time.
.PP
We measure simple process creation by creating a process, having that 
D 4
process immediately exit(2).  The parent process waits for the child
E 4
I 4
process immediately \*[exit].  The parent process waits for the child
E 4
process to exit.   The benchmark actually includes both the fork and
the exit time, but that is within the intent of the benchmark, which is 
E 19
I 19
D 49
We measure simple process creation by creating a process and immediately
E 49
I 49
\*[lmbench] measures simple process creation by creating a process 
and immediately
E 49
exiting the child process.  The parent process waits for the child
D 28
process to exit.   The benchmark actually includes both the \*[fork] and
E 28
I 28
D 49
process to exit.   The benchmark includes the \*[fork] and
E 28
the \*[exit] time, but that is within the intent of the benchmark, which is 
E 19
to measure the overhead for creating a new thread of control.
E 49
I 49
process to exit.   
The benchmark is intended to measure the overhead for creating a 
new thread of control, so it includes the \*[fork] and
the \*[exit] time.
E 49
I 28
.PP
The benchmark also includes a \f(CWwait\fP system call in the parent and
context switches from the parent to the child and back again.   Given that
context switches of this sort are on the order of 20 microseconds and a 
system call is on the order of 5 microseconds, and that the entire benchmark
D 49
time is on the order of a millisecond or more, we just ignore the extra 
overhead as too little to matter.
E 49
I 49
time is on the order of a millisecond or more, the extra overhead
is insignificant.
E 49
E 28
D 10
.PP
E 10
I 10
D 11
.TSTART
E 10
D 5
XXX - results go here.
E 5
I 5
.so ../Results/tmp/lat_nullproc.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "Process fork/exit time in milliseconds"
E 11
I 11
D 51
.\" .TSTART
.\" .so ../Results/tmp/lat_nullproc.tbl
.\" .TEND "Process fork/exit time in milliseconds"
E 51
E 11
E 10
D 19
Note that even this relatively simple task is very expensive, time is
measured in milliseconds most of the other operations we consider are
E 19
I 19
Note that even this relatively simple task is very expensive and is
measured in milliseconds while most of the other operations we consider are
E 19
D 43
measured in microseconds.  It just gets worse as we do more complicated
E 43
I 43
D 49
measured in microseconds.  And costs just get worse as we do more complicated
E 43
things.  
E 49
I 49
measured in microseconds.  
E 49
I 11
D 48
.TSTART
.so ../Results/tmp/lat_allproc.tbl
D 15
.TEND "Process creation"
E 15
I 15
D 43
.TEND "Process creation time in milliseconds"
E 43
I 43
D 44
.TEND "Process creation time (milliseconds)"
E 44
I 44
D 47
.TEND "Process creation time in milliseconds"
E 47
I 47
.TEND "Process creation time (milliseconds)"
E 48
E 47
E 44
E 43
E 15
E 11
D 24
.NH 3
New process creation
.PP
E 24
I 24
.BU "New process creation" .
E 24
D 43
The preceding benchmark did not create a new application, it created a
copy of the old application.   Here we are measuring the cost creating a
E 43
I 43
The preceding benchmark did not create a new application; it created a
D 49
copy of the old application.   Here we are measuring the cost of creating a
E 43
new process and changing that process into a new application.  
.PP
The process of creating a new thread of control and changing that thread
of control to a new application forms the basis of every Unix command
E 49
I 49
copy of the old application.   This benchmark measures the cost of creating a
new process and changing that process into a new application, which.  
forms the basis of every Unix command
E 49
D 24
line interface, or shell.  It is important that this function not grow
D 19
so heavy weight that it becomes unused, for it is this function that is
a fundamental difference between Unix and Windows or the Mac.  Unix is
D 16
inherently multi threaded, it always has been.  Windows is trying to
E 16
I 16
inherently multi-threaded, it always has been.  Windows is trying to
E 16
wedge threading on top of an operating system that fundamental doesn't
E 19
I 19
so heavy-weight that it becomes unused, for it is this function that is
a fundamental difference between Unix and Windows or the Mac.  Unix has
always been inherently multi-threaded, while Windows is trying to
wedge multi-threading on top of an operating system that fundamentally doesn't
E 19
support concurrency.
.PP
E 24
I 24
line interface, or shell.  
E 24
D 43
We measure this facility by forking a new child, and having that child
exec-ing a new program, in this case a tiny program that prints 
E 43
I 43
D 49
We measure this facility by forking a new child and having that child
E 49
I 49
\*[lmbench] measures this facility by forking a new child and having that child
E 49
D 50
execute a new program \- in this case, a tiny program that prints 
E 50
I 50
execute a new program \(em in this case, a tiny program that prints 
E 50
E 43
``hello world'' and exits.
D 10
.PP
E 10
I 10
D 11
.TSTART
E 10
D 5
XXX - results here
E 5
I 5
.so ../Results/tmp/lat_simpleproc.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "Process create time in milliseconds"
E 11
I 11
D 51
.\" .TSTART
.\" .so ../Results/tmp/lat_simpleproc.tbl
D 47
.\" .TEND "Process create time in milliseconds"
E 47
I 47
.\" .TEND "Process create time (milliseconds)"
E 51
E 47
.PP
E 11
E 10
D 28
That cost is especially noticeable
on (some) systems that have shared libraries.  Shared libraries can
D 24
introduce a substantial (10s of milliseconds) start up cost.  This 
benchmark is intended to quantify the time/space tradeoff of shared
libraries.
D 19
.PP
D 4
XXX - I should probably exec a 
statically linked one as well.
E 4
I 4
XXX - I should probably exec a statically linked one as well.
Need to repackage w/ GNU's autoconf.
E 19
I 19
.\" XXX - I should probably exec a statically linked one as well.
.\" XXX - Need to repackage w/ GNU's autoconf.
E 19
E 4
.NH 3
Complicated new process creation
.PP
E 24
I 24
introduce a substantial (10s of milliseconds) start up cost.  
E 28
I 28
The startup cost is especially noticeable
D 43
on (some) systems that {XXX -which?} have shared libraries.  Shared libraries can
introduce a substantial (tens of milliseconds) start up cost.  
E 43
I 43
on (some) systems that have shared libraries.  Shared libraries can
introduce a substantial (tens of milliseconds) startup cost.  
I 51
.\" XXX - statically linked example?
.TSTART
.so ../Results/tmp/lat_allproc.tbl
.TEND "Process creation time (milliseconds)"
E 51
E 43
E 28
.BU "Complicated new process creation" .
E 24
D 19
When programs start other programs, they frequently do so through one of
E 19
I 19
When programs start other programs, they frequently use one of
E 19
D 16
three standard interfaces: popen(), system(), and/or execlp().  All of 
E 16
I 16
D 28
three standard interfaces: \*[popen], \*[system], and/or \*[execlp].  All of 
E 16
these interfaces start a new process by invoking the standard command
E 28
I 28
three standard interfaces: \*[popen], \*[system], and/or \*[execlp].  The first
two interfaces start a new process by invoking the standard command
E 28
D 24
interpreter, /bin/sh, to start the process.  Starting programs this way
E 24
I 24
interpreter, \f(CW/bin/sh\fP, to start the process.  Starting programs this way
E 24
D 19
guarantees that the shell will look around for the requested application
E 19
I 19
guarantees that the shell will look for the requested application
E 19
D 43
in all of the places that the user would look, in other words, the shell
E 43
I 43
D 50
in all of the places that the user would look \- in other words, the shell
E 50
I 50
in all of the places that the user would look \(em in other words, the shell
E 50
E 43
D 19
uses the user's $PATH variable as a list of places to go find the new
E 19
I 19
uses the user's $PATH variable as a list of places to find the
E 19
D 28
application.
E 28
I 28
application.  \*[execlp] is a C library routine which also looks for the
D 50
program in the user's $PATH variable.
E 50
I 50
program using the user's $PATH variable.
E 50
E 28
.PP
D 19
Since this is a common way of starting up new applications, we felt it
E 19
I 19
Since this is a common way of starting applications, we felt it
E 19
D 43
was good to show the costs of the generality.
E 43
I 43
was useful to show the costs of the generality.
E 43
.PP
D 28
We measure this starting a shell to start the same tiny program we ran in
E 28
I 28
D 43
We measure this starting \f(CW/bin/sh\fP to start the same tiny program we ran in
E 28
the last case.
D 10
.PP
E 10
I 10
D 11
.TSTART
E 10
D 5
XXX - results here
E 5
I 5
.so ../Results/tmp/lat_shproc.tbl
E 5
D 10
.PP
E 10
I 10
.TEND "Process create time in milliseconds"
E 11
I 11
D 28
.\" .TSTART
.\" .so ../Results/tmp/lat_shproc.tbl
.\" .TEND "Process create time in milliseconds"
E 28
E 11
E 10
Notice that the cost of asking the shell to go look for the program is
quite excessive, frequently 15 times as expensive as just creating a 
new process, and 4 times as expensive as explicitly naming the location
E 43
I 43
We measure this by starting \f(CW/bin/sh\fP to start the same tiny 
program we ran in the last case.
D 44
.\" XXX Larry 
.\" these 15x and 4x numbers seem wrong given the table in the current printout
E 44
In Table \n[TABLE] the cost of asking the shell to go 
look for the program is
D 44
quite large, frequently 15 times as expensive as just creating a 
E 44
I 44
D 47
quite large, frequently 10 times as expensive as just creating a 
E 47
I 47
quite large, frequently ten times as expensive as just creating a 
E 47
E 44
new process, and four times as expensive as explicitly naming the location
E 43
of the new program.
D 24
.NH 3
E 24
I 24
D 51
.\" XXX - statically linked example?
I 48
.TSTART
.so ../Results/tmp/lat_allproc.tbl
.TEND "Process creation time (milliseconds)"
E 51
.PP
The results that stand out in Table \n[TABLE] are the poor Sun Ultra 1 results.
Given that the processor is one of the fastest, the problem is likely to be
software.  There is room for substantial improvement in the Solaris
process creation code.
E 48
.NH 2
E 24
Context switching
.PP
D 43
A context switch is is defined here as 
the time it takes to save the state of one process and restore the state
E 43
I 43
D 49
A context switch is defined here as 
E 49
I 49
Context switch time is defined here as 
E 49
the time needed to save the state of one process and restore the state
E 43
of another process.  
.PP
Context switches are frequently in the critical performance path of 
D 31
distributed applications.  For example, the IRIX operating system uses
E 31
I 31
D 32
distributed applications.  For example, the multi processor versions
of the IRIX operating system uses
E 32
I 32
D 33
distributed applications.  For example, the IRIX operating system uses
E 33
I 33
D 49
distributed applications.  For example, the multi processor versions
of the IRIX operating system uses
E 49
I 49
distributed applications.  For example, the multiprocessor versions
of the IRIX operating system use
E 49
E 33
E 32
E 31
processes to move data through the networking stack.  This means that the
processing time for each new packet arriving at an idle system includes
D 43
the time it takes to switch in the networking process.  
E 43
I 43
the time needed to switch in the networking process.  
E 43
.PP
I 4
Typical context switch benchmarks measure just the minimal context switch
D 43
time, i.e., the time to switch between two processes that are doing nothing
E 43
I 43
D 50
time \- the time to switch between two processes that are doing nothing
E 50
I 50
time \(em the time to switch between two processes that are doing nothing
E 50
E 43
D 12
but context switching.  That approach is misleading because systems may
E 12
I 12
D 47
but context switching.  Other benchmarks frequently include the cost of 
E 47
I 47
D 49
but context switching.  We feel that using only two small processes is 
E 49
I 49
but context switching.  We feel that this is
E 49
misleading because there are frequently more than two active processes,
D 49
and those processes usually have a larger working set (cache footprint)
E 49
I 49
and they usually have a larger working set (cache footprint)
E 49
than the benchmark processes.
.PP
Other benchmarks frequently include the cost of 
E 47
D 43
of the system calls needed to force the context switches.  
E 43
I 43
the system calls needed to force the context switches.  
E 43
For example, Ousterhout's context switch benchmark 
D 15
measures context switch time plus a \f(CWread()\fP and a \fCWwrite()\fP
E 15
I 15
measures context switch time plus a \*[read] and a \*[write]
E 15
D 32
on a pipe.  In many of the systems measured by \*[lmbench], the pipe overhead 
varies from 30% to 300% of the context switch time.  
E 32
I 32
on a pipe.  
D 47
.PP
E 47
In many of the systems measured by \*[lmbench], the pipe overhead 
D 47
varies between 30% and 300% of the context switch time.  
E 32
That approach is misleading because systems may
E 12
have multiple active processes and the processes typically have more state
(hot cache lines) than just the code required to force another context
switch.  This benchmark takes that into consideration and varies both
the number and the size of the processes.
.PP
E 47
I 47
varies between 30% and 300% of the context switch time, so we were 
careful to factor out the pipe overhead.
.BU "Number of processes."
E 47
E 4
The context switch benchmark is implemented as 
a ring of two to twenty processes that are connected with Unix pipes.  
A token is passed from process to process, forcing context switches.  
D 41
The benchmark measures the time it takes to pass
E 41
I 41
The benchmark measures the time needed to pass
E 41
the token two thousand times from process to process.  
D 41
Each hand off of the token has two costs: (a) the context switch, and
(b) the overhead cost of passing the token.
E 41
I 41
D 50
Each handoff of the token has two costs: the context switch, and
the overhead cost of passing the token.
E 41
In order to get just the context switching time, the benchmark first
E 50
I 50
Each transfer of the token has two costs: the context switch, and
the overhead of passing the token.
In order to calculate just the context switching time, the benchmark first
E 50
measures the cost of passing the token through a ring of pipes in a
single process.  This overhead time is defined as the cost of passing
the token and is not included in the reported context switch time.
D 47
.PP
E 47
I 47
.BU "Size of processes."
E 47
D 4
Typical context switch benchmarks measure just the minimal context switch
time, i.e., the time to switch between two processes that are doing nothing
but context switching.  That approach is misleading because systems may
have multiple active processes and the processes typically have more state
(hot cache lines) than just the code required to force another context
switch.  This benchmark takes that into consideration and varies both
the number and the size of the processes.
.PP
E 4
D 49
In order to measure context switch times that are more realistic, we add
an artificial ``cache footprint'' of varying size to the switching
E 49
I 49
In order to measure more realistic context switch times, we add
an artificial variable size ``cache footprint'' to the switching
E 49
processes.  The cost of the context switch then includes the cost
D 41
of restoring user level state (cache footprint).  The cache footprint
E 41
I 41
D 44
of restoring user-level state (cache footprint).  The cache footprint
E 44
I 44
D 47
of restoring user\-level state (cache footprint).  The cache footprint
E 47
I 47
of restoring user-level state (cache footprint).  The cache footprint
E 47
E 44
E 41
D 36
is implemented by having the process allocate an array of data and sum
the array as a series of integers after receiving the token but before
D 28
passing the token to the next process.  
E 28
I 28
passing the token to the next process.  All arrays are at the same virtual
E 36
I 36
is implemented by having the process allocate an array of data\**
.FS
All arrays are at the same virtual
E 36
address in all processes.
I 36
.FE
and sum
the array as a series of integers after receiving the token but before
passing the token to the next process.  Since most systems will cache data
across context switches, the working set for the benchmark is slightly 
D 47
larger than the number of processes times the array size.  The size
is noted in the graph below.
E 47
I 47
larger than the number of processes times the array size.  
E 47
E 36
E 28
.PP
D 41
It is worthwhile to point out that the overhead mentioned above above
also included the cost of accessing the data, just in the same way as
E 41
I 41
It is worthwhile to point out that the overhead mentioned above
also includes the cost of accessing the data, in the same way as
E 41
the actual benchmark.   However, because the overhead is measured
in a single process, the cost is typically the cost with ``hot''
D 47
caches.  So the context switch time does not include anything other than
E 47
I 47
D 49
caches.  In the graph below, each size is plotted as a line, with
E 49
I 49
D 51
caches.  In the following graph, each size is plotted as a line, with
E 51
I 51
caches.  In the Figure 2, each size is plotted as a line, with
E 51
E 49
context switch times on the Y axis, number of processes on the 
X axis, and the process size as the data set.
The process size and the hot cache overhead costs for
the pipe read/writes and any data access is what is labeled
D 49
as \f(CWsize=0KB overhead=10\fP.  The size is kilobytes and the overhead
E 49
I 49
as \f(CWsize=0KB overhead=10\fP.  The size is in kilobytes and the overhead
E 49
is in microseconds.
.PP
The context switch time does not include anything other than
E 47
D 41
the context switch provided that all the benchmark processes fit in the
cache.  If there the total size of all of the benchmark processes is
E 41
I 41
the context switch, provided that all the benchmark processes fit in the
D 47
cache.  If the total size of all of the benchmark processes in the cache is
E 41
big enough to cause cache misses (this is quite common), the cost of the
D 32
context switch includes the cost of those cache misses.  This is by
design, we are trying to show realistic context switch times as a
E 32
I 32
context switch includes the cost of those cache misses.  
E 47
I 47
cache.  If the total size of all of the benchmark processes is larger
than the cache size,  the cost of each context switch will include cache
misses.
E 47
We are trying to show realistic context switch times as a
E 32
function of both size and number of processes.
D 10
.PP
E 10
I 10
.TSTART 1
E 10
D 5
XXX - hp results here
E 5
I 5
.so ctx.pic
E 5
D 10
.PP
E 10
I 10
D 36
.TEND "Context switch time" 1
E 36
I 36
D 44
.TEND "Context switch times" 1
E 44
I 44
.FEND "Context switch times" 1
E 44
E 36
E 10
D 28
Results for an HP system running at 100 MHz are shown above, others
E 28
I 28
.PP
D 36
Results for an \s-1HP\s0 system running at 100 MHz are shown in Figure
\n[FIGURE], others
E 28
are in appendix XXXX.
D 28
The HP is a particularly nice system for this benchmark because the
E 28
I 28
The \s-1HP\s0 is a particularly nice system for this benchmark because the
E 28
results are quite close to what is expected from a machine with a
256 kilobyte cache.  As the size and number of processes are both increased,
processes start falling out of the cache, resulting in higher context
switch times.  Note that the overhead numbers go up as the size of
the processes go up.
E 36
I 36
Results for an Intel Pentium Pro system running Linux at 167 MHz are
shown in Figure \n[FIGURE].
The data points on the figure are labeled with the working set
due to the sum of data in all of the processes.  The actual working set is
D 47
large, as it includes the process and kernel overhead as well.  We would expect
E 47
I 47
D 49
larger, as it includes the process and kernel overhead as well.  We would expect
E 47
that context switch times stay quite constant until the working set is
E 49
I 49
larger, as it includes the process and kernel overhead as well.  
One would expect the context switch times to stay constant until 
the working set is
E 49
D 41
approximately the size of the second level cache.  The Intel system has a 256
kilobyte second lavel cache, and as expected, the context switc times are 
E 41
I 41
approximately the size of the second level cache.  The Intel system has a 
D 44
256kilobyte second lavel cache, and as expected, the context switch times are 
E 44
I 44
D 47
256K second level cache, and as expected, the context switch times are 
E 47
I 47
D 49
256K second level cache, and as expected, the context switch times 
E 47
E 44
E 41
start to increase just as we reach that size (marked as .25M in the graph).
E 49
I 49
256K second level cache, and the context switch times 
stay almost constant until about 256K (marked as .25M in the graph).
E 49
I 48
.BU "Cache issues"
The context switch benchmark is a deliberate measurement of the
effectiveness of the caches across process context switches.  If the
cache does not include the process identifier (PID, also sometimes
called an address space identifier) as part of the address, then the
cache must be flushed on every context switch.  If the cache does not map
the same virtual addresses from different processes to different cache
lines, then the cache will appear to be flushed on every context
switch.
E 48
E 36
.PP
I 4
D 41
The graph of the context switch benchmark can quickly show if the system
has made a design flaw in the cache design.  An early version of the
E 41
I 41
D 48
The graph of the context switch benchmark can quickly show if the 
cache design is flawed.  An early version of the
E 41
Ross HyperSPARC chip implemented the cache as a virtually indexed
and virtually tagged cache (most caches are virtually indexed and 
physically tagged since this allows the cache hit/miss logic to start
D 16
work in parallel with the TLB logic, see [XXX cache memories] for more
information).
E 16
I 16
D 41
work in parallel with the TLB logic, see 
E 41
I 41
work in parallel with the TLB logic; see 
E 41
D 28
.RN Smith82
E 28
I 28
D 34
.RN Patterson96
E 34
I 34
.RN Hennessy96
E 34
E 28
for more information).
I 47
.\" XXX - check this.
E 47
E 16
D 32
What that means is that the cache can not
cache the same virtual address from more than one process at a time,
E 32
I 32
Using a virtually indexed and virtually tagged cache prevents the
cache from
caching the same virtual address for more than one process at a time;
E 32
D 8
essentially turung the cache into a single user device.  The data present
E 8
I 8
D 41
essentially turning the cache into a single user device.  The data present
E 41
I 41
D 44
essentially turning the cache into a single-user device.  The data present
E 44
I 44
D 47
essentially turning the cache into a single\-user device.  The data present
E 47
I 47
essentially turning the cache into a single-user device.  The data present
E 47
E 44
E 41
E 8
in the context switch benchmark is allocated at exactly the same place in
D 8
all of the processes which guarentees that there will be zero reuse across
E 8
I 8
D 41
all of the processes which guarantees that there will be zero reuse across
E 41
I 41
D 47
all of the processes, which guarantees that there will be zero reuse across
E 41
E 8
context switches.  
E 47
I 47
all of the processes, which means that the data from each of the processes
will map to the same location in the second-level cache, causing many
cache conflicts.  The graph for a system with this sort of cache problem
would appear as a series of straight, horizontal, parallel lines.  The
number of processes will not matter, the two process case will be just as
bad as the twenty process case since the second level cache will not be 
E 48
I 48
If the caches do
not cache across context switches there would be no grouping at the
lower left corner of Figure \n[FIGURE], instead, the graph would
appear as a series of straight, horizontal, parallel lines.  The number
of processes will not matter, the two process case will be just as bad
as the twenty process case since the cache would not be
E 48
useful across context switches.
E 47
D 10
.PP
E 10
I 10
.TSTART
E 10
E 4
D 5
XXX - summary results go here
E 5
I 5
D 11
.so ../Results/tmp/lat_ctx.tbl
E 11
I 11
.so ../Results/tmp/ctx.tbl
E 11
E 5
D 10
.PP
E 10
I 10
D 15
.TEND "Context switch time in \(*mseconds"
E 15
I 15
D 41
.TEND "Context switch time in microseconds"
E 41
I 41
D 44
.TEND "Context switch time (microseconds)"
E 44
I 44
D 47
.TEND "Context switch time in microseconds"
E 47
I 47
.TEND "Context switch time (microseconds)"
E 47
E 44
E 41
I 36
.PP
D 44
We picked four points on the graph and extracted those values for Figure
\n[FIGURE].  The complete set of values, as well as tools to graph them,
E 44
I 44
We picked four points on the graph and extracted those values for Table
\n[TABLE].  The complete set of values, as well as tools to graph them,
E 44
are included with \*[lmbench].
.PP
E 36
E 15
E 10
D 16
Note that multi processor context switch times are frequently more expensive
than uniprocessor context switch times.  Multi processor operating systems
E 16
I 16
D 37
Note that multi-processor context switch times are frequently more expensive
D 31
than uniprocessor context switch times.  Multi-processor operating systems
E 16
tend to have very complicated scheduling code.  It is possible to come
D 16
within about ten percent of the uniprocessor time on multi processor, 
E 16
I 16
within about ten percent of the uniprocessor time on multi-processor, 
E 16
context switches that take longer than that are a warning sign.
E 31
I 31
D 32
than uniprocessor context switch times.  This is because Multi-processor
operating systems tend to have very complicated scheduling code. 
We believe that multi process context switch times can be, and should be,
within ten percent of the uniprocessor times.
E 32
I 32
D 33
than uniprocessor context switch times.  Multi-processor operating systems
tend to have very complicated scheduling code.  It is possible to come
within about ten percent of the uniprocessor time on multi-processor, 
context switches that take longer than that are a warning sign.
E 33
I 33
than uniprocessor context switch times.  This is because Multi-processor
E 37
I 37
Note that multiprocessor context switch times are frequently more expensive
than uniprocessor context switch times.  This is because multiprocessor
E 37
operating systems tend to have very complicated scheduling code. 
D 41
We believe that multi process context switch times can be, and should be,
within ten percent of the uniprocessor times.
E 41
I 41
We believe that multiprocessor context switch times can be, and should be,
within 10% of the uniprocessor times.
E 41
E 33
E 32
E 31
.PP
D 24
Also note that Solaris is a fully preemptable operating system so even the
D 16
uniprocessors have to run a multi threaded kernel.  Solaris uniprocessor
context switches should be quite similar in cost to Solaris multi processor
E 16
I 16
uniprocessors have to run a multi-threaded kernel.  Solaris uniprocessor
context switches should be quite similar in cost to Solaris multi-processor
E 16
context switches.
.PP
E 24
D 16
An interesting multi processor note: for this benchmark, the results are
significantly better on an multi processor if both processes execute on the
E 16
I 16
D 37
An interesting multi-processor note: for this benchmark, the results are
significantly better on an multi-processor if both processes execute on the
E 37
I 37
D 48
An interesting multiprocessor note: for this benchmark, the results are
D 41
significantly better on an multiprocessor if both processes execute on the
E 37
E 16
same CPU.  The reason is that the benchmark has very little parallelism,
E 41
I 41
significantly better on a multiprocessor if both processes execute on the
same CPU.  The reason is that the benchmark has very little parallelism;
E 41
each process runs and then sleeps.  Both processes are executing identical
code paths in the kernel, resulting in hot instruction caches.  Finally,
it is typically less expensive to do a kernel operation on the same CPU
than it is to ask another CPU to do the operation, since the other CPU 
essentially must be interrupted.
E 48
I 48
Linux does quite well on context switching, especially on the more
D 50
recent architectures.  By comparing the Linux 2 0KB processes to the
Linux 2 32KB processes, it is apparent that there is something wrong
E 50
I 50
recent architectures.  By comparing the Linux 2 0K processes to the
Linux 2 32K processes, it is apparent that there is something wrong
E 50
with the Linux/i586 case.  If we look back to Table \n[MEMTABLE], we can
find at least part of the cause.  The second level cache latency for the
i586 is substantially worse than either the i686 or the Alpha.  
.PP
Given the poor second level cache behavior of the PowerPC, it is surprising
that it does so well on context switches, especially the larger sized cases.
.PP
The Sun Ultra1 context switches quite well in part because of enhancements
to the register window handling in SPARC V9.
E 48
.NH 2
Interprocess communication latencies
.PP
D 32
Inter process communication latency is important because many operations
E 32
I 32
D 37
Inter-process communication latency is important because many operations
E 37
I 37
Interprocess communication latency is important because many operations
E 37
E 32
D 49
are control messages that tell another process (frequently on another
system) to do something.  The latency of telling the remote process to
E 49
I 49
are control messages to another process (frequently on another
D 50
system).  The latency of telling the remote process to
E 50
I 50
system).  The time to tell the remote process to
E 50
E 49
do something is pure overhead and is frequently in the critical path
D 41
of important functions, such as distributed applications, i.e.,
databases, network servers, etc.
E 41
I 41
of important functions such as distributed applications (e.g.,
databases, network servers).
E 41
.PP
D 41
The inter process communication latency benchmarks are mostly the same
E 41
I 41
D 50
The interprocess communication latency benchmarks are mostly the same
E 41
idea: pass a small message (a byte or so) back and forth between two
E 50
I 50
The interprocess communication latency benchmarks typically have the 
following form: pass a small message (a byte or so) back and forth between two
E 50
D 41
processes.  The reported results are always the microseconds it takes
to do one round trip.  If you are interested in a one way timing, then
E 41
I 41
processes.  The reported results are always the microseconds needed
to do one round trip.  For one way timing, 
E 41
D 50
about half the round trip is right (however, the CPU cycles tend to be
E 50
I 50
about half the round trip is right.  However, the CPU cycles tend to be
E 50
D 41
somewhat asymmetric for a one trip, receiving is typically more
E 41
I 41
somewhat asymmetric for one trip: receiving is typically more
E 41
D 50
expensive than sending).  
E 50
I 50
expensive than sending.  
E 50
D 24
.NH 3
Pipe latency
.PP
E 24
I 24
.BU "Pipe latency" .
E 24
Unix pipes are an interprocess communication mechanism implemented as 
D 41
a one way byte stream; each end of the stream has an associated file
descriptor, one is the write descriptor and the other is the read
E 41
I 41
D 44
a one-way byte stream.  Each end of the stream has an associated file
E 44
I 44
D 47
a one\-way byte stream.  Each end of the stream has an associated file
E 47
I 47
a one-way byte stream.  Each end of the stream has an associated file
E 47
E 44
descriptor; one is the write descriptor and the other the read
E 41
descriptor.
.PP
Pipes are frequently used as a local IPC mechanism.  Because of the 
simplicity of pipes, they are frequently the fastest portable 
communication mechanism.
.PP
Pipe latency is measured by creating a pair of pipes, forking a child process,
and passing a word back and forth.  This benchmark is identical to the 
D 41
two process, ``zero'' sized context switch benchmark, except that it includes
E 41
I 41
D 44
two-process, zero-sized context switch benchmark, except that it includes
E 44
I 44
D 47
two\-process, zero\-sized context switch benchmark, except that it includes
E 47
I 47
two-process, zero-sized context switch benchmark, except that it includes
E 47
E 44
E 41
both the context switching time and the pipe overhead in the results.
D 41
The latency reported is one round trip, i.e., from process A to process B
E 41
I 41
.nr NTABLE \n[TABLE]+1
I 48
.nr LTABLE \n[TABLE]
E 48
Table \n[NTABLE] shows the round trip latency from process A to process B
E 41
and back to process A.
D 10
.PP
E 10
I 10
.TSTART
E 10
D 5
XXX - results go here.
E 5
I 5
.so ../Results/tmp/lat_pipe.tbl
E 5
D 10
.PP
E 10
I 10
D 15
.TEND "Pipe latency in \(*mseconds"
E 15
I 15
D 41
.TEND "Pipe latency in microseconds"
E 41
I 41
D 45
.TEND "Pipe latency (microseconds)"
E 45
I 45
D 47
.TEND "Pipe latency in microseconds"
E 47
I 47
.TEND "Pipe latency (microseconds)"
I 48
.PP
E 48
E 47
E 45
E 41
E 15
E 10
D 50
The time can be broken down as two context switches plus four system calls
E 50
I 50
The time can be broken down to two context switches plus four system calls
E 50
D 12
plus the pipe overhead.
E 12
I 12
D 48
plus the pipe overhead.  Note that this benchmark is identical to what
Ousterhout called a context switch.
E 48
I 48
D 51
plus the pipe overhead.  The context switch component is two of the 0K
E 51
I 51
plus the pipe overhead.  The context switch component is two of the small
E 51
processes in Table \n[LTABLE].
D 50
Note that this benchmark is identical to what Ousterhout called a context switch.
E 50
I 50
This benchmark is identical to the context switch benchmark in
.RN Ousterhout90 .
E 50
E 48
E 12
D 24
.NH 3
TCP and RPC/TCP latency
.PP
E 24
I 24
.BU "TCP and RPC/TCP latency" .
E 24
TCP sockets may be viewed as an interprocess communication mechanism similar
D 41
to pipes with an added feature that TCP sockets work across machine 
E 41
I 41
to pipes with the added feature that TCP sockets work across machine 
E 41
boundaries.
.PP
D 41
TCP and RPC/TCP connections are frequently used in low bandwidth, latency
sensitive applications.  The default Oracle distributed lock manager uses
TCP sockets and the locks per second available from this service is
accurately modeled by the TCP latency test.
E 41
I 41
D 44
TCP and RPC/TCP connections are frequently used in low-bandwidth, 
latency-sensitive applications.  The default Oracle distributed 
E 44
I 44
D 47
TCP and RPC/TCP connections are frequently used in low\-bandwidth, 
latency\-sensitive applications.  The default Oracle distributed 
E 47
I 47
TCP and RPC/TCP connections are frequently used in low-bandwidth, 
latency-sensitive applications.  The default Oracle distributed 
E 47
E 44
lock manager uses TCP sockets, and the locks per second available 
from this service are accurately modeled by the TCP latency test.
I 51
.TSTART
.so ../Results/tmp/lat_tcp.tbl
.TEND "TCP latency (microseconds)"
E 51
E 41
.PP
D 31
Sun's RPC may be layered over TCP.  
E 31
I 31
D 32
Sun's RPC may be layered over TCP or UDP.
E 32
I 32
D 33
Sun's RPC may be layered over TCP.  
E 33
I 33
D 47
Sun's RPC may be layered over TCP or UDP.
E 47
I 47
D 48
Sun's RPC is be layered either over TCP or over UDP.
E 48
I 48
Sun's RPC is layered either over TCP or over UDP.
E 48
E 47
E 33
E 32
E 31
D 41
The RPC layer is responsible for connection management (the port mapper), 
E 41
I 41
The RPC layer is responsible for managing connections (the port mapper), 
E 41
managing different byte orders and word sizes (XDR), and implementing a 
remote procedure call abstraction.
D 41
We show the same benchmark with and
E 41
I 41
D 51
.nr NTABLE \n[TABLE]+1
Table \n[NTABLE] shows the same benchmark with and
E 51
I 51
Table \n[TABLE] shows the same benchmark with and
E 51
E 41
without the RPC layer to show the cost of the RPC implementation.
.PP
D 41
TCP latency is measured by having a server process which waits for connections
E 41
I 41
TCP latency is measured by having a server process that waits for connections
E 41
and a client process that connects to the server.  The two processes then
D 41
exchange a word between them in a loop.  The latency reported is round trip
time.  The measurements reported here are local or loop back measurements,
E 41
I 41
D 50
exchange a word between them in a loop.  The latency reported is 
E 50
I 50
exchange a word between them in a loop.  The latency reported is one 
E 50
D 44
round-trip time.  The measurements in Figure \n[FIGURE] are local 
E 44
I 44
D 47
round\-trip time.  The measurements in Figure \n[FIGURE] are local 
E 47
I 47
D 48
round-trip time.  The measurements in Figure \n[FIGURE] are local 
E 48
I 48
D 51
round-trip time.  The measurements in Table \n[NTABLE] are local 
E 51
I 51
round-trip time.  The measurements in Table \n[TABLE] are local 
E 51
E 48
E 47
E 44
or loopback measurements, 
E 41
since our intent is to show the overhead of the software.  The same benchmark
D 41
may be, and frequently is, used to measure host to host latency.\**
E 41
I 41
D 44
may be, and frequently is, used to measure host-to-host latency.\**
E 44
I 44
D 47
may be, and frequently is, used to measure host\-to\-host latency.\**
E 47
I 47
D 48
may be, and frequently is, used to measure host-to-host latency.\**
E 47
E 44
E 41
.FS
D 14
For what it is worth, the best host to host round trip I have so far is
E 14
I 14
D 41
For what it is worth, the best host to host round trip we have so far is
E 41
I 41
D 44
The best host-to-host round trip we have so far is
E 44
I 44
D 47
The best host\-to\-host round trip we have so far is
E 47
I 47
The best host-to-host round trip we have so far is
E 47
E 44
E 41
E 14
D 20
between a pair of SGI 200MHz Indy workstations: 543 microseconds.
E 20
I 20
D 28
between a pair of Sun UltraSPARC workstations using 100Mbit ethernet: 
282 microseconds.
E 28
I 28
between a pair of SUN UltraSPARC workstations using 100Mbit ethernet: 
282 microseconds round trip time.
E 28
E 20
.FE
E 48
I 48
may be, and frequently is, used to measure host-to-host latency.
E 48
D 10
.PP
E 10
I 10
D 51
.TSTART
E 10
D 5
XXX - results go here, TCP & RPC/TCP  side by side.
E 5
I 5
.so ../Results/tmp/lat_tcp.tbl
E 5
D 10
.PP
E 10
I 10
D 15
.TEND "TCP latency in \(*mseconds"
E 15
I 15
D 41
.TEND "TCP latency in microseconds"
E 41
I 41
D 45
.TEND "TCP latency (microseconds)"
E 45
I 45
D 47
.TEND "TCP latency in microseconds"
E 47
I 47
.TEND "TCP latency (microseconds)"
E 51
I 48
.PP
E 48
E 47
E 45
E 41
E 15
E 10
D 24
Note that the RPC layer frequently adds as hundreds of microseconds of
E 24
I 24
Note that the RPC layer frequently adds hundreds of microseconds of
E 24
D 41
additional latency.  We know that the problem is not the XDR layer - the
data being passed back and forth is a byte, there is no XDR to be done.
There is no justification for the extra cost, it is simply
E 41
I 41
D 50
additional latency.  We know that the problem is not the external data
representation (XDR) layer \- the
E 50
I 50
additional latency.  The problem is not the external data
representation (XDR) layer \(em the
E 50
data being passed back and forth is a byte, so there is no XDR to be done.
There is no justification for the extra cost; it is simply
E 41
D 29
an expensive implementation.  And DCE RPC is worse.
E 29
I 29
D 32
an expensive implementation.  DCE RPC is worse.
E 32
I 32
D 33
an expensive implementation.  And DCE RPC is worse.
E 33
I 33
an expensive implementation.  DCE RPC is worse.
I 48
.TSTART
.so ../Results/tmp/lat_udp.tbl
.TEND "UDP latency (microseconds)"
E 48
E 33
E 32
E 29
D 24
.NH 3
UDP and RPC/UDP latency
.PP
E 24
I 24
.BU "UDP and RPC/UDP latency" .
E 24
UDP sockets are an alternative to TCP sockets.  They differ in that UDP
sockets are unreliable messages that leave the retransmission issues to
D 50
the application.  UDP sockets have a few advantages, however: they preserve
E 50
I 50
the application.  UDP sockets have a few advantages, however.  They preserve
E 50
D 41
message boundaries, whereas TCP does not; a single UDP socket may send messages
E 41
I 41
message boundaries, whereas TCP does not; and a single UDP socket may 
send messages
E 41
to any number of other sockets, whereas TCP sends data to only one place.
.PP
D 41
UDP and RPC/UDP messages are commonly used in many client server applications.
E 41
I 41
UDP and RPC/UDP messages are commonly used in many client/server applications.
E 41
NFS is probably the most widely used RPC/UDP application in the world.
.PP
D 41
UDP latency is measured by having a server process which waits for connections
E 41
I 41
Like TCP latency, UDP latency is measured by having a server process 
that waits for connections
E 41
and a client process that connects to the server.  The two processes then
D 41
exchange a word between them in a loop.  The latency reported is round trip
time.  The measurements reported here are local or loop back measurements,
E 41
I 41
D 44
exchange a word between them in a loop.  The latency reported is round-trip
E 44
I 44
D 47
exchange a word between them in a loop.  The latency reported is round\-trip
E 47
I 47
exchange a word between them in a loop.  The latency reported is round-trip
E 47
E 44
D 48
.nr NTABLE \n[TABLE]+1
time.  The measurements in Table \n[NTABLE] are local or loopback measurements,
E 48
I 48
time.  The measurements in Table \n[TABLE] are local or loopback measurements,
E 48
E 41
since our intent is to show the overhead of the software.
D 10
.PP
E 10
I 10
D 48
.TSTART
E 10
D 5
XXX - results go here, UDP & RPC/UDP side by side.
E 5
I 5
.so ../Results/tmp/lat_udp.tbl
E 5
D 10
.PP
E 10
I 10
D 15
.TEND "UDP latency in \(*mseconds"
E 15
I 15
D 41
.TEND "UDP latency in microseconds"
E 41
I 41
D 45
.TEND "UDP latency (microseconds)"
E 45
I 45
D 47
.TEND "UDP latency in microseconds"
E 47
I 47
.TEND "UDP latency (microseconds)"
E 48
E 47
E 45
E 41
E 15
E 10
Again, note that the RPC library can add hundreds of microseconds of extra
latency.  
D 47
.PP
D 41
It is interesting to compare UDP latency to TCP latency.  In many cases the
E 41
I 41
It is interesting to compare UDP latency with TCP latency.  In many cases the
E 41
TCP latency is \fBless\fP than the UDP latency.  This flies in the face
D 41
of conventional wisdom which says that TCP is an inherently more expensive
protocol than UDP.  The reasons that TCP may appear faster are:  (a) in this
E 41
I 41
of conventional wisdom, which says that TCP is an inherently more expensive
protocol than UDP.  The reasons that TCP may appear faster are: in this
E 41
benchmark, the protocol costs are dwarfed by the other costs (context
D 41
switching, system calls, and driver overhead), and (b) TCP is frequently
hand tuned for performance and UDP is rarely hand tuned.
E 41
I 41
switching, system calls, and driver overhead); and TCP is frequently
D 44
hand-tuned for performance, while UDP is rarely hand-tuned.
E 44
I 44
hand\-tuned for performance, while UDP is rarely hand\-tuned.
E 47
I 47
.\" .PP
.\" It is interesting to compare UDP latency with TCP latency.  In many cases the
.\" TCP latency is \fBless\fP than the UDP latency.  This flies in the face
.\" of conventional wisdom, which says that TCP is an inherently more expensive
.\" protocol than UDP.  The reasons that TCP may appear faster are: in this
.\" benchmark, the protocol costs are dwarfed by the other costs (context
.\" switching, system calls, and driver overhead); and TCP is frequently
.\" hand-tuned for performance, while UDP is rarely hand-tuned.
I 48
.TSTART
.so ipc.tbl
.TEND "Remote latencies (microseconds)"
.BU "Network latency" .
We have a few results for over the wire latency included in Table \n[TABLE].
As might be expected, the most heavily used network interfaces (i.e., ethernet)
have the lowest latencies.  The times shown include the time on the wire,
which is about 130 microseconds for 10Mbit ethernet, 13 microseconds for 100Mbit
ethernet and FDDI, and less than 10 microseconds for Hippi.
E 48
E 47
E 44
E 41
D 24
.NH 3
TCP connection latency
.PP
E 24
I 24
.BU "TCP connection latency" .
E 24
D 41
TCP is a connection based, reliable, byte stream oriented protocol.  As
E 41
I 41
D 44
TCP is a connection-based, reliable, byte-stream-oriented protocol.  As
E 44
I 44
D 47
TCP is a connection\-based, reliable, byte\-stream\-oriented protocol.  As
E 47
I 47
TCP is a connection-based, reliable, byte-stream-oriented protocol.  As
E 47
E 44
E 41
part of this reliability, a connection must be established before any
D 8
data may transfered.  The connection is accomplished by a ``three way
E 8
I 8
D 41
data may transferred.  The connection is accomplished by a ``three way
E 41
I 41
D 44
data can be transferred.  The connection is accomplished by a ``three-way
E 44
I 44
D 47
data can be transferred.  The connection is accomplished by a ``three\-way
E 47
I 47
data can be transferred.  The connection is accomplished by a ``three-way
E 47
E 44
E 41
E 8
handshake,'' an exchange of packets when the client attempts to connect
to the server.
.PP
D 19
Unlike UDP, where there is no connection established, TCP sends packets
at startup time.  If an application is using TCP to send one message
and then destroys the connection, the startup time can be a substantial
portion of the cost of the connection.   As the benchmark will show,
the connection cost is approximately half of the cost.  Even that does
not tell the whole story.  To connect a TCP socket and then disconnect
costs a total of 7 packets exchanged between the client and server.  
XXX - go figure exactly what the 4 at the end are.
E 19
I 19
Unlike UDP, where no connection is established, TCP sends packets
at startup time.  If an application creates a TCP connection to send
one message, then the startup time can be a substantial
fraction of the total connection and transfer costs.   
D 41
The benchmark will show the connection cost is approximately half of
E 41
I 41
The benchmark shows that the connection cost is approximately half of
E 41
the cost.  
D 47
Connection requires three packets and disconnection uses four packets,
so creating and destroying a TCP connection requires a total of seven
packets of overhead.
.\" XXX - go figure exactly what the 4 at the end are.
E 47
E 19
.PP
D 24
Connection cost is measured by having a server, that is registered using
E 24
I 24
Connection cost is measured by having a server, registered using
E 24
D 41
the portmapper, waiting for connections.  The client figures out where the
E 41
I 41
the port mapper, waiting for connections.  The client figures out where the
E 41
D 4
server is register and then repeatedly times a connect(2) system call to
E 4
I 4
D 19
server is register and then repeatedly times a \*[connect] system call to
E 19
I 19
server is registered and then repeatedly times a \*[connect] system call to
E 19
E 4
the server.  The socket is closed after each connect.  Twenty connects
D 31
are completed and the fastest of them is used as the result.
E 31
I 31
D 32
are completed and the fastest of them is used as the result.  The time measured
will include two of the three packets that make up the three way TCP handshake,
so the cost is actually greater than the times listed.
E 32
I 32
D 33
are completed and the fastest of them is used as the result.
E 33
I 33
are completed and the fastest of them is used as the result.  The time measured
will include two of the three packets that make up the three way TCP handshake,
so the cost is actually greater than the times listed.
E 33
E 32
E 31
I 19
.\" XXX Larry --- if a machine's clock granularity is on the order of
.\" 10 milliseconds, won't this benchmark run into granularity problems?
E 19
D 10
.PP
E 10
I 10
.TSTART
E 10
D 5
XXX - results go here.
E 5
I 5
.so ../Results/tmp/lat_connect.tbl
E 5
D 10
.PP
E 10
I 10
D 15
.TEND "TCP connect latency in \(*mseconds"
E 15
I 15
D 41
.TEND "TCP connect latency in microseconds"
E 15
E 10
Compare these numbers against UDP costs.  If what you want to do is to send
E 41
I 41
D 45
.TEND "TCP connect latency (microseconds)"
E 45
I 45
D 47
.TEND "TCP connect latency in microseconds"
E 47
I 47
.TEND "TCP connect latency (microseconds)"
I 48
.PP
E 48
E 47
E 45
Table \n[TABLE] shows that if the need is to send
E 41
a quick message to another process, given that most packets get through,
D 41
a UDP message will cost you a send and a reply (if positive
acknowledgments are needed, and they are in order to have an apples to apples 
comparison with TCP).  If the transmission media is 10Mbit Ethernet, the
E 41
I 41
D 50
a UDP message will cost a send and a reply (if positive
E 50
I 50
a UDP message will cost a \f(CWsend\fP and a \f(CWreply\fP (if positive
E 50
D 44
acknowledgments are needed, which they are in order to have an apples-to-apples 
E 44
I 44
D 47
acknowledgments are needed, which they are in order to have an apples\-to\-apples 
E 47
I 47
acknowledgments are needed, which they are in order to have an apples-to-apples 
E 47
E 44
comparison with TCP).  If the transmission medium is 10Mbit Ethernet, the
E 41
time on the wire will be approximately 65 microseconds each way, or 130
D 41
microseconds total.  To do the same thing with a short lived TCP 
E 41
I 41
D 44
microseconds total.  To do the same thing with a short-lived TCP 
E 44
I 44
D 47
microseconds total.  To do the same thing with a short\-lived TCP 
E 47
I 47
microseconds total.  To do the same thing with a short-lived TCP 
E 47
E 44
E 41
connection would cost 896 microseconds of wire time alone.
.PP
D 19
The comparison is not meant to deride TCP, TCP is a useful protocol.  Nor
E 19
I 19
D 41
The comparison is not meant to deride TCP; TCP is a useful protocol.  Nor
E 41
I 41
The comparison is not meant to disparage TCP; TCP is a useful protocol.  Nor
E 41
E 19
is the point to suggest that all messages should be UDP.  In many cases,
D 19
the difference between 130 microseconds and 900 microseconds would be
lost in the noise.  However, if the application is very latency sensitive
E 19
I 19
the difference between 130 microseconds and 900 microseconds is
D 41
insignificant compared to other aspects of application performance.
E 41
I 41
insignificant compared with other aspects of application performance.
E 41
However, if the application is very latency sensitive
E 19
D 41
and the transmission media is slow (such as serial link or a message
through many routers) then a UDP message may prove cheaper.
E 41
I 41
and the transmission medium is slow (such as serial link or a message
through many routers), then a UDP message may prove cheaper.
E 41
D 8
.NH 2
Virtual memory costs
.PP
XXX - Carl, I want to delete this section.  The mapping costs are pretty
consistent across all the machines.  AIX was the one big loser and they 
fixed it so it is of questionable usefulness.  The pagefault stuff isn't
portable, not everyone implements msync(.., MS_INVALIDATE).
.NH 3
Memory mapping 
.PP
Memory mapping is the process of making a file part of a process' address
space, allowing direct access to the file's pages.  It is an alternative
to the traditional read and write interfaces, an alternative that 
D 4
does not use the bcopy() that read and write must use.
E 4
I 4
does not use the \*[bcopy] that \*[read] and \*[write] must use.
E 4
.NH 3
Pagefaults
.PP
A benchmark that measures how fast the file system can pagefault in a
page that is not in memory.
.PP
E 8
.NH 2 
File system latency
.PP
D 41
File system latency is defined as the time it takes to create or delete
E 41
I 41
File system latency is defined as the time required to create or delete
E 41
a zero length file.  
D 8
.PP
E 8
We define it this way because in many file systems,
such as the BSD fast file system, the directory operations are done
D 41
synchronously in order to maintain on disk integrity.  Since the 
E 41
I 41
D 44
synchronously in order to maintain on-disk integrity.  Since the 
E 44
I 44
D 47
synchronously in order to maintain on\-disk integrity.  Since the 
E 47
I 47
synchronously in order to maintain on-disk integrity.  Since the 
E 47
E 44
E 41
file data is typically cached and sent to disk at some later date,
D 41
it is the file creation and deletion that becomes the bottleneck
D 19
seen by an application.  This bottleneck is substantial, to do
E 19
I 19
seen by an application.  This bottleneck is substantial; to do
E 19
a synchronous update to a disk is a matter of 10s of milliseconds.
E 41
I 41
the file creation and deletion become the bottleneck
seen by an application.  This bottleneck is substantial: to do
a synchronous update to a disk is a matter of tens of milliseconds.
E 41
In many cases, this bottleneck is much more of a perceived performance
issue than processor speed.
.PP
D 41
The benchmark creates 1000 zero sized files, and then deletes them.
E 41
I 41
D 44
The benchmark creates 1,000 zero-sized files and then deletes them.
E 44
I 44
D 47
The benchmark creates 1,000 zero\-sized files and then deletes them.
E 47
I 47
The benchmark creates 1,000 zero-sized files and then deletes them.
E 47
E 44
E 41
All the files are created in one directory and their names are
D 41
short, i.e., "a", "b", "c", ... "aa", "ab", ....
E 41
I 41
short, such as "a", "b", "c", ... "aa", "ab", ....
E 41
D 10
.PP
E 10
I 10
.TSTART
E 10
D 5
XXX - results go here, creates then deletes.
E 5
I 5
D 48
.so ../Results/tmp/lat_fs.tbl
E 5
D 10
.PP
E 10
I 10
D 41
.TEND "File system latency in milliseconds"
E 10
Notice that Linux does quite well here.  However, Linux is not guaranteeing
anything about the disk integrity, the directory operations are done in
E 41
I 41
D 45
.TEND "File system latency (milliseconds)"
E 45
I 45
D 47
.TEND "File system latency in milliseconds"
E 47
I 47
.TEND "File system latency (milliseconds)"
E 48
I 48
.so lat_fs.tbl
.TEND "File system latency (microseconds)"
.PP
E 48
E 47
E 45
The create and delete latencies are shown in Table \n[TABLE].
D 50
Notice that Linux does quite well here.  However, Linux does not guarantee
E 50
I 50
Notice that Linux does extremely well here, 2 to 3 orders of magnitude faster
than the slowest systems.  However, Linux does not guarantee
E 50
anything about the disk integrity; the directory operations are done in
E 41
D 48
memory.  Other systems issue the directory updates into a log and play
the log out to disk such that the disk is always in a stable state.
E 48
I 48
D 50
memory.  Other systems may issue the directory updates into a log and play
the log out to disk such that the disk is always in a stable state.  Unless 
Unixware has modified UFS substantially, they must be running in an unsafe
mode since the FreeBSD UFS is much slower and both file systems are basically
the 4BSD fast file system.  The DEC Alpha@300 is using battery backed memory
as safe place to do their updates.
E 50
I 50
memory.  Other fast systems, such as SGI's XFS, use a log to guarantee the 
file system integrity.
The slower systems, all those with ~10 millisecond file latencies, are
using synchronous writes to guarantee the file system integrity.
Unless Unixware has modified UFS substantially, they must be running in
an unsafe mode since the FreeBSD UFS is much slower and both file
systems are basically the 4BSD fast file system.
E 50
E 48
.NH 2
Disk latency
I 19
.\" XXX - either get more results for this benchmark or delete it.
.\" I'd really like to not delete it - lmdd is probably the most
.\" useful tool and it gets the least press.
E 19
.PP
I 33
D 50
Included with \*[lmbench] is a small benchmarking program, useful for
E 50
I 50
Included with \*[lmbench] is a small benchmarking program useful for
E 50
D 41
measuring disk and file I/O.  \f(CWlmdd\fP, which is patterned after 
E 41
I 41
measuring disk and file I/O.  \*[lmdd], which is patterned after 
E 41
the Unix utility \f(CWdd\fP, measures both sequential and random I/O,
optionally generates patterns on output and checks them on input, 
supports flushing the data from the buffer cache on systems that 
D 51
support \f(CWmsync\fP, and has a very flexible user interface.  Almost
all I/O benchmarks can be trivially replaced with a \f(CWperl\fP script
E 51
I 51
support \f(CWmsync\fP, and has a very flexible user interface.  
Many I/O benchmarks can be trivially replaced with a \f(CWperl\fP script
E 51
D 41
wrapped around \f(CWlmdd\fP.  
E 41
I 41
wrapped around \*[lmdd].  
E 41
.PP
While we could have generated both sequential and random I/O results as
D 48
part of this benchmark, we did not because those benchmarks are heavily
E 48
I 48
part of this paper, we did not because those benchmarks are heavily
E 48
influenced by the performance of the disk drives used in the test.  We
intentionally measure only the system overhead of a SCSI command since
that overhead may become a bottleneck in large database configurations.
.PP
E 33
I 31
D 32
Included with \*[lmbench] is a small benchmarking program, useful for
measuring disk and file I/O.  \f(CWlmdd\fP, which is patterned after 
the Unix utility \f(CWdd\fP, measures both sequential and random I/O,
optionally generates patterns on output and checks them on input, 
supports flushing the data from the buffer cache on systems that 
support \f(CWmsync\fP, and has a very flexible user interface.  Almost
all I/O benchmarks can be trivially replaced with a \f(CWperl\fP script
wrapped around \f(CWlmdd\fP.  
.PP
While we could have generated both sequential and random I/O results as
part of this benchmark, we did not because those benchmarks are heavily
influenced by the performance of the disk drives used in the test.  We
intentionally measure only the system overhead of a SCSI command since
that overhead may become a bottleneck in large database configurations.
.PP
E 32
E 31
D 19
XXX - either get more results for this benchmark or delete it.
I 6
I'd really like to not delete it - lmdd is probably the most
useful tool and it gets the least press.
E 6
.PP
A benchmark that is designed to measure the overhead of a disk
operation.  Results are reported as operations per second.
.PP
The benchmark is designed with SCSI disks in mind.  It actually simulates
a large number of disks in the following way.  The benchmark reads 512 byte
chunks sequentially from the raw disk device (raw disks are unbuffered
and are not read ahead by Unix).  The benchmark ``knows'' that most
disks have read ahead buffers that read ahead the next 32-128 kilobytes.
Furthermore, the benchmark ``knows'' that the disks rotate and read ahead
E 19
I 19
Some important applications, such as transaction processing, are
limited by random disk IO latency.  
Administrators can increase the number of disk operations per
second by buying more disks, until the processor overhead becomes
the bottleneck.
D 48
This benchmark measures the processor overhead associated with each
E 48
I 48
The \f(CWlmdd\fP  benchmark measures the processor overhead associated with each
E 48
disk operation, and it can provide an upper bound on the number of
disk operations the processor can support.
D 41
It is designed for SCSI disks and it assumes that most
disks have 32-128KB read ahead buffers and they can read ahead
E 41
I 41
It is designed for SCSI disks, and it assumes that most
D 44
disks have 32-128KB read-ahead buffers and that they can read ahead
E 44
I 44
D 47
disks have 32\-128KB read\-ahead buffers and that they can read ahead
E 47
I 47
D 50
disks have 32\-128KB read-ahead buffers and that they can read ahead
E 50
I 50
disks have 32-128K read-ahead buffers and that they can read ahead
E 50
E 47
E 44
E 41
E 19
faster than the processor can request the chunks of data.\**
.FS
D 41
This may not always be true - a processor could be fast enough to make the
D 28
requests faster than the rotating disk.  If we take 3 megabyte/sec to be disk
speed, a fair speed, and divide that by 512, that is 6144 IOs/second, or
D 14
163 microseconds per IO.  I don't know of any processor/OS/IO controller
E 14
I 14
163 microseconds per IO.  We don't know of any processor/OS/IO controller
E 14
D 19
combinations that can do an
IO in 163 microseconds.
E 19
I 19
combinations that can do an IO in 163 microseconds.
E 28
I 28
requests faster than the rotating disk.  If we take 6 megabyte/sec to be disk
speed, and divide that by 512, that is 12288 IOs/second, or
81 microseconds per IO.  We don't know of any processor/OS/IO controller
E 41
I 41
This may not always be true: a processor could be fast enough to make the
requests faster than the rotating disk.  
D 44
If we take 6 megabytes/second to be disk
E 44
I 44
If we take 6M/second to be disk
E 44
D 48
speed, and divide that by 512, that is 12,288 IOs/second, or
E 48
I 48
speed, and divide that by 512 (the minimum transfer size), that is 12,288 IOs/second, or
E 48
81 microseconds/IO.  We don't know of any processor/OS/IO controller
E 41
combinations that can do an IO in 81 microseconds.
E 28
E 19
.FE
D 19
So the benchmark is basically reading small chunks of data from the
disks track buffer.  Another way to look at this is that the benchmark 
E 19
I 19
.PP
D 41
The benchmark simulates a large number of disks by reading 512 byte
E 41
I 41
The benchmark simulates a large number of disks by reading 512byte
E 41
D 28
chunks sequentially from the raw disk device (raw disks are unbuffered
E 28
I 28
transfers sequentially from the raw disk device (raw disks are unbuffered
E 28
and are not read ahead by Unix).  
Since the disk can read ahead faster than the system can request
D 28
data, the benchmark is reading small chunks of data from the
disks track buffer.  
E 28
I 28
data, the benchmark is doing small transfers of data from the
disk's track buffer.  
E 28
Another way to look at this is that the benchmark 
E 19
D 41
is doing memory to memory transfers across a SCSI channel.
I 19
D 31
It is possible to generate loads of upwards of 500 operations on a
E 31
I 31
D 32
It is possible to generate loads of upwards of 1000 operations on a
E 32
I 32
D 33
It is possible to generate loads of upwards of 500 operations on a
E 33
I 33
It is possible to generate loads of upwards of 1000 operations on a
E 33
E 32
E 31
D 27
single SCSI disk.  
E 27
I 27
single SCSI disk.
.TSTART
.so ../Results/tmp/lat_disk.tbl
D 28
.TEND "Disk latency microseconds"
E 28
I 28
.TEND "Disk I/O latency in microseconds"
E 28
E 27
E 19
.PP
E 41
I 41
D 44
is doing memory-to-memory transfers across a SCSI channel.
E 44
I 44
D 47
is doing memory\-to\-memory transfers across a SCSI channel.
E 47
I 47
is doing memory-to-memory transfers across a SCSI channel.
E 47
E 44
D 48
It is possible to generate loads of more than 1,000 operations on a
single SCSI disk.
E 48
I 48
It is possible to generate loads of more than 1,000 SCSI
operations/second on a single SCSI disk.  For comparison, disks under
database load typically run at 20-80 operations per second.
E 48
.TSTART
.so ../Results/tmp/lat_disk.tbl
D 47
.TEND "Disk I/O latency in microseconds"
E 47
I 47
D 50
.TEND "Disk I/O latency (microseconds)"
E 50
I 50
.TEND "SCSI I/O overhead (microseconds)"
E 50
E 47
.PP
E 41
D 19
No matter how you look at it, the resulting number represents a 
\fBlower\fP bound on the overhead of a disk I/O.  In point of fact,
the real numbers will be higher on SCSI systems.  Most SCSI controllers
will not disconnect if the request can be satisfied immediately; that is
the case here.  In practice, the overhead numbers will be higher because
the processor will send the request, disconnect, get interrupted,
reconnect, and transfer.
E 19
I 19
The resulting overhead number represents a 
\fBlower\fP bound on the overhead of a disk I/O.  
The real overhead numbers will be higher on SCSI systems because
most SCSI controllers will not disconnect if the request can be
satisfied immediately.
During the benchmark, the processor simply sends the request and
transfers the data, while 
during normal operation, the processor will send the request,
disconnect, get interrupted, reconnect, and transfer the data.
E 19
.PP
D 19
It is possible to generate loads of upwards of 500 IOPs on a single
SCSI disk using this technique.  It is useful to do that to figure out
how many drives could be supported on a system before there are no
more processor cycles to handle the load.  Using this trick, you 
do not have to hook up 30 drives, you simulate them.
E 19
I 19
This technique can be used to discover how many drives a system can support
D 41
before the system becomes CPU-limited because you can produce the
E 41
I 41
D 44
before the system becomes CPU-limited because it can produce the
E 44
I 44
D 47
before the system becomes CPU\-limited because it can produce the
E 47
I 47
before the system becomes CPU-limited because it can produce the
E 47
E 44
E 41
overhead load of a fully configured system with just a few disks.
E 19
D 17
.NH 2 
D 6
Memory read latency
E 6
I 6
Memory read latency background
E 6
.PP
I 4
Memory read latency has many definitions because there are many different
numbers all defined as some sort of read latency.  The most common definitions,
in increasing time order,
are memory chip cycle time, processor pins to memory and back time, load
in a vacuum time, and back to back load time.  
D 6
.IP "\fIMemory chip cycle latency\fP"
E 6
I 6
D 10
.IP "\fIMemory chip cycle latency\fP" 2
E 10
I 10
.BU
\fIMemory chip cycle latency\fP
E 10
E 6
Memory chips are rated in nanoseconds, typical speeds are around 60 nanoseconds.
This is sometimes referred to as the chip cycle time.  That is a bit misleading,
since the chips are operated as a two dimensional array and need one cycle
for each of the RAS (row access strobe) and CAS (column access strobe) 
before the memory is available.  This means that 60 nanosecond memory
chips really are more like 120 nanosecond memory chips.  Some systems
operate memory in ``page mode'' or ``static column'' memory systems
hold either RAS or CAS and allow subsequent accesses in the same row
or column in one cycle instead of two.
D 10
.IP "\fIPin to pin latency\fP"
E 10
I 10
.BU
\fIPin to pin latency\fP
E 10
In addition to the memory cycle time, there is the time that it takes for the
memory request to travel from the processor's pins to the memory subsystem
and back again.  The paper describing the DEC 8400 quotes memory latencies of
265 nanoseconds; these numbers are the pin to pin values.
D 10
.IP "\fILoad in a vacuum latency\fP"
E 10
I 10
.BU
\fILoad in a vacuum latency\fP
E 10
A load in vacuum is the time that the processor will wait for one load that
must be fetched from main memory, i.e., is a cache miss.  The vacuum part
means that there is no other activity on the system bus, including no other
loads.  Systems architects frequently refer to this value as the memory
latency.
D 10
.IP "\fIBack to back load latency\fP"
E 10
I 10
.BU
\fIBack to back load latency\fP
E 10
Back to back load latency is the time that each load takes, assuming that the
instruction before and after was also a cache missing load.  Back to back
loads may take longer than loads in a vacuum for the following reason: 
many systems implement something known as \fIcritical word first\fP, 
D 12
which means that the subblock of the cache line that contains the word being
E 12
I 12
which means that the sub block of the cache line that contains the word being
E 12
loaded is delivered to the processor before the entire cache line has been
brought into the cache.  If another load occurs quickly enough after the 
processor gets restarted from the current load, the second load may get
stalled because the cache is still busy filling the cache line for the 
previous load.  On some unnamed future systems, the difference between
back to back and load in a vacuum will be about 35%.
.PP
\*[lmbench] measures back to back load latency and calls that memory
read latency.  This has caused some systems architects a bit of frustration
when customers measure memory latency and get numbers worse than what the
systems architects said they would get.  We are sticking with this definition
because we feel that it is what most software people consider to be memory
latency.  Consider the following code fragment:
.DS
I 5
.nf
E 5
.ft CW
D 5
for (p = head; p->p_next != NULL; p = p->p_next);
E 5
I 5
p = head;
while (p->p_next)
	p = p->p_next;
.ft
.fi
E 5
.DE
On a DEC Alpha, the loop part turns into three instructions, including the
load.  A 300Mhz processor has a 3.33 nanosecond cycle time, so the loop
could execute in slightly less than 10 nanoseconds.  However, the load itself
will take 400 nanoseconds on a 300Mhz DEC 8400.  In other words, the 
instructions cost 10 nanoseconds but the load stalled for 400.  Another
way to look at it is that there would need to be 400/3.3 or 121 non-dependent,
non-loading
instructions following the load to hide the load latency.  Super scalar 
processors are even worse off since since they really want to be doing
more than one thing per cycle.
.PP
In summary, we believe that processors are more than fast enough that the
load latency seen on average for loads that miss, will be closer to the
back to back load number than the load in a vacuum number.
I 6
.NH 3 
Memory read latency
E 6
.PP
E 4
This is perhaps the most interesting benchmark in the suite.  The
D 4
entire memory hierarchy is measured, including on board cache latency
E 4
I 4
entire memory hierarchy may be measured, including on board cache latency
E 4
and size, external cache latency and size, main memory latency, and TLB
miss latency.  
.PP
The benchmark varies two parameters, array size and array stride.  
For each size, a list of pointers is created for all of the different 
strides.  Then the list is walked like so
.DS
D 15
.ft CB
E 15
I 15
.ft CW
E 15
mov  r0,(r0)  # C code: p = *p;
.ft
.DE
The time to do about fifty thousand loads (the list wraps) is measured and
reported.  The time reported is pure latency time and may be zero even though
the load instruction does not execute in zero time.  Zero is defined as one
clock cycle; in other words, the time reported is \fBonly\fP memory latency
time, as it does not include the instruction execution time.  It is assumed
that all processors can do a load instruction (not counting stalls) in one
processor cycle.  In other words, if the processor cache load time
is 60 nanoseconds on a 20 nanosecond processor, the load latency reported
would be 40 nanoseconds, the missing 20 seconds is for the load instruction
itself.  Processors that can manage to get the load address out to the 
address pins before the end of the load cycle get some free time in this
benchmark (I don't think any processors can do that).
.PP
Note that this benchmark has been validated by logic analyzer measurements
on an SGI indy. The
clever reader might realize that last few nanoseconds of inaccuracy could be
rounded off by realizing that the latency is always going to be a multiple
of the processor clock rate.
I 11
.TSTART 1
.so mem.pic
.TEND "Memory latency" 1
E 11
.PP
The raw data is a series of data sets.  Each data set is a stride size,
with array size varied from about one kilobyte up to eight megabytes.
When these data sets are all plotted together (using a log base 2 scale
for the size variable), the data will be seen to contain a series of 
horizontal plateaus.  The first is the on board data cache latency (if there
is an on board cache).  The point where the lines start to go up marks the
size of the cache.  The second is the external cache, the third is the
main memory, and the last is main memory plus TLB miss cost.  In addition
to this information, the cache line size can be derived by noticing which
strides are faster than main memory times.  The first stride that is
main memory speed is likely to be the cache line size.  The reason is
that the strides that are faster than memory indicate that the benchmark is
getting more than one hit per cache line.  Note that prefetching may confuse
D 14
you.
E 14
I 14
the issue because a demand read may stall behind a prefetch load.
E 14
.PP
D 11
The graph below shows a particularly nicely made machine, a DEC alpha.
E 11
I 11
The graph XXX shows a particularly nicely made machine, a DEC alpha.
E 11
D 14
This machine is nice because (a) it shows the latencies and sizes of
E 14
I 14
We use this machine as the example 
because (a) it shows the latencies and sizes of
E 14
the on chip level 1 and motherboard level 2 caches, and (b) because it
has the best all around numbers, especially considering it can support a
D 14
4 megabyte level 2 cache.  Nice work, DEC.
E 14
I 14
4 megabyte level 2 cache.
E 17
E 14
I 10
D 11
.TSTART 1
E 10
.so mem.pic
I 10
.TEND "Memory latency" 1
E 11
E 10
.NH 1
I 21
Future work
.PP
D 28
There are several known improvements and exstensions that could be made
E 28
I 28
There are several known improvements and extensions that could be made
E 28
to \*[lmbench].
D 23
.BU "Memory write latency"
The memory latency section is read only.  
.BU "TLB miss cost"
.BU "MP benchmarks"
.BU "Static vs dynamic processes"
E 23
I 23
.BU "Memory latency" .
D 41
The current benchmark measures clean read latency.  By clean, we mean that 
E 41
I 41
D 44
The current benchmark measures clean-read latency.  By clean, we mean that 
E 44
I 44
D 47
The current benchmark measures clean\-read latency.  By clean, we mean that 
E 47
I 47
The current benchmark measures clean-read latency.  By clean, we mean that 
E 47
E 44
E 41
the cache lines being replaced are highly likely to be unmodified, so there
D 41
is no associated write back cost.  We would like to extend the benchmark
to measure dirty read latency, as well as write latency.  Other changes 
include making the benchmark impervious to sequential prefetching, and
E 41
I 41
D 44
is no associated write-back cost.  We would like to extend the benchmark
to measure dirty-read latency, as well as write latency.  Other changes 
E 44
I 44
D 47
is no associated write\-back cost.  We would like to extend the benchmark
to measure dirty\-read latency, as well as write latency.  Other changes 
E 47
I 47
is no associated write-back cost.  We would like to extend the benchmark
to measure dirty-read latency, as well as write latency.  Other changes 
E 47
E 44
include making the benchmark impervious to sequential prefetching and
E 41
measuring TLB miss cost.
.BU "MP benchmarks" .
D 41
None of the benchmarks in \*[lmbench] are designed to directly measure any
multi processor features.  At a minimum, we could measure cache to cache
latency as well as cache to cache bandwidth.
E 41
I 41
None of the benchmarks in \*[lmbench] is designed to measure any
multiprocessor features directly.  At a minimum, we could measure 
D 44
cache-to-cache latency as well as cache-to-cache bandwidth.
E 44
I 44
D 47
cache\-to\-cache latency as well as cache\-to\-cache bandwidth.
E 47
I 47
cache-to-cache latency as well as cache-to-cache bandwidth.
E 47
E 44
E 41
.BU "Static vs. dynamic processes" .
In the process creation section, we allude to the cost of starting up processes
that use shared libraries.  When we figure out how to create statically linked
processes on all or most systems, we could quantify these costs exactly.
D 24
.BU "McCalpin's streams benchmark" .
E 24
I 24
.BU "McCalpin's stream benchmark" .
E 24
D 41
We will probably incorporate part or all of this benchmark.
E 41
I 41
We will probably incorporate part or all of this benchmark into \*[lmbench].
E 41
.BU "Automatic sizing" .
We have enough technology that we could determine the size of the external
cache and autosize the memory used such that the external cache had no effect.
I 48
.BU "More detailed papers" .
There are several areas that could yield some interesting papers.  The
memory latency section could use an in-depth  treatment, and the
context switching section could turn into an interesting discussion of
caching technology.
E 48
I 32
D 33
.BU "Autoconf" .
The Free Software Foundation has a package wrapper that automatically 
configures the source code to the target platform.  We really need to be
using this package soon.
E 33
E 32
D 31
.BU "autoconf" .
The Free Software Foundation has a package wrapper that automatically 
configures the source code to the target platform.  We really need to be
using this package soon.
E 31
E 23
D 22
.BU 
E 22
.NH 1
E 21
I 14
D 41
Conclusions
E 41
I 41
Conclusion
E 41
.PP
D 44
\*[lmbench] is a useful, portable micro-benchmark suite designed to
E 44
I 44
D 47
\*[lmbench] is a useful, portable micro\-benchmark suite designed to
E 47
I 47
\*[lmbench] is a useful, portable micro-benchmark suite designed to
E 47
E 44
D 50
measure important aspects of system performance.   The results presented
D 19
here make a strong case for correlating over all performance to memory
sub system performance.  It would appear that the processor wars may be
over and it is now time for the memory wars to commence.
E 19
I 19
D 41
here make a strong case for the correlation of memory sub-system 
performance to over-all performance.
E 41
I 41
here make a strong case for the correlation of memory subsystem 
performance with overall performance.
E 41
It would appear that the processor wars may be
over and it is time for the memory wars to commence.
E 50
I 50
measure important aspects of system performance.   We have found that a good
memory subsystem is at least as important as the processor speed.
As processors get faster and faster, more and more of the system design
effort will need to move to the cache and memory subsystems.
E 50
E 19
.NH 1
E 14
D 6
Other measurements
.PP
.NH 2
Processor cycle time
mhz
.PP
Calculates the megahertz and clock speed of the processor.  This is the
standard loop in which a series of interlocked operations are timed,
and then the megahertz is derived from the timing.  The operations 
are purposefully interlocked to overcome any super scalar features of the
system under test.
.PP
There are actually three versions of mhz, a generic one that works on
most systems, and two specific versions for SuperSPARC and rs6000
systems.
.PP
It turns out that the
SuperSPARC processor has two ALU's that are run at twice the clock rate,
allowing two interlocked operations to complete in one processor clock.\**
.FS
Credit and thanks to John Mashey of SGI/MIPS fame, who kindly took the
time to out why the benchmark wasn't working on SuperSPARC
systems.  He explained the SuperSPARC pipeline and the solution to the
E 3
problem.
.FE
Fortunately, the ALU's are asymmetric and can not do two shifts in
one processor clock.  Shifts are used on SuperSPARC systems.
.PP
IBM rs6000 systems have a C compiler that does not honor the
``register'' directive in unoptimized code.  The IBM loop looks
like it is doing half as many instructions as the others.  This
is on purpose, each add on the IBM is actually two instructions
(I think it is a load/add/store or something like that).
.NH 1
E 6
Acknowledgments
D 3
.LP
E 3
I 3
.PP
E 3
D 8
I would like to acknowledge Sun Microsystems for supporting the development
of this project.  In particular,  my personal thanks to Paul Borrill,
E 8
I 8
D 21
We would like to acknowledge Sun Microsystems for supporting the development
of this project.  Particular thanks to Paul Borrill,
E 8
Director of the Architecture and Performance group, for conceiving and
supporting the development of these benchmarks.
E 21
I 21
D 28
Many people have provided invaluable help and insight.  We thank all of them
E 28
I 28
Many people have provided invaluable help and insight into both the
benchmarks themselves and the paper.  The \s-1USENIX\s0 reviewers
were especially helpful.
We thank all of them
E 28
and especially thank:
D 28
Ken Okin \s-2(Sun)\s0,
Kevin Normoyle \s-2(Sun)\s0,
Satya Nishtala \s-2(Sun)\s0,
E 28
I 28
D 41
Ken Okin \s-2(SUN)\s0,
Kevin Normoyle \s-2(SUN)\s0,
Satya Nishtala \s-2(SUN)\s0,
E 28
Greg Chesson \s-2(SGI)\s0,
John Mashey \s-2(SGI)\s0,
Neal Nuckolls \s-2(SGI)\s0,
John McCalpin \s-2(Univ. of Delaware)\s0,
Ron Minnich \s-2(Sarnoff)\s0,
E 41
I 41
D 48
Ken Okin (SUN),
Kevin Normoyle (SUN),
Satya Nishtala (SUN),
Greg Chesson (SGI),
John Mashey (SGI),
Neal Nuckolls (SGI),
John McCalpin (Univ. of Delaware),
Ron Minnich (Sarnoff),
E 41
and
D 41
Tom Rokicki \s-2(HP)\s0.
E 41
I 41
Tom Rokicki (HP).
E 48
I 48
Ken Okin \s-1(SUN)\s0,
Kevin Normoyle \s-1(SUN)\s0,
Satya Nishtala \s-1(SUN)\s0,
Greg Chesson \s-1(SGI)\s0,
John Mashey \s-1(SGI)\s0,
Neal Nuckolls \s-1(SGI)\s0,
John McCalpin \s-1(Univ. of Delaware)\s0,
Ron Minnich \s-1(Sarnoff)\s0,
D 51
Ron Minnich \s-1(Sarnoff),
E 51
Chris Ruemmler \s-1(HP)\s0,
Tom Rokicki \s-1(HP)\s0,
and 
D 51
John Weitz \s-1(Digidesign).
E 51
I 51
John Weitz \s-1(Digidesign)\s0.
E 51
E 48
E 41
E 21
.PP
D 8
My thanks to John Mashey and Neal Nuckolls of Silicon Graphics for reviews,
E 8
I 8
D 21
Thanks to John Mashey and Neal Nuckolls of Silicon Graphics for reviews,
E 8
comments, and explanations of the more obscure problems.
E 21
I 21
We would also like to thank all of the people that have run the
D 42
benchmark and contributed their results, none of this would be possible
E 42
I 42
benchmark and contributed their results; none of this would have been possible
E 42
without their assistance.
E 21
.PP
D 8
My thanks to Satya Nishtala of Sun Microsystems for (a) listening to me
E 8
I 8
D 21
Thanks to Satya Nishtala of Sun Microsystems for (a) listening to Larry
E 8
complain about memory latencies over and over, (b) doing something about
it in future SPARC systems, and (c) reviewing the memory latency results
D 8
and explained IBM's sub blocking scheme (I still don't really understand
it but he does.  Ask him).
E 8
I 8
D 19
and explained IBM's sub blocking scheme.
E 19
I 19
and explaining IBM's sub-blocking scheme.
E 21
I 21
D 22
Special thanks to Linus Torvalds and all of the Linux hackers - Linux is the 
development platform for \*[lmbench].
E 22
I 22
D 23
Our heartfelt thanks to the following people
E 23
I 23
D 28
Our thanks to the following people
E 23
and all of the free software community for tools that were used by us.
E 28
I 28
Our thanks to 
all of the free software community for tools that were used during this
project.
E 28
D 23
\*[lmbench] is currently developed on Linux, a free Unix written by 
E 23
I 23
\*[lmbench] is currently developed on Linux, a copylefted Unix written by 
E 23
Linus Torvalds and his band of happy hackers.
This paper and all of the 
\*[lmbench] documentation was produced using
the \f(CWgroff\fP suite of tools written by James Clark.
Finally, all of the data processing of the results is done with
\f(CWperl\fP written by Larry Wall.  
E 22
.PP
D 22
Sun Microsystems, and in particular, Ken Okin and Paul Borrill,
E 22
I 22
D 41
Sun Microsystems, and in particular, Paul Borrill,
E 41
I 41
Sun Microsystems, and in particular Paul Borrill,
E 41
E 22
supported the initial development of this project.  Silicon Graphics
has supported ongoing development that turned into far more time then we 
ever imagined.  We are grateful to both of these companies for their
financial support.
E 21
E 19
E 8
.NH 1
Obtaining the benchmarks
D 3
.LP
E 3
I 3
.PP
E 3
D 21
The benchmarks will be posted to the Usenet comp.benchmarks group.  In 
addition,
D 15
mail sent to \f(CBarchives@slovax.engr.sgi.com\fP with a request for 
\f(CBlmbench.shar\fP
E 15
I 15
D 19
mail sent to \f(CWarchives@slovax.engr.sgi.com\fP with a request for 
\f(CWlmbench.shar\fP
E 15
sources will get the latest and greatest.
E 19
I 19
you may request the latest version of lmbench by sending email 
to \f(CWarchives@slovax.engr.sgi.com\fP with \f(CWlmbench1.0*\fP 
E 21
I 21
D 48
The benchmarks are available via anonymous ftp as 
\f(CWftp.sgi.com:/pub/lmbench.tgz\fP as well as via a mail server.
E 48
I 48
The benchmarks are available at
.ft I
http://reality.sgi.com/employees/lm_engr/lmbench.tgz
.ft
as well as via a mail server.
E 48
D 41
You may request the latest version of lmbench by sending email 
E 41
I 41
You may request the latest version of \*[lmbench] by sending email 
E 41
D 48
to \f(CWarchives@slovax.engr.sgi.com\fP with \f(CWlmbench1.1*\fP 
E 48
I 48
to \fIarchives@slovax.engr.sgi.com\fP with \fIlmbench-current*\fP 
E 48
E 21
as the subject.
I 31
D 32
.\" Override groff's -mgs definition of the reference printout.  This one
.\" takes less space.
.de ref*end-print
.ie d [F \{\
.LP
.in +1.0
.ti -1.0
[\\*([F].
E 32
I 32
.\" .R1
.\" bibliography references
.\" .R2
.\"********************************************************************
.\" Redefine the IP paragraph format so it won't insert a useless line
.\" break when the paragraph tag is longer than the indent distance
.\"
.de @IP
.if \\n[.$]>1 .nr \\n[.ev]:ai (n;\\$2)
.par*start \\n[\\n[.ev]:ai] 0
.if !'\\$1'' \{\
.	\" Divert the label so as to freeze any spaces.
.	di par*label
.	in 0
.	nf
\&\\$1
.	di
.	in
.	fi
.	chop par*label
.	ti -\\n[\\n[.ev]:ai]u
.	ie \\n[dl]+1n<=\\n[\\n[.ev]:ai] \\*[par*label]\h'|\\n[\\n[.ev]:ai]u'\c
.	el \{\
\\*[par*label]
.\".	br
.	\}
.	rm par*label
E 32
.\}
I 32
..
.\"********************************************************************
.\" redefine the way the reference tag is printed so it is enclosed in
.\" square brackets
.\"
.de ref*end-print
D 51
.ie d [F .IP "[\\*([F]"
E 51
I 51
.ie d [F .IP "[\\*([F]" 2
E 51
E 32
.el .XP
\\*[ref*string]
..
E 31
E 19
I 4
D 14
.NH 1
Conclusions
.PP
\*[lmbench] is a useful, portable micro-benchmark suite designed to
D 12
measure important aspects of system performance.
E 12
I 12
measure important aspects of system performance.   The results presented
here make a strong case for correlating over all performance to memory
sub system performance.  It would appear that the processor wars may be
over and it is now time for the memory wars to commence.
E 14
E 12
D 9
.NH 1
References
.[
Chen93d
.]
.[
Chen94a
.]
.[
Fenwick95
.]
.[
Howard88
.]
.[
Jain91
.]
.[
McCalpin95
.]
.[
Ousterhout90
.]
.[
Park90
.]
.[
I 8
Shein89
.]
.[
E 8
Smith82b
.]
.[
Smith85
.]
.[
Wolman89
.]
.[
Wong88
.]
.[
I 8
Wulf95
.]
.[
E 8
$LIST$
.]
E 9
I 9
D 32
.\" .R1
.\" bibliography references
.\" .R2
E 32
I 32
.\"********************************************************************
.\" Get journal number entries right.  Now will print as V(N) rather
.\" than the awful V, N.
.\"
.de ref*add-N
.ref*field N "" ( ) 
..
I 34
.\"********************************************************************
I 49
.\" Get journal volume entries right.  Now will print as V(N) rather
.\" than the awful V, N.
.\"
.de ref*add-V
.ref*field V , "" "" ""
..
.\"********************************************************************
E 49
.\" Get the date entry right.  Should not be enclosed in parentheses.
.\"
.de ref*add-D
.ref*field D ","
..
E 34
E 32
.R1
accumulate
sort A+DT
database references
label-in-text
label A.nD.y-2
D 32
bracket-label \*([. \*(.] ", "
E 32
I 32
bracket-label [ ] ", "
E 32
bibliography references
.R2
I 48
.so bios
E 48
D 10


E 10
E 9
E 4
E 2
I 1
E 1
